[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TDCJ Prison Heat",
    "section": "",
    "text": "This is for a group project under UT’s Media Innovation Group.\nIt is an examination of weather conditions recorded in prisons across the state of Texas using documents obtained through the Texas Department of Criminal Justice. More extensive sourcing information can be found on the project’s cleaning files.\nLooking at the navigation of this site, you’ll mainly want to look at the files under the Analysis heading.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#outdoor-logs",
    "href": "index.html#outdoor-logs",
    "title": "TDCJ Prison Heat",
    "section": "Outdoor logs",
    "text": "Outdoor logs\nLauren McGaughy of the Texas Newsroom public radio collaborative used a public information act to obtain the outdoor temperature logs kept by Texas prisons. The logs are hand-written and the data could not be accurately pulled through mechanical means, so a group of Media Innovation Group fellows transcribed a sample set of logs from each prison, from July 24, 2023 through July 31, 2023.\nHere is what we found:\n\nBasics and legibility\nMay 2025\n\nWe transcribed 8 days of logs for 82 units, making up about 15,700 rows of data.\nIf we found a row of the data was corrected or there was a question about the legibility, we included a note within that row. We included some consistent-worded notes so we could count them, but also had some more free-form when those categories didn’t fit. We tried to be consistent, but different humans were involved, so variability is inevitable. These based on the last seven days of July 2023.\n\nAbout 15% of the records had notes.\nAbout 1,300 records (7.5%) had corrections.\nAbout 1,000 records (6.5%) we marked with legibility issues.\n\nOf those with notes, we also counted what was at issue.\n\nThere were about 650 notes concerning the heat index/wind chill column. That column is challenging because it captures two different things (though not at the same time). Some records also include what we determined was a heat risk category category, which we recorded in a separate column.\nThere were about 590 notes about the temperature column.\n\nWe wanted to see for which units we added notes.\n\nMore than 60% of the records for the Stevenson unit had notes, and many of those were flagged as “corrections”. However, when you look at the original document, it appears the unit reviews the logs and clears up any legibility issues. i.e. having more corrections could be a positive thing.\nIn some cases we recorded an overall note about the unit as opposed to notes for each individual line. They are printed in browsable form here.\n\n\n\n\nPrecautionary measures\nAugust 2025 update\nHere we wanted to find which units should be under heat-related “precautionary measures” based on written data logs, and how that compared to actual Incident Command System activations.\nThis was challenging because the TDCJ definitions and directives to define if the unit should be in “precautionary measures” are ambiguous and confusing. See the Precautionary measures notebook for a full discussion.\nWe ended up using the following definition: When temperatures are 105+ degrees or heat index is 113+ for three or more consecutive days.\nWe use the term “unit day” to describe when any of the 82 units meets a criteria on a single day.\nOf our eight days of records from 82 prisons …\n\nSeven units met the ICS criteria on some days for a total of 17 unit days, based on the definition given to us by the TDCJ communications department. None of those units had active ICS protocols in place on those days. This count could be low since units could have met protocol in the days before our time period.\nAccording to the activation data, there were 24 days in our time period where ICS heat mitigation protocols were put in place. On those activation days, none of these units met the criteria we’ve been given by TDCJ communications based on the weather logs kept by units. That said, half of the activations are within the first two days of our study, meaning they could have met the criteria in the days before our time frame.\nOut of 656 “unit days” possible (82 units, 8 days), excessive heat conditions (105 degrees or 113 heat index) were reached 127 times. Half of the units reached the criteria at least once. In only 12 of those instances was a unit in active precautionary measures.\n\nThere are also 15 “unit days” where there was no heat index values for that day, but the unit did include a heat index “risk category” that was Cat 3 or higher. Those are potentially 113 degrees or greater and would put the day in precautionary measures territory.\n\n\nLog accuracy vs weather stations\nMay 2025\n\nI’d like to take another crack at pairing these weather stations and include the distance the weather station is from the prison unit. I’ve found a few issues, along with some solutions to them, but they’ve yet to be implemented. For now, consider these stats with caution.\n\nTo gauge the accuracy of recorded temperature and humidity/heat index we paired each prison unit with a “nearby” weather station so we could compare the temperatures.\n\nWe don’t have a good match for about 15% of the records.\nOn average, temperatures are recorded -2.9 degrees lower than the closest weather station.\nAbout half of the temperatures are within 2 degrees, and 3/4ths are within 4 degrees.\n\nYou can peruse these ranges by unit.\n\nThe heat index differences are a little harder because of so much missing data, but we’ve charted them.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#indoor-temperatures",
    "href": "index.html#indoor-temperatures",
    "title": "TDCJ Prison Heat",
    "section": "Indoor temperatures",
    "text": "Indoor temperatures\nJuly 2025 update\nWhile we did some earlier analysis of indoor temperatures in the spring, there were enough changes in the companion data that we need to check and rework it.\nHere is what we’d like to find:\n\nWhat was the temperature inside on the days that units were under active ICS protocol?\nAre there days where inside a prison it was hotter than on the hottest day under protocol?\nHow many days were above 85 degrees inside? 95? 100?",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#collaborators",
    "href": "index.html#collaborators",
    "title": "TDCJ Prison Heat",
    "section": "Collaborators",
    "text": "Collaborators\nWorking on weather station data and analysis for indoor logs:\n\nPearson Neal\nJohan Villatoro\nAli Juell\nKarina Kumar\n\nAli Juell served as project lead and Christian McDonald mentored and edited.\nThose transcribing the outdoor logs included:\n\nEmily deMotte\nTeresa Do\nNicolas Pinto\nDiego Torrealba",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html",
    "href": "02-outdoor-readability.html",
    "title": "Outdoor log readability",
    "section": "",
    "text": "We hand-coded eight days of temperatures logs (7/24/2023 to 7/31/2023) for 82 Texas prisons. Here we take those logs and try to answer the following questions.",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#questions-to-answer",
    "href": "02-outdoor-readability.html#questions-to-answer",
    "title": "Outdoor log readability",
    "section": "Questions to answer",
    "text": "Questions to answer\n\nIn the logs, how many cases did we find where the records were hard to read or had problems?\nWhat were those problems, generally?\nDo certain units have more legibility problems?\n\n\nSomething we’ve yet to look at: there are cases where the log does not have a heat index hi_wc1 recorded, but there is a temperature and humidity. We could calculate that heat index when we have the data (and then compare to what the prison log has when it is present.)\n\n\nAlso, we perhaps need to go back to cleaning to see when units recorded hi_wc as categories, which would",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#setup",
    "href": "02-outdoor-readability.html#setup",
    "title": "Outdoor log readability",
    "section": "Setup",
    "text": "Setup\n\n\nExpand this to see code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(DT)",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#import",
    "href": "02-outdoor-readability.html#import",
    "title": "Outdoor log readability",
    "section": "Import",
    "text": "Import\nWe’re bringing in:\n\nOur coded and cleaned outdoor log data\nActivation records\nHourly weather readings\nPrison unit information\n\n\n\nExpand this to see code\nlogs_all &lt;- read_rds(\"data-processed/01-outdoor-cleaned.rds\")\nactivations &lt;- read_rds(\"data-processed/01-activation-cleaned.rds\")\nhourly &lt;- read_rds(\"data-processed/01-station-hourly-protocols.rds\")\nunits &lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\")",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#basic-information",
    "href": "02-outdoor-readability.html#basic-information",
    "title": "Outdoor log readability",
    "section": "Basic information",
    "text": "Basic information\nPeek at a sample\n\n\nExpand this to see code\nlogs_all |&gt; slice_sample(n = 5)\n\n\n\n  \n\n\n\nand glimpse the columns …\n\n\nExpand this to see code\nlogs_all |&gt; glimpse()\n\n\nRows: 15,744\nColumns: 13\n$ unit     &lt;chr&gt; \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd…\n$ region   &lt;chr&gt; \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"…\n$ date     &lt;date&gt; 2023-07-24, 2023-07-24, 2023-07-24, 2023-07-24, 2023-07-24, …\n$ rec      &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"…\n$ datetime &lt;dttm&gt; 2023-07-24 00:30:00, 2023-07-24 01:30:00, 2023-07-24 02:30:0…\n$ hour     &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ temp     &lt;dbl&gt; 83, 82, 81, 81, 80, 79, 79, 80, 83, 86, 90, 91, 92, 96, 100, …\n$ humid    &lt;dbl&gt; 54, 62, 67, 71, 74, 76, NA, 79, 72, 69, 61, 57, 53, 44, 36, 3…\n$ wind     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hi_wc    &lt;dbl&gt; 65, 68, 69, 71, 71, 71, NA, 84, 86, 95, 100, 100, 100, 103, 1…\n$ hi_wc_n  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ person   &lt;chr&gt; \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. J…\n$ notes    &lt;chr&gt; NA, NA, NA, NA, \"hi_wc corrected\", \"humid corrected\", NA, NA,…\n\n\nDate range of the data\n\n\nExpand this to see code\nlogs_all$date |&gt; summary()\n\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2023-07-24\" \"2023-07-25\" \"2023-07-27\" \"2023-07-27\" \"2023-07-29\" \"2023-07-31\" \n\n\n\nCount units\n\n\nExpand this to see code\nlogs_all |&gt; count(unit)\n\n\n\n  \n\n\n\n\n\nClip log data\nWe’ll remove July 24th so we have the last seven days of July, 2023.\n\n\nExpand this to see code\nlogs &lt;- logs_all |&gt; filter(date != \"2023-07-24\")\n\nlogs |&gt; count(date) |&gt; adorn_totals() |&gt; tibble()\n\n\n\n  \n\n\n\n\n\nTA: Basics\nWe transcribed outdoor temperature logs from 82 different units within the Texas prison system. Each log had 192 entries (24 hours for 8 days). We’ve clipped these records to be the last 7 days of July 2023, for a total of 13,776 records.",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#readability-answers",
    "href": "02-outdoor-readability.html#readability-answers",
    "title": "Outdoor log readability",
    "section": "Readability answers",
    "text": "Readability answers\nWhen we transcribed the outdoor temperature logs, we added notes when something was illegible or corrected on the form. Here we analyze those notes.\n\nRecords with notes\nHere we set flags if a record (an hour within a log) had a note, along with some categories. The result here is just a record sample to check our work.\n\n\nExpand this to see code\nlogs_notes &lt;- logs |&gt;\n  mutate(\n    notes_a = if_else(is.na(notes), F, T),\n    notes_c = case_when(str_detect(notes, \"correct\") ~ T, .default = F),\n    notes_l = case_when(str_detect(notes, \"legi\") ~ T, .default = F),\n  )\n\nlogs_notes |&gt; \n  select(unit, date, starts_with(\"notes\")) |&gt; \n  slice_sample(n = 20)\n\n\n\n  \n\n\n\nThis is the total percentage of records where we included some kind of note. TRUE means we included a note.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  tabyl(notes_a) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nWhen we recorded notes, we had some standardization. We included the term “corrected” if a record was scratched out and replaced with a new value or otherwise amended.\nThis is the percentage of records where something was corrected, per our notes.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  tabyl(notes_c) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nIf there was a legibility problem, we included the term “legibility”. This is the percentage of records where we noted some kind of legibility problem.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  tabyl(notes_l) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\n\n\nTA: Notes statistics\nWe have some kind of note in about 15% of all records. About 7.5% of records had a correction of some kind, and about 6.5% had legibility issues. (Some records might have both.). We should remember that data fellows had to use personal judgement on what to record and how, and that four different individuals performed the transcriptions. We tried our best to be consistent, but we are humans.\n\n\nTypes of notes\nSome records have more than one note. Here we “explode” those to count the notes individually. In some cases we had some standard notes, in other cases we didn’t.\nThe result here is just a sample to check our work.\n\n\nExpand this to see code\nnotes_exploded &lt;- logs |&gt; \n  select(unit, date, notes) |&gt; \n  filter(!is.na(notes)) |&gt; \n  group_by(unit, date) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  ungroup()\n\n# number of rows in new tibble\nnotes_exploded |&gt; nrow()\n\n\n[1] 2652\n\n\nExpand this to see code\n# sample rows\nnotes_exploded |&gt; slice_sample(n = 10)\n\n\n\n  \n\n\n\nLet’s look at the kinds of notes we recorded.\n\n\nExpand this to see code\nnotes_exploded_cnts &lt;- notes_exploded |&gt; count(notes, sort = T)\n\nnotes_exploded_cnts\n\n\n\n  \n\n\n\nHere we count how many individual records we labeled as “corrected” or had a “legibility” concern. I also counted a couple of other instances I saw, like where a “double” or “range” of values were recorded.\n\n\nExpand this to see code\nnotes_exploded_cnts |&gt;\n  # filter(str_detect(notes, \"corrected\")) |&gt; \n  summarise(\n    total_indiv_corrected = sum(n[str_detect(notes, \"correct\")]),\n    total_indiv_legibility = sum(n[str_detect(notes, \"legib\")]),\n    total_indiv_double = sum(n[str_detect(notes, \"double\")]),\n    total_indiv_range = sum(n[str_detect(notes, \"range\")]),\n    ) |&gt; \n  pivot_longer(cols = everything())\n\n\n\n  \n\n\n\n\n\nTA: Types statistics\nMost of the individual notes identified (about 1,300) were corrections of some kind. About 1,000 of them concerned legibility.\n\n\nVariable counts\nHere we try to get a handle on which variables recorded had the most notes (i.e., temp vs wind, etc.). We are counting how many times our variable terms were included in individual notes.\n\n\nExpand this to see code\nnotes_exploded_cnts |&gt;\n  # filter(str_detect(notes, \"corrected\")) |&gt; \n  summarise(\n    total_indiv_temp = sum(n[str_detect(notes, \"temp\")]),\n    total_indiv_humid = sum(n[str_detect(notes, \"humid\")]),\n    total_indiv_wind = sum(n[str_detect(notes, \"wind\")]),\n    total_indiv_hi_wc = sum(n[str_detect(notes, \"hi_wc\")]),\n    total_indiv_hi_wc_n = sum(n[str_detect(notes, \"hi_wc_n\")]),\n    total_indiv_person = sum(n[str_detect(notes, \"person\")]),\n  ) |&gt; \n  pivot_longer(everything()) |&gt; \n  arrange(value |&gt; desc())\n\n\n\n  \n\n\n\nWe had more notes on the hi_wc variable (Heat index/Wind chill) than any other, followed by temperature.\nThe heat index/wind chill record is already challenging because it measures two different things. Some records would also sometimes include what we determined was a heat index “category”, which we recorded in a separate column.\nHere we see how much that field was at issue by counting any notes that included hi_wc. The heat index category notes show up in this list, too.\n\n\nExpand this to see code\nnotes_exploded_cnts |&gt; \n  filter(str_detect(notes, \"hi_wc\")) |&gt; \n  adorn_totals() |&gt; tibble()\n\n\n\n  \n\n\n\n\n\nTA: Variable statistics\nThe heat index/wind chill columns was at issue about 650 times, with about half of these being corrections.\n\n\nNotes by unit\n\nUnits with most notes\nThis looks at which units had the most notes of any kind. The pct_notes is the percetage of rows that had a note of any kind.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  count(unit, notes_a) |&gt; \n  pivot_wider(names_from = notes_a, values_from = n) |&gt; \n  mutate(pct_notes = ((`TRUE` / (`FALSE` + `TRUE`)) * 100) |&gt; round(1)) |&gt; \n  arrange(pct_notes |&gt; desc())\n\n\n\n  \n\n\n\nTo gain some insight on what these might be, let’s look at these notes by Stevenson unit.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  filter(unit == \"Stevenson\" & !is.na(notes)) |&gt; \n  select(unit, region, date, hour, notes)\n\n\n\n  \n\n\n\nIt looks like there are many “corrections”. When you look at the original documents, it appears the unit must review the logs and regularly clears up any legibility issues. The logs are also signed. i.e. having more corrections could be a positive thing.\n\n\nUnit corrections, legibilty\nHere we explode all the notes for all the units and count how many are for corrections and legibility.\nWe sort the same list twice … once by corrections and once by legibility.\n\n\nExpand this to see code\nunits_cor_leg &lt;- logs_notes |&gt; \n  filter(!is.na(notes)) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  count(unit, region, notes, sort = T) |&gt; \n  group_by(unit, region) |&gt; \n  summarise(\n    total_indiv_correct = sum(n[str_detect(notes, \"correct\")]),\n    total_indiv_legibility = sum(n[str_detect(notes, \"legib\")]),\n    .groups = \"drop\"\n    )\n\nunits_cor_leg |&gt; \n  arrange(total_indiv_correct |&gt; desc())\n\n\n\n  \n\n\n\nExpand this to see code\nunits_cor_leg |&gt; \n  arrange(total_indiv_legibility |&gt; desc())\n\n\n\n  \n\n\n\nLet’s look at bit more at Telford to see the legibility issues.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  filter(unit == \"Telford\" & !is.na(notes)) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  count(notes, sort = T)\n\n\n\n  \n\n\n\nAnd then Glossbrenner …\n\n\nExpand this to see code\nlogs_notes |&gt; \n  filter(unit == \"Glossbrenner\" & !is.na(notes)) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  count(notes, sort = T)\n\n\n\n  \n\n\n\n\n\n\nTA: Notes by unit\nThe units with the most correction notes include Garza West, Briscoe, Stevenson, Connally, Dominguez. Given what we found with Stevenson, this may mean they are more accurate, but they should be reviewed.\nWhen it comes to legibility, the Telford unit stands out. Most of the issues are around the signature of the person recording the record.",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#unit-notes",
    "href": "02-outdoor-readability.html#unit-notes",
    "title": "Outdoor log readability",
    "section": "Unit notes",
    "text": "Unit notes\nIn some cases we recorded an overall note about the unit as opposed to notes for each individual line. I’m just printing all of these out for perusal.\n\n\nExpand this to see code\noverall_notes &lt;- read_rds(\"data-processed/01-outdoor-notes.rds\")\n\noverall_notes |&gt; datatable()",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html",
    "href": "02-indoor-analysis.html",
    "title": "Indoor logs analysis",
    "section": "",
    "text": "Tasks for Emily on this:\n\nMake sure there isn’t anything important from your previous work that isn’t in the newer section.\nGo through the newer section and make sure you understand it. Read through the takeaways and correct as needed. Add anything you might thing is important that I might have missed.\nRemove all my notes that start with &gt;\nWrite a section near the top of the index.qmd that highlights the most important things we find here.\nThen remove this callout section ;-)\nWe have recorded indoor temperatures for each unit April through September of 2022, 2023 and 2024 and April through July 25 (day the info request was filled) of 2025. According to the Texas Department of Criminal Justice, indoor temps are taken at 3pm every day from April through September at all units that do not have air conditioning. The data was acquired by Lauren McGaughy at KUT through a public information request to the Texas Department of Criminal Justice.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#goals-of-this-notebook",
    "href": "02-indoor-analysis.html#goals-of-this-notebook",
    "title": "Indoor logs analysis",
    "section": "Goals of this notebook",
    "text": "Goals of this notebook\nIn this notebook, we will explore the following questions:\n\nWhat was the highest temperature reached each year, and where?\n\nHow many days was the indoor temperature at least X degrees (85, 90, 95, 100+) each year from 2022 to 2025.\nHow many consecutive days in those measures? (Not tackled yet)",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#setup",
    "href": "02-indoor-analysis.html#setup",
    "title": "Indoor logs analysis",
    "section": "Setup",
    "text": "Setup\nLoading our libraries\n\n\nClick to show code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(ggthemes)\nlibrary(DT)\nlibrary(ggwaffle)\n\n# suppress group warning\noptions(dplyr.summarise.inform = FALSE)",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#import",
    "href": "02-indoor-analysis.html#import",
    "title": "Indoor logs analysis",
    "section": "Import",
    "text": "Import\nReading in the cleaned and combined indoor logs.\n\n\nClick to show code\nindoor_logs_clean &lt;- read_rds(\"data-processed/01-indoor-temps-all-cleaned.rds\")\n\n\n\nCreating date parts\nAdding variables to help analysis by date.\n\n\nClick to show code\nindoor_logs &lt;- indoor_logs_clean |&gt; mutate(\n  flo_mo = floor_date(date, unit = \"month\"),\n  year = year(date)\n) \n\nindoor_logs\n\n\n\n  \n\n\n\n\n\nRecords per unit, year\nChecking where we might have missing data …\n\nI rearranged this a little so we could see all years at once. Let’s check on the missing units. I also pulled in the test from the unit analysis, so the most important parts are here and I don’t think we need the rest.\n\n\n\nClick to show code\nindoor_logs |&gt; \n  group_by(unit, year) |&gt; \n  summarize(days_recorded = n()) |&gt; \n  pivot_wider(names_from = year, values_from = days_recorded)\n\n\n\n  \n\n\n\nData takeaway: For 2022, 2023 and 2024, we have 183 total days of indoor temperature recordings. As of Oct. 17, 2025, we only have 122 days of 2025 recordings (April 1 through July 25) because we are waiting for the rest of the data to come in via a public information request.\n\nBartlett: 2025 only\nBaten: 2025 only\nGarza East: 2024 and 2025 only\nGurney: 2025 only\nJohnston: 2022 and 2023 only\nMountain View: 2022 and 2023 only\nO’Daniel: 2024 only\nPlane: 2022, 2023 and 2024 only\nSayle: appears in 2022 only\nSegovia: 2023 (June, July and August and September only), 2024 and 2025 only\nYoung: 2022, 2023 and 2024 only",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#hottest-temps-each-year",
    "href": "02-indoor-analysis.html#hottest-temps-each-year",
    "title": "Indoor logs analysis",
    "section": "Hottest temps each year",
    "text": "Hottest temps each year\nThe top 5 hottest days indoors for each year.\n\n\nClick to show code\nindoor_logs |&gt; \n  group_by(year) |&gt; \n  slice_max(temperature, n = 5) |&gt; \n  select(year, unit, date, temperature)",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#temp-ranges-by-month-year",
    "href": "02-indoor-analysis.html#temp-ranges-by-month-year",
    "title": "Indoor logs analysis",
    "section": "Temp ranges by month, year",
    "text": "Temp ranges by month, year\nThis shows how many days a unit reached a specific indoor temperature each month.\n\n\nClick to show code\ncount_temps_unit_month &lt;- indoor_logs|&gt; \n  group_by(unit, flo_mo) |&gt; \n  summarize(\n    total_days = n(),\n    cnt_85 = sum(temperature &gt;= 85, na.rm = T),\n    cnt_90 = sum(temperature &gt;= 90, na.rm = T),\n    cnt_95 = sum(temperature &gt;= 95, na.rm = T),\n    cnt_100 = sum(temperature &gt;= 100, na.rm = T),\n    cnt_105 = sum(temperature &gt;= 105, na.rm = T),\n    .groups = \"drop\"\n  )\n\ncount_temps_unit_month",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#temp-ranges-by-unit-year",
    "href": "02-indoor-analysis.html#temp-ranges-by-unit-year",
    "title": "Indoor logs analysis",
    "section": "Temp ranges by unit, year",
    "text": "Temp ranges by unit, year\nNumber of days unit reached a certain temperature within a year (which is really a 6-month period). Also the percentage of recorded days it was reached.\n\n\nClick to show code\ncount_temps_unit_year &lt;- indoor_logs |&gt; \n  group_by(unit, year) |&gt; \n  summarize(\n    total_days = n(),\n    cnt_n85 = sum(temperature &lt; 85, na.rm = T),\n    cnt_85 = sum(temperature &gt;= 85, na.rm = T),\n    cnt_90 = sum(temperature &gt;= 90, na.rm = T),\n    cnt_95 = sum(temperature &gt;= 95, na.rm = T),\n    cnt_100 = sum(temperature &gt;= 100, na.rm = T),\n    .groups = \"drop\"\n  ) |&gt; mutate(\n    pct_85 = cnt_85/total_days,\n    pct_90 = cnt_90/total_days,\n    pct_95 = cnt_95/total_days,\n    pct_100 = cnt_100/total_days\n  ) |&gt; \n  mutate(across(starts_with(\"pct\"), round, 2)) # rounds percentage\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(starts_with(\"pct\"), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nClick to show code\ncount_temps_unit_year |&gt; arrange(cnt_100 |&gt; desc())",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#temp-ranges-by-year",
    "href": "02-indoor-analysis.html#temp-ranges-by-year",
    "title": "Indoor logs analysis",
    "section": "Temp ranges by year",
    "text": "Temp ranges by year\nHow many “prison days” it reached a certain temperature each year.\n\n\nClick to show code\ncount_temps_year &lt;- indoor_logs|&gt; \n  group_by(year) |&gt; \n  summarize(\n    total_days = n(),\n    cnt_85 = sum(temperature &gt;= 85, na.rm = T),\n    cnt_90 = sum(temperature &gt;= 90, na.rm = T),\n    cnt_95 = sum(temperature &gt;= 95, na.rm = T),\n    cnt_100 = sum(temperature &gt;= 100, na.rm = T)\n  ) |&gt; mutate(\n    pct_85 = cnt_85/total_days,\n    pct_90 = cnt_90/total_days,\n    pct_95 = cnt_95/total_days,\n    pct_100 = cnt_100/total_days\n  )  |&gt; \n  mutate(across(starts_with(\"pct\"), round, 2)) # rounds percentage\n\ncount_temps_year  \n\n\n\n  \n\n\n\nData takeaways:\n\nAcross all prisons in the data in from 2022-2024, it reached 85 degrees at least half the time.\nIt reached 90 degrees at least a quarter of the time. (Doesn’t include 2025 since we don’t have full data.)\nIt reached 100 degrees inside prisons more than 100 times in 2023.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#crit",
    "href": "02-indoor-analysis.html#crit",
    "title": "Indoor logs analysis",
    "section": "Crit",
    "text": "Crit\n\nI’ve reworked much of this for two reasons: a) to organize by threshold first, then each summary grouped by year. This makes it easier to see changes over time. b) I wanted a code result for each takeaway.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#degree-threshold",
    "href": "02-indoor-analysis.html#degree-threshold",
    "title": "Indoor logs analysis",
    "section": "85 degree threshold",
    "text": "85 degree threshold\n\nCount units reaching 85 at least once\nStarting with the 85 threshold data above, we find total units, then subtract a count units that didn’t reach 85.\n\n\nClick to show code\ncount_temps_unit_year |&gt;\n  group_by(year) |&gt;   \n  summarise(\n    total_units = n(),\n    units_met_85 = total_units - sum(cnt_85 == 0),\n  )\n\n\n\n  \n\n\n\nTakeaway: Every unit hit 85 degrees at least once every year with one exception in 2022.\nLet’s find that unit that didn’t reach 85 degrees. These are the lowest of the “highest temperature for each unit” where we can see Sayle didn’t reach 85 degrees twice. We only have 2022 data for the Sayle unit.\n\n\nClick to show code\nindoor_logs |&gt; \n  filter(year == 2022) |&gt;\n  group_by(unit) |&gt; \n  slice_max(temperature) |&gt; # find highest temp each unit\n  arrange(temperature) |&gt; # find lowest of those\n  select(unit, date, temperature) |&gt; \n  filter(temperature &lt;= 90) # cutting here to show the next units for comparison\n\n\n\n  \n\n\n\n\n\nUnits reaching 85 half the time\nThis counts the number of units that reached 85 degrees for half of the days measured. We also calculate for 75% of the days measured.\n\n\nClick to show code\ncount_temps_unit_year |&gt;\n  group_by(year) |&gt;\n  select(year, total_days, cnt_85, pct_85) |&gt; arrange(desc(cnt_85)) |&gt; \n  summarize(\n    total_units = n(),\n    cnt_half_85 = sum(pct_85 &gt;= .50),\n    cnt_34ths_85 = sum(pct_85 &gt;= .75)\n  )\n\n\n\n  \n\n\n\nExample takeaways:\n\nIn 2022, 53 of 68 units reached 85 degrees at least half of the days measured.\nTen units reached 85 degrees on 75% of days measured in 2023.\n\n\n\nUnits most at 85\nThis shows units each year with the most days to reach 85.\n\n\nClick to show code\ncount_temps_unit_year |&gt; \n  group_by(year) |&gt; \n  slice_max(pct_85, n = 3) |&gt; # n argument adjust number per year\n  select(unit, year, total_days, cnt_85, pct_85)\n\n\n\n  \n\n\n\nTakeaways:\n\nIn the Segovia unit in 2023, 94% of the days reached 85 degrees.\nGarza West reached 85 degrees on at least 80% of days each year since 2022.\n\n\n\nMinimum days 85 threshold met\nHere we find the fewest number of days it reached the 85-degree threshold within each year. We remove the Sayle unit that didn’t ever reach 85 in 2022.\n\n\nClick to show code\ncount_temps_unit_year |&gt;\n  filter(cnt_85 &gt; 0) |&gt; # removes Sayle in 2022\n  group_by(year) |&gt; \n  slice_min(cnt_85, n = 1) |&gt; \n  select(unit, year, total_days, cnt_85, pct_85)\n\n\n\n  \n\n\n\nTakeaways\n\nWith the exception of the Sayle unit, it reached 85 degrees inside every other unit for at least 43 days in 2022.\nIn 2023, every unit saw at least 40 days reaching 85 degrees.\nEven in the relatively cool 2024 summer, it reached 85 degrees inside every unit at least 27 times.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#degree-threshold-1",
    "href": "02-indoor-analysis.html#degree-threshold-1",
    "title": "Indoor logs analysis",
    "section": "90 degree threshold",
    "text": "90 degree threshold\n\nUnits reaching 90 at least once\n\n\nClick to show code\ncount_temps_unit_year |&gt;\n  group_by(year) |&gt;   \n  summarise(\n    total_units = n(),\n    units_met_90 = total_units - sum(cnt_90 == 0),\n  )\n\n\n\n  \n\n\n\nAlmost every unit also reached 90 degrees inside at least once each year. All but three units in 2022 and one in 2023.\n\n\nUnits that hit 90° half the time\nThis counts the number of units that reached 90 degrees for half of the days measured. We also calculate for 75% of the days measured.\n\n\nClick to show code\ncount_temps_unit_year |&gt;\n  group_by(year) |&gt;\n  select(year, total_days, cnt_90, pct_90) |&gt; arrange(desc(cnt_90)) |&gt; \n  summarize(\n    total_units = n(),\n    cnt_half_90 = sum(pct_90 &gt;= .50),\n    cnt_34th_90 = sum(pct_90 &gt;= .75)\n  )\n\n\n\n  \n\n\n\nExample takeaway:\n\nIn 2023, 19 of 68 units reached 90 degrees at least half of the days measured.\nTwo units reached 90 degrees for at least 75% of days measured, one in 2023 and one in 2024.\n\nLet’s find those units that reach 90 for 75% of the time.\n\n\nClick to show code\ncount_temps_unit_year |&gt; \n  # filter(year == 2023) |&gt; \n  select(unit, year, total_days, cnt_90, pct_90) |&gt; \n  filter(pct_90 &gt;= .75) |&gt; \n  arrange(year)\n\n\n\n  \n\n\n\nSegovia has fewer days measured in 2023, but it still reached 90 degrees more than 100 times.\n\n\nUnits with most days at 90\nThis shows units with the most days to reach 95 within each year.\n\n\nClick to show code\nunits_at_90 &lt;- count_temps_unit_year |&gt; \n  group_by(year) |&gt; \n  slice_max(pct_90, n = 3) |&gt; # n argument adjust number per year\n  select(unit, year, total_days, cnt_90, pct_90)\n\nunits_at_90\n\n\n\n  \n\n\n\nClick to show code\nunits_at_90 |&gt; filter(unit == \"Garza West\")\n\n\n\n  \n\n\n\nTakeaways:\n\nIn the Garza West unit, the temperature reached at least 90 degrees at least 3 out of 5 days each year from 2022 to 2024. In 2024 it was 3 out of 4 days measured between April and September.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#threshold",
    "href": "02-indoor-analysis.html#threshold",
    "title": "Indoor logs analysis",
    "section": "95 threshold",
    "text": "95 threshold\n\nUnits reaching 95 at least once\n\n\nClick to show code\ncount_temps_unit_year |&gt;\n  group_by(year) |&gt;   \n  summarise(\n    total_units = n(),\n    units_met_95 = total_units - sum(cnt_95 == 0),\n  )\n\n\n\n  \n\n\n\nTakeaway: In each year from 2022-2024, at least 50 units reached 95 degrees inside.\n\n\nUnits that reach 95 quarter of time\nHere we calculate the number of units that reach 95 degrees at least a quarter of recorded days each year. This is a lower percentage than other thresholds.\n\n\nClick to show code\ncount_temps_unit_year |&gt;\n  group_by(year) |&gt; \n  summarize(\n    total_units = n(),\n    cnt_quarter_95 = sum(pct_95 &gt;= .25)\n  )\n\n\n\n  \n\n\n\nIn 2023, there were 13 units where it reached 95 degrees inside a quarter of the time between April and September.\n\n\nUnits most at 95\nThis shows units with the most days to reach 95 within each year.\n\n\nClick to show code\nunits_at_95 &lt;- count_temps_unit_year |&gt; \n  group_by(year) |&gt; \n  slice_max(pct_95, n = 1) |&gt; # n argument adjust number per year\n  select(unit, year, total_days, cnt_95, pct_95)\n\nunits_at_95\n\n\n\n  \n\n\n\nTakeaway: At the Garza West in 2023 it reached 95 degrees more than a 100 times.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#threshold-1",
    "href": "02-indoor-analysis.html#threshold-1",
    "title": "Indoor logs analysis",
    "section": "100 threshold",
    "text": "100 threshold\n\nUnits reaching 100\n\n\nClick to show code\ncount_temps_unit_year |&gt;\n  group_by(year) |&gt;   \n  summarise(\n    total_units = n(),\n    units_met_100 = total_units - sum(cnt_100 == 0),\n  )\n\n\n\n  \n\n\n\nTakeaways:\n\nBetween 2022 and 2025, each year more than a dozen units reach 100 degrees inside at least once.\n\n\nPlot attempt\n\nThis section is experimental\n\nThis attempts to show how many units reached 100 degrees at least once in a given year.\nSince we’ll want to make this chart for several years, I’ll make it into a function to reuse.\n\n\nClick to show code\nprison_waffle &lt;- function(.data, yr) {\n  .data |&gt; \n    select(unit, year, cnt_100) |&gt; \n    mutate(r_100 = if_else(cnt_100 &gt; 0, \"At least 100°\", \"High below 100°\")) |&gt; \n    filter(year == yr) |&gt;\n    waffle_iron(aes_d(group = r_100)) |&gt; \n    ggplot(aes(x, y, fill = group)) + \n    geom_waffle() +\n    theme_waffle() +\n    labs(\n      title = paste(\"Number of units to reach 100 in\", yr),\n      x = \"\", y = \"\", fill = \"High temp\"\n    )\n}\n\ncount_temps_unit_year |&gt; prison_waffle(2022)\n\n\n\n\n\n\n\n\n\nClick to show code\ncount_temps_unit_year |&gt; prison_waffle(2023)\n\n\n\n\n\n\n\n\n\nClick to show code\ncount_temps_unit_year |&gt; prison_waffle(2024)\n\n\n\n\n\n\n\n\n\n\n\n\nUnits that hit 100 inside\nHow many times each year a unit hit 100 degrees inside.\n\n\nClick to show code\nunit_days_100 &lt;- count_temps_unit_year |&gt; \n  filter(cnt_100 &gt; 0) |&gt; \n  select(unit, year, total_days, cnt_100, pct_100) |&gt; \n  arrange(year, cnt_100 |&gt; desc())\n\nunit_days_100\n\n\n\n  \n\n\n\nLet’s date that and peek at just Garza West\n\n\nClick to show code\nunit_days_100 |&gt; filter(unit == \"Garza West\")\n\n\n\n  \n\n\n\nTakeaway: Garza West reached 100 degrees inside the unit on 46 days in 2023.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#plot-thresholds-by-year",
    "href": "02-indoor-analysis.html#plot-thresholds-by-year",
    "title": "Indoor logs analysis",
    "section": "Plot thresholds by year",
    "text": "Plot thresholds by year\nHere we’ll attempt to plot how many days a unit reached a certain temperature within a given year. We need to recalculate our thresholds.\nHere we bin the number of days at certain temperatures, which is different than we we’ve been doing previously (which was total days at/above a temp).\n\n\nClick to show code\ntemp_thrsh_unit_year &lt;- indoor_logs |&gt; \n  group_by(unit, year) |&gt; \n  summarize(\n    cnt_n85 = sum(temperature &lt; 85, na.rm = T),\n    cnt_85 = sum(temperature &gt;= 85 & temperature &lt; 90, na.rm = T),\n    cnt_90 = sum(temperature &gt;= 90 & temperature &lt; 95, na.rm = T),\n    cnt_95 = sum(temperature &gt;= 95 & temperature &lt; 100, na.rm = T),\n    cnt_100 = sum(temperature &gt;= 100, na.rm = T),\n    .groups = \"drop\"\n  )\n\ntemp_thrsh_unit_year\n\n\n\n  \n\n\n\nTo plot this we have to reshape the data, then plot as a bar chart. We save the chart before displaying it so we can control the size since there are so many units to show.\nThe plotting has been turned into a function to keep it dRy.\n\n\nClick to show code\n# pivoting the data\nthresh_data &lt;- temp_thrsh_unit_year |&gt; \n  pivot_longer(\n    cols = starts_with(\"cnt\"),\n    names_to = \"threshold\",\n    values_to = \"days\"\n  ) |&gt; \n  mutate(threshold = fct_inorder(threshold))\n\n# plot the data into an object and then save that object into the figures folder.\nplot_thresh_data &lt;- function(yr) {\n  thresh_plot &lt;- thresh_data |&gt; \n  filter(year == yr) |&gt; \n  ggplot(aes(x = days, y = fct_rev(unit), fill = threshold)) +\n  geom_bar(stat = \"identity\", position = position_stack(reverse = TRUE)) +\n  scale_fill_brewer(\n    palette='OrRd',\n    labels = c(\n      cnt_n85  = \"Under 85°\",\n      cnt_85   = \"85° to 89°\",\n      cnt_90   = \"90° to 94°\",\n      cnt_95   = \"95° to 99°\",\n      cnt_100  = \"100° or more\"\n    )) +\n  # theme(legend.position = \"bottom\") +\n  labs(\n    title = paste(\"Tracking indoor heat in prisons,\", yr),\n    x = \"Number of days at each temperature\", y = \"\",\n    fill = \"Indoor high\"\n  )\n  \n  ggsave(paste0(\"figures/thresh_plot_\", yr, \".png\"), width = 6, height = 12)\n\n}\n\nplot_thresh_data(2022)\nplot_thresh_data(2023)\nplot_thresh_data(2024)\nplot_thresh_data(2025)\n\n\nWe’ll print out each of the plots, though they will be big.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#emily",
    "href": "02-indoor-analysis.html#emily",
    "title": "Indoor logs analysis",
    "section": "Emily",
    "text": "Emily\nI started to rearrange and rename what was here, but decided we needed to look at each threshold first, then see how those changed over time (instead of breaking it up by year first.) I’m keeping this for now but",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#plotting-days-above-each-temp-per-unit-per-year",
    "href": "02-indoor-analysis.html#plotting-days-above-each-temp-per-unit-per-year",
    "title": "Indoor logs analysis",
    "section": "Plotting days above each temp per unit per year",
    "text": "Plotting days above each temp per unit per year\nNote: there is a substantial dip in the number of days recorded in 2025 because the dataset is only complete through July 25.\n\nEdits on first title, subtitle for conciseness. Would want to carry through. Perhaps visualize these differently, but I’m not sure how yet.\n\n\n85° and above\n\n\nClick to show code\ncount_temps_unit_year |&gt; \n  # filter(year != 2025) |&gt; \n  ggplot(aes(x = year, y= cnt_85, group = unit)) +\n    geom_line(aes(group = unit)) +\n    theme(legend.position = \"none\") +\n    labs(\n      title = \"Number of days each prison reached 85°F\",\n      subtitle = str_wrap(\"Indoor temperatures recorded from April 1 to Sept. 30, taken at 3 p.m. in units without air conditioning. 2025 data is incomplete (122 days vs 183 days in other years.\"),\n      x = \"Year\", y = \"Days at or above 85°F\",\n      caption = \"Source: Texas Department of Criminal Justice\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n90° and above\n\n\nClick to show code\nggplot(\n  count_temps_unit_year,\naes(x = year, y= cnt_90, group = unit)\n) +\ngeom_line(aes(group = unit)) +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"Number of days with indoor temperature recordings at or above 90°F\",\n    # subtitle = str_wrap(\"According to the Texas Department of Criminal Justice, indoor temperature recordings are taken at 3 p.m. every day from April 1 to Sept. 30 at all units that do not have air conditioning. There are 183 days of temperature recordings in 2022, 2023 and 2024, and 122 days of recordings in 2025.\"),\n    x = \"Year\", y = \"Number of days at or above 90°F\",\n    caption = \"Source: Texas Department of Criminal Justice\n    2025 has fewer days\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n95° and above\n\n\nClick to show code\nggplot(\n  count_temps_unit_year,\naes(x = year, y= cnt_95, group = unit)\n) +\ngeom_line(aes(group = unit)) +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"Number of days with indoor temperature recordings at or above 95°F\",\n    # subtitle = str_wrap(\"According to the Texas Department of Criminal Justice, indoor temperature recordings are taken at 3 p.m. every day from April 1 to Sept. 30 at all units that do not have air conditioning. There are 183 days of temperature recordings in 2022, 2023 and 2024, and 122 days of recordings in 2025.\"),\n    x = \"Year\", y = \"Number of days at or above 95°F\",\n    caption = \"Source: Texas Department of Criminal Justice\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n100° and above\n\n\nClick to show code\nggplot(\n  count_temps_unit_year,\naes(x = year, y= cnt_100, group = unit)\n) +\ngeom_line(aes(group = unit)) +\n  theme(legend.position = \"none\") +\n  labs(\n    title = \"Number of days with indoor temperature recordings at or above 100°F\",\n    # subtitle = str_wrap(\"According to the Texas Department of Criminal Justice, indoor temperature recordings are taken at 3 p.m. every day from April 1 to Sept. 30 at all units that do not have air conditioning. There are 183 days of temperature recordings in 2022, 2023 and 2024, and 122 days of recordings in 2025.\"),\n    x = \"Year\", y = \"Number of days at or above 100°F\",\n    caption = \"Source: Texas Department of Criminal Justice\"\n  ) +\n  theme_bw()",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#at-85",
    "href": "02-indoor-analysis.html#at-85",
    "title": "Indoor logs analysis",
    "section": "2022 at 85°",
    "text": "2022 at 85°\nFiltering the indoor log data to include only readings taken in 2022 where the temperature was above 85°F.\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2022) |&gt; select(year, total_days, cnt_85, pct_85) |&gt; arrange(desc(cnt_85))\n\n\n\n  \n\n\n\n\nIn 2022, 67 out of 68 units without air conditioning recorded indoor temperatures at or above 85 degrees for at least 43 days between April 1 and Sept. 30.\nThe Sayle unit, located in Breckenridge, was the only unit to not record temperatures at or above 85 degrees.\n51 units recorded indoor temperatures at or above 85 degrees for over 50% of days, or at least 92 days.\nTen units recorded indoor temperatures at or above 85 degrees for over 75% of days, or at least 138 days.\nThe Garza West unit, located in Beeville, recorded the most days with indoor temperatures at or above 85 degrees, with 168 days, or 92% of days between April 1 and Sept. 30.\n[I DONT UNDERSTAND THIS ONE] Of the days where temperatures were measured in all prisons across the year, only 38.4% didn’t reach 85 degrees.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#at-90",
    "href": "02-indoor-analysis.html#at-90",
    "title": "Indoor logs analysis",
    "section": "2022 at 90°",
    "text": "2022 at 90°\nFiltering the indoor log data to include only readings taken in 2022 where the temperature was above 90°F.\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2022) |&gt; select(year, total_days, cnt_90, pct_90) |&gt; arrange(desc(cnt_90)) |&gt; filter(pct_90 &gt;= .5)\n\n\n\n  \n\n\n\nData takeaway: In 2022, 65 out of 68 units without air conditioning recorded indoor temperatures at or above 90 degrees at least once between April 1 and Sept. 30. 41 units recorded indoor temperatures at or above 90 degrees for over 25% of days, or at least 46 days. Nine units recorded indoor temperatures at or above 90 degrees for over 50% of days, or at least 92 days. The Dominguez unit, located in San Antonio, Texas, recorded the most days with indoor temperatures at or above 90 degrees, with 122 days, or two-thirds of days between April 1. and Sept. 30. Of the days where temperatures were measured in all prisons across the year, 30.6% reached 90 degrees.\n\n\nClick to show code\n# crit\ncount_temps_unit_year |&gt;\n  group_by(year) |&gt;   \n  summarise(\n    total_days = n(),\n    days_not_90 = sum(cnt_90 == 0),\n    days_met_90 = total_days - days_not_90\n  )",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#at-95",
    "href": "02-indoor-analysis.html#at-95",
    "title": "Indoor logs analysis",
    "section": "2022 at 95°",
    "text": "2022 at 95°\nFiltering the indoor log data to include only readings taken in 2022 where the temperature was above 95°F.\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2022) |&gt; select(year, total_days, cnt_95, pct_95) |&gt; arrange(desc(cnt_95)) |&gt; filter(pct_95 &gt;= .25)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2022) |&gt; select(year, total_days, cnt_95, pct_95)\n\n\n\n  \n\n\n\nData takeaway: In 2022, 51 out of 68 units without air conditioning recorded indoor temperatures at or above 95 degrees at least once between April 1 and Sept. 30. Six units recorded indoor temperatures at or above 95 degrees for over 25% of days, or at least 46 days. The Stevenson unit, located in Cuero, Texas, recorded the most days with indoor temperatures at or above 95 with 74 days, or over 40% of days between April 1. and Sept. 30. Of the days where temperatures were measured in all prisons across the year, 8.1% reached 95 degrees.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#at-100",
    "href": "02-indoor-analysis.html#at-100",
    "title": "Indoor logs analysis",
    "section": "2022 at 100°",
    "text": "2022 at 100°\nFiltering the indoor log data to include only readings taken in 2022 where the temperature was above 100°F.\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2022) |&gt; select(year, total_days, cnt_100, pct_100) |&gt; arrange(desc(cnt_100)) |&gt; filter(cnt_100 &gt;= 1)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2022) |&gt; select(year, total_days, cnt_100, pct_100)\n\n\n\n  \n\n\n\nData takeaway: In 2022, 15 out of 68 units without air conditioning recorded indoor temperatures at or above 100 degrees at least once between April 1 and Sept. 30. The Stevenson unit, located in Cuero, Texas, recorded the most days with indoor temperatures at or above 100 degrees with 13 days between April 1 and Sept. 30. This includes four consecutive days between July 10 and July 13.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#at-85-1",
    "href": "02-indoor-analysis.html#at-85-1",
    "title": "Indoor logs analysis",
    "section": "2023 at 85°",
    "text": "2023 at 85°\nFiltering the indoor log data to include only readings taken in 2023 where the temperature was above 85°F.\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2023) |&gt; select(year, total_days, cnt_85, pct_85) |&gt; arrange(desc(cnt_85)) \n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2023) |&gt; select(year, total_days, cnt_85, pct_85)\n\n\n\n  \n\n\n\nData takeaway: In 2023, all 68 units without air conditioning recorded indoor temperatures at or above 85 degrees for at least 40, or 21.9% of days between April 1 and Sept. 30. 54 units recorded temperatures at or above 85 for over 50% of recorded days, and seven units recorded temperatures at or above 85 for over 75% of days. The Lopez unit, located in Edinburg, Texas just north of McAllen, recorded the the most days with indoor temperatures at or above 85, with 152, or 83.1% of days. Of the days where temperatures were measured in all prisons across the year, only 39.4% didn’t reach 85 degrees.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#at-90-1",
    "href": "02-indoor-analysis.html#at-90-1",
    "title": "Indoor logs analysis",
    "section": "2024 at 90",
    "text": "2024 at 90\nFiltering the indoor log data to include only readings taken in 2023 where the temperature was above 90°F.\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2023) |&gt; select(year, total_days, cnt_90, pct_90) |&gt; arrange(desc(cnt_90)) \n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2023) |&gt; select(year, total_days, cnt_90, pct_90)\n\n\n\n  \n\n\n\nData takeaway: In 2023, 67 out of 68 units without air conditioning recorded indoor temperatures at or above 90 degrees for at least 9, or 5% of days between April 1 and Sept. 30. Just one unit did not record temperatures at or above 90, the Clements unit located east of Amarillo, Texas. 50 units recorded temperatures at or above 90 for over 25% of recorded days, and 18 units recorded temperatures at or above 90 for over 50% of days. The Garza West unit, located near Beeville, Texas, recorded the the most days with indoor temperatures at or above 90, with 133, or 72.7% of days. Of the days where temperatures were measured in all prisons across the year, 36.8% reached 90 degrees.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#days-at-or-above-95",
    "href": "02-indoor-analysis.html#days-at-or-above-95",
    "title": "Indoor logs analysis",
    "section": "Days at or above 95",
    "text": "Days at or above 95\nFiltering the indoor log data to include only readings taken in 2023 where the temperature was above 95°F.\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2023) |&gt; select(year, total_days, cnt_95, pct_95) |&gt; arrange(desc(cnt_95)) |&gt; filter(pct_95 &gt;= .5)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2023) |&gt; select(year, total_days, cnt_95, pct_95)\n\n\n\n  \n\n\n\nData takeaway: In 2023, 61 out of 68 units without air conditioning recorded indoor temperatures at or above 95 degrees for at least 1 day between April 1 and Sept. 30. 13 units recorded temperatures at or above 95 for at least 48 days, or 25.2% of the time. Garza West recorded the the most days with indoor temperatures at or above 95, with 108, or 59% of days. Of the days where temperatures were measured in all prisons across the year, 11.5% reached 95 degrees.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#days-at-or-above-100",
    "href": "02-indoor-analysis.html#days-at-or-above-100",
    "title": "Indoor logs analysis",
    "section": "Days at or above 100",
    "text": "Days at or above 100\nFiltering the indoor log data to include only readings taken in 2023 where the temperature was above 100°F.\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2023) |&gt; select(year, total_days, cnt_100, pct_100) |&gt; arrange(desc(cnt_100)) |&gt; filter(cnt_100 &gt;= 1)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2023) |&gt; select(year, total_days, cnt_100, pct_100)\n\n\n\n  \n\n\n\nData takeaway: In 2023, 18 out of 68 units without air conditioning recorded indoor temperatures at or above 100 degrees for at least 1 day between April 1 and Sept. 30. Garza West recorded the most days with indoor temperatures at or above 100, with 46, or 25.1% of days.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#indoor-logs-analysis",
    "href": "02-indoor-analysis.html#indoor-logs-analysis",
    "title": "Indoor logs analysis",
    "section": "2024 indoor logs analysis",
    "text": "2024 indoor logs analysis",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#days-at-or-above-85",
    "href": "02-indoor-analysis.html#days-at-or-above-85",
    "title": "Indoor logs analysis",
    "section": "Days at or above 85",
    "text": "Days at or above 85\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2024) |&gt; select(year, total_days, cnt_85, pct_85) |&gt; arrange(desc(cnt_85)) |&gt; filter(pct_85 &gt;= .75)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2024) |&gt; select(year, total_days, cnt_85, pct_85)\n\n\n\n  \n\n\n\nData takeaway: In 2024, all 68 units without air conditioning recorded indoor temperatures at or above 85 degrees for at least 27, or 14.8% of days between April 1 and Sept. 30. 46 units recorded temperatures at or above 85 for over 50% of recorded days, and eight units recorded temperatures at or above 85 for over 75% of days. Garza West recorded the most days with indoor temperatures at or above 85, with 163, or 89.1% of days. Of the days where temperatures were measured in all prisons across the year, 56.4% reached 85 degrees.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#days-at-or-above-90",
    "href": "02-indoor-analysis.html#days-at-or-above-90",
    "title": "Indoor logs analysis",
    "section": "Days at or above 90",
    "text": "Days at or above 90\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2024) |&gt; select(year, total_days, cnt_90, pct_90) |&gt; arrange(desc(cnt_90)) |&gt; filter(pct_90 &gt;= .5)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2024) |&gt; select(year, total_days, cnt_90, pct_90)\n\n\n\n  \n\n\n\nData takeaway: In 2024, all 68 units without air conditioning recorded indoor temperatures at or above 90 degrees for at least one day between April 1 and Sept. 30. 33 units recorded temperatures at or above 90 for over 25% recorded days, and five units recorded temperatures at or above 85 for over 50% of days. Garza West recorded the most days with indoor temperatures at or above 90, with 137, or 74.8% of days. Of the days where temperatures were measured in all prisons across the year, 25.5% reached 90 degrees.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#days-at-or-above-95-1",
    "href": "02-indoor-analysis.html#days-at-or-above-95-1",
    "title": "Indoor logs analysis",
    "section": "Days at or above 95",
    "text": "Days at or above 95\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2024) |&gt; select(year, total_days, cnt_95, pct_95) |&gt; arrange(desc(cnt_95)) |&gt; filter(pct_95 &gt;= .25)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2024) |&gt; select(year, total_days, cnt_95, pct_95)\n\n\n\n  \n\n\n\nData takeaway: In 2024, 52 out of 68 units without air conditioning recorded indoor temperatures at or above 95 degrees for at least one day between April 1 and Sept. 30. Two units recorded temperatures at or above 90 for over 25% recorded days. Garza West recorded the most days with indoor temperatures at or above 95, with 98, or 53.6% of days. Of the days where temperatures were measured in all prisons across the year, 5.5% reached 95 degrees.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#days-at-or-above-100-1",
    "href": "02-indoor-analysis.html#days-at-or-above-100-1",
    "title": "Indoor logs analysis",
    "section": "Days at or above 100",
    "text": "Days at or above 100\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2024) |&gt; select(year, total_days, cnt_100, pct_100) |&gt; arrange(desc(cnt_100)) |&gt; filter(cnt_100 &gt;= 1)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2024) |&gt; select(year, total_days, cnt_100, pct_100)\n\n\n\n  \n\n\n\nData takeaway: In 2024, 12 out of 68 units without air conditioning recorded indoor temperatures at or above 100 degrees for at least 1 day between April 1 and Sept. 30. Garza West recorded the most days with indoor temperatures at or above 100, with 16, or 8.7% of days.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#indoor-log-analysis",
    "href": "02-indoor-analysis.html#indoor-log-analysis",
    "title": "Indoor logs analysis",
    "section": "2025 indoor log analysis",
    "text": "2025 indoor log analysis\nNote: As of Nov. 24, we do not have data for 2025 past July 25. The code below will be updated and the data takeaways will be written when this data comes in.",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#days-at-or-above-85-1",
    "href": "02-indoor-analysis.html#days-at-or-above-85-1",
    "title": "Indoor logs analysis",
    "section": "Days at or above 85",
    "text": "Days at or above 85\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2025) |&gt; select(year, total_days, cnt_85, pct_85) |&gt; arrange(desc(cnt_85)) |&gt; filter(cnt_85 &gt;= 1)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2025) |&gt; select(year, total_days, cnt_85, pct_85)",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#days-at-or-above-90-1",
    "href": "02-indoor-analysis.html#days-at-or-above-90-1",
    "title": "Indoor logs analysis",
    "section": "Days at or above 90",
    "text": "Days at or above 90\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2025) |&gt; select(year, total_days, cnt_90, pct_90) |&gt; arrange(desc(cnt_90)) |&gt; filter(cnt_90 &gt;= 1)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2025) |&gt; select(year, total_days, cnt_90, pct_90)",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#days-at-or-above-95-2",
    "href": "02-indoor-analysis.html#days-at-or-above-95-2",
    "title": "Indoor logs analysis",
    "section": "Days at or above 95",
    "text": "Days at or above 95\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2025) |&gt; select(year, total_days, cnt_95, pct_95) |&gt; arrange(desc(cnt_95)) |&gt; filter(cnt_95 &gt;= 1)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2025) |&gt; select(year, total_days, cnt_95, pct_95)",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "02-indoor-analysis.html#days-at-or-above-100-2",
    "href": "02-indoor-analysis.html#days-at-or-above-100-2",
    "title": "Indoor logs analysis",
    "section": "Days at or above 100",
    "text": "Days at or above 100\n\n\nClick to show code\ncount_temps_unit_year |&gt; filter(year == 2025) |&gt; select(year, total_days, cnt_100, pct_100) |&gt; arrange(desc(cnt_100)) |&gt; filter(cnt_100 &gt;= 1)\n\n\n\n  \n\n\n\n\n\nClick to show code\ncount_temps_year |&gt; filter(year == 2025) |&gt; select(year, total_days, cnt_100, pct_100)",
    "crumbs": [
      "Analysis",
      "Indoor logs analysis"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html",
    "href": "01-updated-activation-cleaning.html",
    "title": "Updated activations cleaning",
    "section": "",
    "text": "This data is a record of every time a TDCJ was under ICS protocols, like providing extra ice water and access to cooler areas during extreme heat events, in 2022, 2023 and 2024. This data was acquired by Lauren McGaughy of The Texas Newsroom through a public information request to the Texas Department of Criminal Justice.",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#goals-of-this-notebook",
    "href": "01-updated-activation-cleaning.html#goals-of-this-notebook",
    "title": "Updated activations cleaning",
    "section": "Goals of this notebook",
    "text": "Goals of this notebook\nWhat we’ll do to prepare the data:\n\nDownload the data using read_excel\nImport it into our notebook\nClean up data types and columns\nCombine all three years into one data frame\nExport data into our next notebook",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#setup",
    "href": "01-updated-activation-cleaning.html#setup",
    "title": "Updated activations cleaning",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(data.table)\nlibrary(readxl)\nlibrary(hms)",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#downloading-the-data",
    "href": "01-updated-activation-cleaning.html#downloading-the-data",
    "title": "Updated activations cleaning",
    "section": "Downloading the data",
    "text": "Downloading the data\nThis data includes all instances where units were under ICS protocols for 2022, 2023 and 2024, as reported by the TDCJ from a public information request. The data is limited by when the information request was filled each year: 2022/08/07, 2023/09/12 and 2024/08/29.\nDownloading the xlsx files using read_excel, specifying the sheet, skipping the first row, and specifying column types.\n\nactivations_2022_raw &lt;- read_excel(\"data-original/08.07.22 ICS Seasonal Spreadsheet.xlsx\", sheet = \"FY22 ICS\", skip = 1, col_types = c(\"date\", \"date\", \"text\", \"text\", \"date\", \"date\", \"date\", \"date\")) |&gt; \n  clean_names()\n\nNew names:\n• `Date` -&gt; `Date...5`\n• `Time` -&gt; `Time...6`\n• `Date` -&gt; `Date...7`\n• `Time` -&gt; `Time...8`\n\nactivations_2022_raw\n\n\n  \n\n\n\n** Note on 2022 sheet: we reformatted cells F11 and F21 in the original xlsx file because it was formatted as text rather than time and read in as NA. We also corrected\n\nactivations_2023_raw &lt;- read_excel(\"data-original/09.12.23 ICS Seasonal Spreadsheet .xlsx\", sheet = \"FY 23\", skip = 1, col_types = c(\"date\", \"date\", \"text\", \"text\", \"date\", \"numeric\", \"date\", \"numeric\")) |&gt; \n  clean_names()\n\nNew names:\n• `Date` -&gt; `Date...5`\n• `Time` -&gt; `Time...6`\n• `Date` -&gt; `Date...7`\n• `Time` -&gt; `Time...8`\n\nactivations_2023_raw\n\n\n  \n\n\n\n** Note on 2023 sheet: we reformatted cells E24, E92 and B65 in the original xlsx file because the year was recorded as 2023 rather than 23, causing these cells to read in as NA.\n\nactivations_2024_raw &lt;- read_excel(\"data-original/08.29.24_ICS Seasonal Spreadsheet .xlsx\", sheet = \"CY24\", skip = 1, col_types = c(\"date\", \"date\", \"text\", \"text\", \"date\", \"numeric\", \"date\", \"numeric\")) |&gt; \n  clean_names()\n\nNew names:\n• `Date` -&gt; `Date...5`\n• `Time` -&gt; `Time...6`\n• `Date` -&gt; `Date...7`\n• `Time` -&gt; `Time...8`",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#clean-column-names",
    "href": "01-updated-activation-cleaning.html#clean-column-names",
    "title": "Updated activations cleaning",
    "section": "Clean column names",
    "text": "Clean column names\n\nactivations_2022_names &lt;- activations_2022_raw |&gt; select(\n  initial_extreme_temp = initial_date_of_extreme_temperature,\n  initiation_date = ics_implementation_date,\n  county,\n  unit = unit_affected,\n  activation_date = date_5, \n  activation_time = time_6,\n  deactivation_date = date_7,\n  deactivation_time = time_8 \n  )\n\n\nactivations_2023_names &lt;- activations_2023_raw |&gt; select(\n  initial_extreme_temp = initial_date_of_extreme_temperature,\n  initiation_date = ics_implementation_date,\n  county,\n  unit = unit_affected,\n  activation_date = date_5, \n  activation_time = time_6,\n  deactivation_date = date_7,\n  deactivation_time = time_8 \n  )\n\n\nactivations_2024_names &lt;- activations_2024_raw |&gt; select(\n  initial_extreme_temp = initial_date_of_extreme_temperature,\n  initiation_date = ics_implementation_date,\n  county,\n  unit = unit_affected,\n  activation_date = date_5, \n  activation_time = time_6,\n  deactivation_date = date_7,\n  deactivation_time = time_8 \n  )",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#fix-the-2022-log-activation-and-deactivation-time-columns",
    "href": "01-updated-activation-cleaning.html#fix-the-2022-log-activation-and-deactivation-time-columns",
    "title": "Updated activations cleaning",
    "section": "Fix the 2022 log activation and deactivation time columns",
    "text": "Fix the 2022 log activation and deactivation time columns\nBecause the 2022 log had the the activation and deactivation time columns formatted as hh:mm rather than the hhmm format used in the 2023 and 2024 logs, we had to specify those column types as dates for them to read in properly, which generated a date of 1899-12-31 for each time. We’re using mutate() and the hms() function to pull just the time from this datetime.\n\nactivations_2022_clean &lt;- activations_2022_names |&gt; \n  mutate(\n    activation_time = hms(\n    hour = hour(activation_time),\n    minute = minute(activation_time),\n    second = second(activation_time),\n    )\n  ) |&gt; \n  mutate(\n      deactivation_time = hms(\n      hour = hour(deactivation_time),\n      minute = minute(deactivation_time),\n      second = second(deactivation_time)\n    ))",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#reformat-2023-log-activation-and-deactivation-time-columns",
    "href": "01-updated-activation-cleaning.html#reformat-2023-log-activation-and-deactivation-time-columns",
    "title": "Updated activations cleaning",
    "section": "Reformat 2023 log activation and deactivation time columns",
    "text": "Reformat 2023 log activation and deactivation time columns\nIn the 2023 and 2024 logs, the activation and deactivation times were recorded in hhmm format (i.e. 1539 instead of 15:39:00). Reformatting to hh:mm:ss by using str_pad to restore the leading zeros that were removed when we read in the original file with read_excel, using parse_hms from the lubridate package, using substr to specify which digits are hours, minutes and seconds, and removing unnecessary columns.\n\nactivations_2023_time &lt;- activations_2023_names|&gt; \n  mutate(\n    activation_time_new = str_pad(activation_time, width = 4, side = \"left\", pad = \"0\"),\n    deactivation_time_new = str_pad(deactivation_time, width = 4, side = \"left\", pad = \"0\"))\n\n\nactivations_2023_clean &lt;- activations_2023_time |&gt; \n  mutate(\n    activation_time_clean = parse_hms(paste0(substr(activation_time_new, 1, 2), \":\", substr(activation_time_new, 3, 4), \":00\")),\n    deactivation_time_clean = parse_hms(paste0(substr(deactivation_time_new, 1, 2), \":\", substr(deactivation_time_new, 3, 4), \":00\"))\n  ) |&gt; \n  select(initial_extreme_temp,\n         initiation_date,\n         county,\n         unit,\n         activation_date,\n         activation_time = activation_time_clean,\n         deactivation_date,\n         deactivation_time = deactivation_time_clean)\n\nactivations_2023_clean",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#reformat-2024-log-activation-and-deactivation-time-columns",
    "href": "01-updated-activation-cleaning.html#reformat-2024-log-activation-and-deactivation-time-columns",
    "title": "Updated activations cleaning",
    "section": "Reformat 2024 log activation and deactivation time columns",
    "text": "Reformat 2024 log activation and deactivation time columns\nSame process as above but for the 2024 log.\n\nactivations_2024_time &lt;- activations_2024_names|&gt; \n  mutate(\n    activation_time_new = str_pad(activation_time, width = 4, side = \"left\", pad = \"0\"),\n    deactivation_time_new = str_pad(deactivation_time, width = 4, side = \"left\", pad = \"0\"))\n\n\nactivations_2024_clean &lt;- activations_2024_time |&gt; \n  mutate(\n    activation_time_clean = parse_hms(paste0(substr(activation_time_new, 1, 2), \":\", substr(activation_time_new, 3, 4), \":00\")),\n    deactivation_time_clean = parse_hms(paste0(substr(deactivation_time_new, 1, 2), \":\", substr(deactivation_time_new, 3, 4), \":00\"))\n  ) |&gt; \n  select(initial_extreme_temp,\n         initiation_date,\n         county,\n         unit,\n         activation_date,\n         activation_time = activation_time_clean,\n         deactivation_date,\n         deactivation_time = deactivation_time_clean)\n\nactivations_2024_clean",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#combining-three-years-of-activations-logs-into-one-data-frame",
    "href": "01-updated-activation-cleaning.html#combining-three-years-of-activations-logs-into-one-data-frame",
    "title": "Updated activations cleaning",
    "section": "Combining three years of activations logs into one data frame",
    "text": "Combining three years of activations logs into one data frame\n\nactivations_all &lt;- bind_rows(activations_2022_clean, activations_2023_clean, activations_2024_clean)\n\nactivations_all",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#change-date-columns",
    "href": "01-updated-activation-cleaning.html#change-date-columns",
    "title": "Updated activations cleaning",
    "section": "Change date columns",
    "text": "Change date columns\nUsing the as.Date function to convert the POSIX date columns into real date columns.\n\nactivations_all_dates &lt;- activations_all |&gt; \n  mutate(\n    initial_extreme_temp = as.Date(initial_extreme_temp),\n    initiation_date = as.Date(initiation_date),\n    activation_date = as.Date(activation_date),\n    deactivation_date = as.Date(deactivation_date)\n  ) \n\nactivations_all_dates",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#activation-datestimes",
    "href": "01-updated-activation-cleaning.html#activation-datestimes",
    "title": "Updated activations cleaning",
    "section": "Activation dates/times",
    "text": "Activation dates/times\nCode from Prof. McDonald:\nLooking at how many days there typically were between the initial extreme temperature and the activation.\n\nactivations_all_dates |&gt; \n  mutate(activation_gap = initiation_date - initial_extreme_temp,\n         .after = initiation_date) |&gt; \n  arrange(activation_gap |&gt; desc()) |&gt; \n  count(activation_gap) |&gt; \n  adorn_totals(\"row\") |&gt; \n  # as_tibble() |&gt; \n  rename(\n    gap_days = activation_gap,\n    cnt_records = n\n  )\n\n\n  \n\n\n\nOut of 143 activations between 2022-2024, only one of them took place more than the 3rd day after the “initial extreme temp”.",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#document-active-days",
    "href": "01-updated-activation-cleaning.html#document-active-days",
    "title": "Updated activations cleaning",
    "section": "Document active days",
    "text": "Document active days\nCode from Prof. McDonald:\nIn our data, a single row is an activation period with a start and end date. What we want is a row of data for each activated day … so if is three days between the start and end date, we want a row for each of the three days, with a variable activated that includes that date.\nWe do this by creating a new “list column” that includes a sequence of dates starting at activation and ending at deactivation. We then unnest that list to create a row for each item in the list.\n\nunnested_actives &lt;- activations_all_dates |&gt; \n1  mutate(activated = map2(\n2    activation_date, deactivation_date,\n3    ~seq(from = .x, to = .y, by = \"day\")\n  )) |&gt; \n4  unnest(activated)\n\nunnested_actives\n\n\n1\n\nWe create a column called activated, and that will be filled with the result of the map2() function. What map2() allows us to do is take two variables from our data (our dates) and provide it to a function (seq()).\n\n2\n\nHere we list the two variables, our start and end dates.\n\n3\n\nThis seq() function creates a list of dates in sequence by day, starting with our activation_date (.x) and ending with our deactivation_date (.y). The end result is a nested “list column” activated that holds all the dates. At this point we still have one row for each activation period.\n\n4\n\nHere the unnest() function creates a new copy of each row with one of the dates in our activated list column. So if an activation period was 4 days, here we end up for 4 rows of the same data except for the date in activated.",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#clean-up-our-active-days",
    "href": "01-updated-activation-cleaning.html#clean-up-our-active-days",
    "title": "Updated activations cleaning",
    "section": "Clean up our active days",
    "text": "Clean up our active days\nCode from Prof. McDonald:\nHere we simplify our activations data so we can join it with other data. We get just the unit, date and a protocol_active flag.\n\nclean_actives &lt;- unnested_actives |&gt; \n  select(c(unit, date = activated)) |&gt; \n  mutate(protocol_active = T)\n\nclean_actives",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-updated-activation-cleaning.html#export",
    "href": "01-updated-activation-cleaning.html#export",
    "title": "Updated activations cleaning",
    "section": "Export",
    "text": "Export\n\nclean_actives |&gt; write_rds(\"data-processed/01-activation-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Updated activations cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html",
    "href": "01-station-protocols.html",
    "title": "Stations cleaning",
    "section": "",
    "text": "THIS HAS SOME ISSUES. IF WE ARE FOLLOWING THE EXCESSIVE HEAT WARNING CRITERIA, THEN WE ARE NOT LOOKING AT 105 FOR 2 DAYS.\nHere we enhance our weather station data to include prison protocol data. We do this both for our hourly readings and our daily summaries.\nWill decide later if we add the prison info. That may need to be in more specific analysis.\nlibrary(tidyverse)\nlibrary(janitor)",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html#flag-function",
    "href": "01-station-protocols.html#flag-function",
    "title": "Stations cleaning",
    "section": "Flag function",
    "text": "Flag function\nHere we want a function that will create the flags, once fed a tmp and heat index.\n\nset_protocol_flag &lt;- function(df, tmp_name, hi_name) {\n    df |&gt; mutate(\n    tmp_flag = (case_when(\n      tmp_name &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi_name &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == TRUE | hi_flag == TRUE ~ TRUE,\n      .default = FALSE)\n    )\n}",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html#hourly-data",
    "href": "01-station-protocols.html#hourly-data",
    "title": "Stations cleaning",
    "section": "Hourly data",
    "text": "Hourly data\n\nImport hourly data\n\nstations_hourly &lt;- read_rds(\"data-processed/weather-logs/01-station-hourly-readings.rds\")\n\n\n\nAdd protocol flags\n\nstations_hourly_protocol &lt;- stations_hourly |&gt; \n   mutate(\n    tmp_flag = (case_when(\n      tmp &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == TRUE | hi_flag == TRUE ~ TRUE,\n      .default = FALSE)\n    )\n\n# check results\nstations_hourly_protocol |&gt;\n  select(-name) |&gt; \n  filter(tmp &gt; 100) |&gt; \n  slice_sample(n = 10)\n\n\n  \n\n\n\n\n\nExport hourly\n\nstations_hourly_protocol |&gt; \n  write_rds(\"data-processed/01-station-hourly-protocols.rds\")",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html#daily-summarized-data",
    "href": "01-station-protocols.html#daily-summarized-data",
    "title": "Stations cleaning",
    "section": "Daily summarized data",
    "text": "Daily summarized data\n\nImport daily summaries\n\nstations_daily &lt;- read_rds(\"data-processed/weather-logs/01-station-daily-summary.rds\")\n\nstations_daily |&gt; glimpse()\n\nRows: 17,453\nColumns: 5\n$ station_id &lt;chr&gt; \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KA…\n$ name       &lt;chr&gt; \"ABILENE REGIONAL AIRPORT, TX US\", \"ABILENE REGIONAL AIRPOR…\n$ date       &lt;date&gt; 2023-01-01, 2023-01-02, 2023-01-03, 2023-01-04, 2023-01-05…\n$ tmp_high   &lt;dbl&gt; 78, 76, 60, 64, 63, 77, 63, 63, 77, 83, 83, 60, 64, 67, 67,…\n$ hi_high    &lt;dbl&gt; 76.7, 74.8, 56.9, 61.0, 60.2, 76.1, 62.1, 60.5, 75.3, 80.4,…\n\n\n\n\nAdd daily protocols\n\nstations_daily_protocol &lt;- stations_daily |&gt; \n  mutate(\n    tmp_flag = (case_when(\n      tmp_high &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi_high &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == \"TRUE\" | hi_flag == \"TRUE\" ~ TRUE,\n      .default = FALSE)\n  )\n\n# check results\nstations_daily_protocol |&gt; \n  select(-name) |&gt; \n  filter(tmp_high &gt; 100) |&gt; \n  slice_sample(n = 10)\n\n\n  \n\n\n\n\n\nExport daily\n\nstations_daily_protocol |&gt; write_rds(\"data-processed/01-station-daily-protocols.rds\")",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html",
    "href": "01-indoor-all-cleaning.html",
    "title": "All indoor logs cleaning",
    "section": "",
    "text": "We have recorded indoor temperatures for each unit April through September of 2022, 2023 and 2024, and April through July 25 (day the info request was filled) of 2025. This information was acquired by Lauren McGaughy at KUT through a public information request to the Texas Department of Criminal Justice.\nThe original PDFs were converted into xlsx files and downloaded as csv files using Adobe Acrobat, resulting in the csv files found in ‘data-original/indoor_all_csv’ as ‘SB1R56 - April 2022 (Table 1).csv’ and so on for each month and year. The original documents are found in ‘data-original’ under ‘2022_indoor’, ‘2023_indoor’ and so on for each year.",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#goals-of-this-notebook",
    "href": "01-indoor-all-cleaning.html#goals-of-this-notebook",
    "title": "All indoor logs cleaning",
    "section": "Goals of this notebook",
    "text": "Goals of this notebook\nWe have already cleaned the data for each year individually in separate notebooks. In this notebook, we will consolidate the cleaning code for each year of data and combine all the years of data into one data frame. This data comes from a public information request to the Texas Department of Criminal Justice. We have a separate csv file for each month from April to September, so we will import each in individually and pipe it into the function we created above.\nClean and combine the 2022, 2023 and 2024 indoor logs for April, May, June, July, August and September and 2025 logs for April, May, June and part of July.",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#setup",
    "href": "01-indoor-all-cleaning.html#setup",
    "title": "All indoor logs cleaning",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(lubridate)",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#creating-a-function",
    "href": "01-indoor-all-cleaning.html#creating-a-function",
    "title": "All indoor logs cleaning",
    "section": "Creating a function",
    "text": "Creating a function\nThis function streamlines the process of pivoting the data longer, skipping the first row, creating a real date and renaming/selecting columns for each month of data.\n\npivot_date &lt;- function(df, mo, yr) {\n  df |&gt; pivot_longer(\n    !1\n  ) |&gt; \n  mutate(\n    date = mdy(paste(mo, name, yr))\n  ) |&gt; select(\n    unit = 1,\n    date,\n    temperature = value\n  )\n}\n\n*Note: this function leaves a row for each day of the month where unit = “Month Year” and temperature = NA. For the 2022 logs only, unit = “Month Year - Daily Temperature Log”. i.e. for April 2023, there are 30 rows where the unit is “April 2023” and the temperature is NA, one for each day of the month. The only month where this does not apply is April 2022 because of the different way it was read in.",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#indoor-logs",
    "href": "01-indoor-all-cleaning.html#indoor-logs",
    "title": "All indoor logs cleaning",
    "section": "2022 indoor logs",
    "text": "2022 indoor logs\n**Note: for the April 2022 csv only, we do not need to skip the first row because for some reason it converted differently when we put the original pdf into Adobe Acrobat. For all other logs, we will skip the first row in the read_csv() function to avoid formatting problems.",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#downloading-data-and-applying-function",
    "href": "01-indoor-all-cleaning.html#downloading-data-and-applying-function",
    "title": "All indoor logs cleaning",
    "section": "Downloading data and applying function",
    "text": "Downloading data and applying function\n\napril_2022_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - April 2022 (1)(Table 1).csv\") |&gt; pivot_date(\"April\", \"2022\") |&gt; filter(!unit == \"April 2022 - Daily Temperature Log\", !is.na(unit)) \n\nNew names:\nRows: 69 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\napril_2022_clean \n\n\n  \n\n\n\n\nmay_2022_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - May 2022(Table 1).csv\", \n         skip = 1) |&gt; pivot_date(\"May\", \"2022\") |&gt; filter(!unit == \"May 2022 - Daily Temperature Log\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\njune_2022_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - June 2022(Table 1).csv\", skip = 1) |&gt; pivot_date(\"June\", \"2022\") |&gt; filter(!unit == \"June 2022 - Daily Temperature Log\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\njuly_2022_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - July 2022(Table 1).csv\", skip = 1) |&gt; pivot_date(\"July\", \"2022\") |&gt; filter(!unit == \"July 2022 - Daily Temperature Log\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\naugust_2022_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - August 2022(Table 1).csv\", skip = 1) |&gt; pivot_date(\"August\", \"2022\") |&gt; filter(!unit == \"August 2022 - Daily Temperature Log\",!is.na(unit))\n\nNew names:\nRows: 70 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\nseptember_2022_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - September 2022(Table 1).csv\", skip = 1) |&gt; pivot_date(\"September\", \"2022\") |&gt; filter(!unit == \"September 2022 - Daily Temperature Log\",!is.na(unit))\n\nNew names:\nRows: 70 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#combining-each-month-into-one-data-frame",
    "href": "01-indoor-all-cleaning.html#combining-each-month-into-one-data-frame",
    "title": "All indoor logs cleaning",
    "section": "Combining each month into one data frame",
    "text": "Combining each month into one data frame\n\nindoor_2022_combined &lt;- bind_rows(april_2022_clean, may_2022_clean, june_2022_clean, july_2022_clean, august_2022_clean, september_2022_clean)",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#indoor-logs-1",
    "href": "01-indoor-all-cleaning.html#indoor-logs-1",
    "title": "All indoor logs cleaning",
    "section": "2023 indoor logs",
    "text": "2023 indoor logs\n**Note: There are two dates for units in 2023 where the temperature value is NA: - Huntsville 2023-08-25 - Segovia 2023-06-01 For these dates and units, there was no value included the original pdfs from TDCJ (the cells are blank).",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#downloading-data-and-applying-function-1",
    "href": "01-indoor-all-cleaning.html#downloading-data-and-applying-function-1",
    "title": "All indoor logs cleaning",
    "section": "Downloading data and applying function",
    "text": "Downloading data and applying function\n\napril_2023_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - April 2023(Table 1).csv\", skip = 1) |&gt; pivot_date(\"April\", \"2023\") |&gt; filter(!unit == \"April 2023\", !is.na(unit))\n\nNew names:\nRows: 69 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\nmay_2023_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - May 2023(Table 1).csv\", \n         skip = 1) |&gt; pivot_date(\"May\", \"2023\") |&gt; filter(!unit == \"May 2023\", !is.na(unit))\n\nNew names:\nRows: 69 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n**Important note: we manually removed an extra space from cell Young:26 in the original May 2023 csv because it was causing the column to read as characters instead of doubles.\n\njune_2023_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - June 2023(Table 1).csv\", skip = 1) |&gt; pivot_date(\"June\", \"2023\") |&gt; filter(!unit == \"June 2023\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n**Important note: we manually removed an extra space from cell Murray:30 in the original June 2023 csv because it was causing the column to read as characters instead of doubles.\n\njuly_2023_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - July 2023(Table 1).csv\", skip = 1) |&gt; pivot_date(\"July\", \"2023\") |&gt; filter(!unit == \"July 2023\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\naugust_2023_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - August 2023(Table 1).csv\", skip = 1) |&gt; pivot_date(\"August\", \"2023\") |&gt; filter(!unit == \"August 2023\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\nseptember_2023_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - September 2023(Table 1).csv\", skip = 1) |&gt; pivot_date(\"September\", \"2023\") |&gt; filter(!unit == \"September 2023\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#combining-each-month-into-one-data-frame-1",
    "href": "01-indoor-all-cleaning.html#combining-each-month-into-one-data-frame-1",
    "title": "All indoor logs cleaning",
    "section": "Combining each month into one data frame",
    "text": "Combining each month into one data frame\n\nindoor_2023_combined &lt;- bind_rows(april_2023_clean, may_2023_clean, june_2023_clean, july_2023_clean, august_2023_clean, september_2023_clean)",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#indoor-logs-2",
    "href": "01-indoor-all-cleaning.html#indoor-logs-2",
    "title": "All indoor logs cleaning",
    "section": "2024 indoor logs",
    "text": "2024 indoor logs\napril_2024_clean |&gt; filter(unit == “April 2024”)\napril_2024_clean |&gt; filter(is.na(unit))",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#downloading-data-and-applying-function-2",
    "href": "01-indoor-all-cleaning.html#downloading-data-and-applying-function-2",
    "title": "All indoor logs cleaning",
    "section": "Downloading data and applying function",
    "text": "Downloading data and applying function\n\napril_2024_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - April 2024(Table 1).csv\", skip = 1) |&gt; pivot_date(\"April\", \"2024\") |&gt; filter(!unit == \"April 2024\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\nmay_2024_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - May 2024(Table 1).csv\", \n         skip = 1) |&gt; pivot_date(\"May\", \"2024\") |&gt; filter(!unit == \"May 2024\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\njune_2024_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - June 2024(Table 1).csv\", skip = 1) |&gt; pivot_date(\"June\", \"2024\") |&gt; filter(!unit == \"June 2024\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n**Important note: we manually removed an apostrophe from cell Memorial:22 in the original June 2024 csv because it was causing the column to read as characters instead of doubles.\n\njuly_2024_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - July 2024(Table 1).csv\", skip = 1) |&gt; pivot_date(\"July\", \"2024\") |&gt;  filter(!unit == \"July 2024\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\naugust_2024_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - August 2024(Table 1).csv\", skip = 1) |&gt; pivot_date(\"August\", \"2024\") |&gt; filter(!unit == \"August 2024\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\nseptember_2024_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - September 2024(Table 1).csv\", skip = 1) |&gt; pivot_date(\"September\", \"2024\") |&gt; filter(!unit == \"September 2024\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n**Important note: we manually removed an extra space from Connally:21 and 22 in the original September 2024 csv because it was causing the column to read as characters instead of doubles",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#combining-each-month-into-one-data-frame-2",
    "href": "01-indoor-all-cleaning.html#combining-each-month-into-one-data-frame-2",
    "title": "All indoor logs cleaning",
    "section": "Combining each month into one data frame",
    "text": "Combining each month into one data frame\n\nindoor_2024_combined &lt;- bind_rows(april_2024_clean, may_2024_clean, june_2024_clean, july_2024_clean, august_2024_clean, september_2024_clean)",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#indoor-logs-3",
    "href": "01-indoor-all-cleaning.html#indoor-logs-3",
    "title": "All indoor logs cleaning",
    "section": "2025 indoor logs",
    "text": "2025 indoor logs",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#downloading-data-and-applying-function-3",
    "href": "01-indoor-all-cleaning.html#downloading-data-and-applying-function-3",
    "title": "All indoor logs cleaning",
    "section": "Downloading data and applying function",
    "text": "Downloading data and applying function\n** Because of when this public information request was filled, we only have data through July 25, 2025. Thus, all units will show “NA” for temperature between July 26 and July 31, 2025. As of Oct. 14, 2025, Lauren is still trying to get the updated data through September 2025.\n\napril_2025_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - April 2025(Table 1).csv\", skip = 1) |&gt; pivot_date(\"April\", \"2025\") |&gt; filter(!unit == \"April 2025\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\nmay_2025_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - May 2025(Table 1) (1).csv\", skip = 1) |&gt; pivot_date(\"May\", \"2025\") |&gt; filter(!unit == \"May 2025\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n\njune_2025_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - June 2025(Table 1).csv\", skip = 1) |&gt; pivot_date(\"June\", \"2025\") |&gt; filter(!unit == \"June 2025\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 31\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\n**Important note: we manually removed an extra space from Bartlett:8 in the original June 2025 csv because it was causing the column to read as characters instead of doubles.\n\njuly_2025_clean &lt;- read_csv(\"data-original/indoor_all_csv/SB1R56 - July 2025(Table 1).csv\", skip = 1) |&gt; pivot_date(\"July\", \"2025\") |&gt; filter(!unit == \"July 2025\", !is.na(unit))\n\nNew names:\nRows: 70 Columns: 32\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n18, 19,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#combining-each-month-into-one-data-frame-3",
    "href": "01-indoor-all-cleaning.html#combining-each-month-into-one-data-frame-3",
    "title": "All indoor logs cleaning",
    "section": "Combining each month into one data frame",
    "text": "Combining each month into one data frame\n\nindoor_2025_combined &lt;- bind_rows(april_2025_clean, may_2025_clean, june_2025_clean, july_2025_clean)",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#combining-all-years-of-indoor-data-into-one-data-frame",
    "href": "01-indoor-all-cleaning.html#combining-all-years-of-indoor-data-into-one-data-frame",
    "title": "All indoor logs cleaning",
    "section": "Combining all years of indoor data into one data frame",
    "text": "Combining all years of indoor data into one data frame\n\nindoor_temps_all_combined &lt;- bind_rows(indoor_2022_combined, indoor_2023_combined, indoor_2024_combined, indoor_2025_combined)\n\nindoor_temps_all_combined |&gt; head(50)",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-all-cleaning.html#export",
    "href": "01-indoor-all-cleaning.html#export",
    "title": "All indoor logs cleaning",
    "section": "Export",
    "text": "Export\n\nindoor_temps_all_combined |&gt; write_rds(\"data-processed/01-indoor-temps-all-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "All indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html",
    "href": "01-activation-cleaning-old.html",
    "title": "Activations cleaning",
    "section": "",
    "text": "Things to check\n\n\n\nI need to check some things with Lauren:\n\nWhere is the original file from TDCJ with the activations data?\nConfirm what these dates are for.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html#overview",
    "href": "01-activation-cleaning-old.html#overview",
    "title": "Activations cleaning",
    "section": "Overview",
    "text": "Overview\nThese are the days that a unit was under ICS protocols like providing extra ice water and access to cooler areas during extreme heat events. This data was acquired by Lauren McGaughy of The Texas Newsroom through a public information request to the Texas Department of Criminal Justice.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html#possible-update",
    "href": "01-activation-cleaning-old.html#possible-update",
    "title": "Activations cleaning",
    "section": "Possible update",
    "text": "Possible update\nWe don’t have a complete record of where the activations CSV comes from. Pretty sure this came from Lauren.\nIn our tracking Google sheet we do have the same data, except there are errors on the last line.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html#set-up",
    "href": "01-activation-cleaning-old.html#set-up",
    "title": "Activations cleaning",
    "section": "Set up",
    "text": "Set up\nYou know the drill!\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(data.table)",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html#comparison-activation-log",
    "href": "01-activation-cleaning-old.html#comparison-activation-log",
    "title": "Activations cleaning",
    "section": "Comparison activation log",
    "text": "Comparison activation log\nWe’ll check about the original CSV with Lauren, but in the meantime we’ll compare that with the Activation records we also have in our tracking sheet. This copy was downloaded from our MIG = TDCJ Basic Information on the “Activation” tab. The Google Sheet has errors on the last record for 9/5/2023, but that record is in full in our csv log.\n\n# download.file(\n#   \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRebFs9O2i0HoygXTtIvuzdVTmyrjX3MeUorU9d4fpYkGX7Bb026OradFQ1MMk2ltcGnyILih6ow4F4/pub?gid=1968960884&single=true&output=csv\",\n#   \"data-original/unit-activation-direct.csv\"\n#       )\n\nImport direct log\n\nraw_activation_direct &lt;- read_csv(\"data-original/unit-activation-direct.csv\") |&gt; \n  clean_names()\n\nRows: 102 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Initial Date of Extreme Temperature, ICS Implementation Date, Count...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nraw_activation_direct\n\n\n  \n\n\n\nThis original files lacks on activation that is in our already-downloaded unit-activation-log.csv. We’ll continue to use the -log version while we track down an original.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html#import-activation-log",
    "href": "01-activation-cleaning-old.html#import-activation-log",
    "title": "Activations cleaning",
    "section": "Import activation log",
    "text": "Import activation log\nLet’s add our activation log file in. (This file was downloaded directly into the repo, but we lost track of the original file. It has an additional record or other copies don’t have.)\n\nraw_activation_log &lt;- read_csv(\"data-original/unit-activation-log.csv\", skip = 1) |&gt; \n  clean_names()\n\nNew names:\nRows: 102 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): Initial Date of Extreme Temperature, ICS Implementation Date, Count... dbl\n(2): Time...6, Time...8 lgl (2): ...9, ...10\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `Date` -&gt; `Date...5`\n• `Time` -&gt; `Time...6`\n• `Date` -&gt; `Date...7`\n• `Time` -&gt; `Time...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n\nraw_activation_log",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html#clean-column-names",
    "href": "01-activation-cleaning-old.html#clean-column-names",
    "title": "Activations cleaning",
    "section": "Clean column names",
    "text": "Clean column names\nOur column names didn’t translate perfectly, so let’s work on cleaning them up and remove the last two unnecessary columns.\n\nclean_activation_log &lt;- raw_activation_log |&gt; \n  select(\n    initial_extreme_temp = initial_date_of_extreme_temperature,\n    initiation_date = ics_implementation_date,\n    county,\n    unit = unit_affected,\n    activation_date = date_5, \n    activation_time = time_6,\n    deactivation_date = date_7,\n    deactivation_time = time_8 \n  )",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html#change-date-columns",
    "href": "01-activation-cleaning-old.html#change-date-columns",
    "title": "Activations cleaning",
    "section": "Change date columns",
    "text": "Change date columns\nWe need our dates to be date columns instead of character columns. Let’s adjust.\n\nactivation_log_dates &lt;- clean_activation_log |&gt; \n  mutate(\n    initial_extreme_temp = mdy(initial_extreme_temp),\n    initiation_date = mdy(initiation_date),\n    activation_date = mdy(activation_date),\n    deactivation_date = mdy(deactivation_date)\n  ) \n\nactivation_log_dates",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html#document-active-days",
    "href": "01-activation-cleaning-old.html#document-active-days",
    "title": "Activations cleaning",
    "section": "Document active days",
    "text": "Document active days\nWe need to turn our initial_extreme_temp column, implementation_date column and our deactivation_date column into date ranges.\n\ndirty_actives &lt;- activation_log_dates |&gt; \n  group_by(unit) |&gt; \n  mutate(occurence = dense_rank(initial_extreme_temp)) |&gt; \n  mutate(start = as.Date(activation_date), end = as.Date(deactivation_date)) |&gt; \n  mutate(\n    activated = map2(start, end, ~seq(from = .x, to = .y, by = \"day\"))\n    ) |&gt; \n  unnest(activated) \n\ndirty_actives",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html#clean-up-our-active-dates",
    "href": "01-activation-cleaning-old.html#clean-up-our-active-dates",
    "title": "Activations cleaning",
    "section": "Clean up our active dates",
    "text": "Clean up our active dates\nWe need to create a status column, so let’s remove everything other than unit and active dates.\n\nclean_actives &lt;- dirty_actives |&gt; \n  select(c(unit, activated)) |&gt; \n  mutate(protocol_active = T) |&gt; \n  mutate(date = activated) |&gt; \n  select(!activated)\n\nclean_actives |&gt; \n  filter(unit == \"Byrd\")",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning-old.html#export",
    "href": "01-activation-cleaning-old.html#export",
    "title": "Activations cleaning",
    "section": "Export",
    "text": "Export\n\nclean_actives |&gt; write_rds(\"data-processed/01-activation-cleaned-old.rds\")",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html",
    "href": "01-outdoor-cleaning.html",
    "title": "Outdoor logs cleaning",
    "section": "",
    "text": "This notebook compiles data collected from one week of outdoor temperature logs collected by the Texas Department of Criminal Justice. The dates considered were July 24, 2023 through July 31, 2023. The logs are hand-written and data could not be accurately pulled from them using AI, so a group of Media Innovaiton Group fellows transcribed the week of logs for each prison we had records for, XXX in total. Those records were acquired through a public information request by Lauren McGaughey of the Texas Newsroom public radio collaborative.\nWe created a Google Spreadsheet file for each prison, recording the temperatures for each hour as best as we could decipher them. Here we download a tracking spreadsheet, which is then used to download all of the individual log sheets, combining them into a single file. When then prepare that file for analysis.",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#setup",
    "href": "01-outdoor-cleaning.html#setup",
    "title": "Outdoor logs cleaning",
    "section": "Setup",
    "text": "Setup",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#download-the-tracking-sheet",
    "href": "01-outdoor-cleaning.html#download-the-tracking-sheet",
    "title": "Outdoor logs cleaning",
    "section": "Download the tracking sheet",
    "text": "Download the tracking sheet\nThis sheet is in MIG Data &gt; Jail Logs &gt; Outdoor Logs Data. It is called Outdoor Logs Tracking Sheet.\n\nOptions are set to eval: false to avoid re-downloading. Set to true to update.\n\n\n\nExpand this to see code\n# download.file(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRQBIKas7s5Wwe4bd9tMlAheCsjqeFU2JMZfk6h8Hc_QH6Dx02SEZmP6ieny13hCyZQosmsRirDEu9P/pub?output=csv\", \"data-original/outdoor-jail-logs-urls.csv\")\n\n\nRead in the tracking sheet url\n\n\nExpand this to see code\nlog_urls &lt;- read_csv(\"data-original/outdoor-jail-logs-urls.csv\") |&gt; clean_names()\n\nlog_urls |&gt; glimpse()\n\n\nRows: 101\nColumns: 7\n$ unit_name &lt;chr&gt; \"Bell\", \"Byrd\", \"Diboll\", \"Duncan\", \"Ellis\", \"Estelle\", \"Fer…\n$ unit_code &lt;chr&gt; \"CV\", \"DU\", \"DO\", \"N6\", \"E1\", \"E2\", \"FE\", \"GG\", \"GR\", \"NF\", …\n$ region    &lt;chr&gt; \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", …\n$ slug      &lt;chr&gt; NA, \"r1-byrd\", NA, \"r1-duncan\", \"r1-ellis\", \"r1-estelle\", \"r…\n$ url       &lt;chr&gt; NA, \"https://docs.google.com/spreadsheets/d/e/2PACX-1vROqTyA…\n$ notes_gid &lt;dbl&gt; NA, 1133970569, NA, 535687266, 1770803795, 1551327169, 96070…\n$ notes     &lt;chr&gt; NA, NA, \"closely linked to Duncan, pre-release\", NA, NA, NA,…\n\n\nIn some cases we had the urls formatted incorrectly. Result should be 0 rows.\n\n\nExpand this to see code\nlog_urls |&gt; \n  filter(str_detect(url, \"pubhtml\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#download-all-the-files",
    "href": "01-outdoor-cleaning.html#download-all-the-files",
    "title": "Outdoor logs cleaning",
    "section": "Download all the files",
    "text": "Download all the files\nThis code below was courtesy of ChatGPT, based on the following prompt: “I’m using R. I have a data frame that has a list of urls and slugs, which are short file names. Please write me a loop or other code that will download all the files at those urls, with their names as the slug. These are all csv files.” There were some edits.\n\nOptions are set to eval: false to avoid re-downloading. Set to true to update.\n\n\n\nExpand this to see code\n# set our urls to df\ndf &lt;- log_urls\n\n# Download loop\nfor (i in seq_len(nrow(df))) {\n  url &lt;- df$url[i]\n  slug &lt;- df$slug[i]\n  file_name &lt;- paste0(\"data-original/2023_logs/\", slug, \".csv\")\n  \n  if (is.na(url)) {\n    # message(paste(\"No URL for slug:\", slug))\n    next\n  } else {\n    tryCatch({\n      download.file(url, destfile = file_name, mode = \"wb\")\n    }, error = function(e) {\n      message(paste(\"Failed to download:\", url, \"-\", e$message, \"\\n\"))\n    })\n  }\n}\n\n\n\nMake a files list\nMake a list of files:\n\n\nExpand this to see code\nlogs_list &lt;- list.files(\n  \"data-original/2023_logs\",\n  pattern = \".csv\",\n  full.names = TRUE\n  )\n\nlogs_list\n\n\n [1] \"data-original/2023_logs/r1-byrd.csv\"              \n [2] \"data-original/2023_logs/r1-duncan.csv\"            \n [3] \"data-original/2023_logs/r1-ellis.csv\"             \n [4] \"data-original/2023_logs/r1-estelle.csv\"           \n [5] \"data-original/2023_logs/r1-ferguson.csv\"          \n [6] \"data-original/2023_logs/r1-goodman.csv\"           \n [7] \"data-original/2023_logs/r1-goree.csv\"             \n [8] \"data-original/2023_logs/r1-holliday.csv\"          \n [9] \"data-original/2023_logs/r1-huntsville.csv\"        \n[10] \"data-original/2023_logs/r1-lewis.csv\"             \n[11] \"data-original/2023_logs/r1-polunsky.csv\"          \n[12] \"data-original/2023_logs/r1-wainwright.csv\"        \n[13] \"data-original/2023_logs/r1-wynne.csv\"             \n[14] \"data-original/2023_logs/r2-beto.csv\"              \n[15] \"data-original/2023_logs/r2-boyd.csv\"              \n[16] \"data-original/2023_logs/r2-choice-moore.csv\"      \n[17] \"data-original/2023_logs/r2-cole.csv\"              \n[18] \"data-original/2023_logs/r2-hutchins.csv\"          \n[19] \"data-original/2023_logs/r2-johnston.csv\"          \n[20] \"data-original/2023_logs/r2-michael.csv\"           \n[21] \"data-original/2023_logs/r2-powledge.csv\"          \n[22] \"data-original/2023_logs/r2-skyview.csv\"           \n[23] \"data-original/2023_logs/r2-telford.csv\"           \n[24] \"data-original/2023_logs/r3-clemens.csv\"           \n[25] \"data-original/2023_logs/r3-gist.csv\"              \n[26] \"data-original/2023_logs/r3-henley.csv\"            \n[27] \"data-original/2023_logs/r3-hightower.csv\"         \n[28] \"data-original/2023_logs/r3-hospital-galveston.csv\"\n[29] \"data-original/2023_logs/r3-jester-3.csv\"          \n[30] \"data-original/2023_logs/r3-kegans.csv\"            \n[31] \"data-original/2023_logs/r3-leblanc.csv\"           \n[32] \"data-original/2023_logs/r3-lychner.csv\"           \n[33] \"data-original/2023_logs/r3-memorial.csv\"          \n[34] \"data-original/2023_logs/r3-plane.csv\"             \n[35] \"data-original/2023_logs/r3-ramsey.csv\"            \n[36] \"data-original/2023_logs/r3-scott.csv\"             \n[37] \"data-original/2023_logs/r3-stiles.csv\"            \n[38] \"data-original/2023_logs/r3-stringfellow.csv\"      \n[39] \"data-original/2023_logs/r3-terrell.csv\"           \n[40] \"data-original/2023_logs/r3-vance.csv\"             \n[41] \"data-original/2023_logs/r3-young.csv\"             \n[42] \"data-original/2023_logs/r4-briscoe.csv\"           \n[43] \"data-original/2023_logs/r4-connally.csv\"          \n[44] \"data-original/2023_logs/r4-cotulla.csv\"           \n[45] \"data-original/2023_logs/r4-dominguez.csv\"         \n[46] \"data-original/2023_logs/r4-fort-stockton.csv\"     \n[47] \"data-original/2023_logs/r4-garza-west.csv\"        \n[48] \"data-original/2023_logs/r4-glossbrenner.csv\"      \n[49] \"data-original/2023_logs/r4-lopez.csv\"             \n[50] \"data-original/2023_logs/r4-lynaugh.csv\"           \n[51] \"data-original/2023_logs/r4-mcconnell.csv\"         \n[52] \"data-original/2023_logs/r4-sanchez.csv\"           \n[53] \"data-original/2023_logs/r4-stevenson.csv\"         \n[54] \"data-original/2023_logs/r4-torres.csv\"            \n[55] \"data-original/2023_logs/r5-allred.csv\"            \n[56] \"data-original/2023_logs/r5-dalhart.csv\"           \n[57] \"data-original/2023_logs/r5-daniel.csv\"            \n[58] \"data-original/2023_logs/r5-formby.csv\"            \n[59] \"data-original/2023_logs/r5-jordan.csv\"            \n[60] \"data-original/2023_logs/r5-mechler.csv\"           \n[61] \"data-original/2023_logs/r5-montford.csv\"          \n[62] \"data-original/2023_logs/r5-roach.csv\"             \n[63] \"data-original/2023_logs/r5-smith.csv\"             \n[64] \"data-original/2023_logs/r5-wallace.csv\"           \n[65] \"data-original/2023_logs/r5-wheeler.csv\"           \n[66] \"data-original/2023_logs/r6-crain.csv\"             \n[67] \"data-original/2023_logs/r6-halbert.csv\"           \n[68] \"data-original/2023_logs/r6-hamilton.csv\"          \n[69] \"data-original/2023_logs/r6-havins.csv\"            \n[70] \"data-original/2023_logs/r6-hilltop.csv\"           \n[71] \"data-original/2023_logs/r6-hobby.csv\"             \n[72] \"data-original/2023_logs/r6-hughes.csv\"            \n[73] \"data-original/2023_logs/r6-luther.csv\"            \n[74] \"data-original/2023_logs/r6-marlin.csv\"            \n[75] \"data-original/2023_logs/r6-middleton.csv\"         \n[76] \"data-original/2023_logs/r6-o-daniel.csv\"          \n[77] \"data-original/2023_logs/r6-pack.csv\"              \n[78] \"data-original/2023_logs/r6-robertson.csv\"         \n[79] \"data-original/2023_logs/r6-san-saba.csv\"          \n[80] \"data-original/2023_logs/r6-sayle.csv\"             \n[81] \"data-original/2023_logs/r6-travis-county.csv\"     \n[82] \"data-original/2023_logs/r6-woodman.csv\"           \n\n\n\n\nFile checks\nA little ChatGPT help here to find a way to count the number of lines in each file. There should be 193 in each one. In some cases our spreadsheets included blank rows, which we fixed. There is still a blank column, but we take care of that later.\nThis takes the list of files above and counts the lines. When then filter the list for any that aren’t 193 lines long. The result should be 0 rows.\n\n\nExpand this to see code\ncount_lines &lt;- function(file) {\n  length(readLines(file, warn = FALSE))\n}\n\nline_counts &lt;- sapply(logs_list, count_lines)\nline_counts_data &lt;- tibble(file = basename(names(line_counts)), lines = line_counts)\n\nline_counts_data |&gt; filter(lines != 193)\n\n\n\n  \n\n\n\nSometimes we needed to check a specific file being downloaded.\n\n\nExpand this to see code\n# download.file(\n#   \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRPdRSPyFwnYPgrLkJktmtg2hy4BmWl0eILzwO7KLalYxjPF2xYMZj4kSTuo3u7wwUckskou4-Dm4zF/pub?gid=0&single=true&output=csv\",\n#   \"data-original/2023_logs/r1-estelle.csv\"\n# )",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#combine-the-files",
    "href": "01-outdoor-cleaning.html#combine-the-files",
    "title": "Outdoor logs cleaning",
    "section": "Combine the files",
    "text": "Combine the files\nAgain uses the logs_list from above.\n\n\nExpand this to see code\nlogs_raw &lt;- logs_list |&gt;  #set_names(basename) |&gt;\n  map(\n  read_csv,\n  col_types = cols(.default = col_character())\n) |&gt; list_rbind() |&gt;\n  clean_names()\n\n\nNew names:\nNew names:\nNew names:\n• `` -&gt; `...12`\n\n\nExpand this to see code\nlogs_raw |&gt; glimpse()\n\n\nRows: 15,744\nColumns: 12\n$ unit    &lt;chr&gt; \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\"…\n$ date    &lt;chr&gt; \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023…\n$ rec     &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",…\n$ time    &lt;chr&gt; \"12:30 a.m.\", \"1:30 a.m.\", \"2:30 a.m.\", \"3:30 a.m.\", \"4:30 a.m…\n$ temp    &lt;chr&gt; \"83\", \"82\", \"81\", \"81\", \"80\", \"79\", \"79\", \"80\", \"83\", \"86\", \"9…\n$ humid   &lt;chr&gt; \"54\", \"62\", \"67\", \"71\", \"74\", \"76\", \"N/A\", \"79\", \"72\", \"69\", \"…\n$ wind    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hi_wc   &lt;chr&gt; \"65\", \"68\", \"69\", \"71\", \"71\", \"71\", \"N/A\", \"84\", \"86\", \"95\", \"…\n$ hi_wc_n &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ person  &lt;chr&gt; \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Jo…\n$ notes   &lt;chr&gt; NA, NA, NA, NA, \"hi_wc corrected\", \"humid corrected\", NA, NA, …\n$ x12     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\nCheck and remove extra column\n\n\nExpand this to see code\nlogs_raw |&gt; filter(!is.na(x12))\n\n\n\n  \n\n\n\n\n\nExpand this to see code\nlogs_tight &lt;- logs_raw |&gt; select(-x12)\n\nlogs_tight |&gt; glimpse()\n\n\nRows: 15,744\nColumns: 11\n$ unit    &lt;chr&gt; \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\"…\n$ date    &lt;chr&gt; \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023…\n$ rec     &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",…\n$ time    &lt;chr&gt; \"12:30 a.m.\", \"1:30 a.m.\", \"2:30 a.m.\", \"3:30 a.m.\", \"4:30 a.m…\n$ temp    &lt;chr&gt; \"83\", \"82\", \"81\", \"81\", \"80\", \"79\", \"79\", \"80\", \"83\", \"86\", \"9…\n$ humid   &lt;chr&gt; \"54\", \"62\", \"67\", \"71\", \"74\", \"76\", \"N/A\", \"79\", \"72\", \"69\", \"…\n$ wind    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hi_wc   &lt;chr&gt; \"65\", \"68\", \"69\", \"71\", \"71\", \"71\", \"N/A\", \"84\", \"86\", \"95\", \"…\n$ hi_wc_n &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ person  &lt;chr&gt; \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Jo…\n$ notes   &lt;chr&gt; NA, NA, NA, NA, \"hi_wc corrected\", \"humid corrected\", NA, NA, …",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#fix-date-time-rec",
    "href": "01-outdoor-cleaning.html#fix-date-time-rec",
    "title": "Outdoor logs cleaning",
    "section": "Fix date, time, rec",
    "text": "Fix date, time, rec\nTwo decisions in the formatting of our log necessitate this:\n\nWe configured our date and time columns like the form itself instead of datetime formats that easily import.\nWe originally numbered the rec column from 1 to 24 thinking we needed to label each reading, but the weather station data will be labeled 0 to 23 because it is the hour of time.\n\nHere we create a real datetime, date and proper hour.\nAlso, we remove artifact columns and any blank columns by specifying the columns we do need.\n\n\nExpand this to see code\nlogs_dates &lt;- logs_tight |&gt; \n  mutate(\n    new_time = case_when(\n      time == \"12:30 a.m.\" ~ \"00:30:00\",\n      time == \"12:30 p.m.\" ~ \"12:30:00\",\n      str_sub(time, -4, -1) == \"a.m.\" ~ paste0(str_extract(time, \"^(\\\\d+)\"),\":30:00\"),\n      str_sub(time, -4, -1) == \"p.m.\" ~ paste0(str_extract(time, \"^(\\\\d+)\") |&gt; as.numeric() + 12, \":30:00\"),\n      .default = NA\n    )\n  ) |&gt; \n  mutate(\n    datetime = mdy_hms(paste(date, new_time)),\n    date = mdy(date),\n    hour = hour(datetime),\n    .after = time\n  ) |&gt; \n  select(\n    unit,\n    date,\n    rec,\n    datetime,\n    hour,\n    temp,\n    humid,\n    wind,\n    hi_wc,\n    hi_wc_n,\n    person,\n    notes\n  )\n\nlogs_dates |&gt; slice_sample(n = 10)",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#convert-the-numbers",
    "href": "01-outdoor-cleaning.html#convert-the-numbers",
    "title": "Outdoor logs cleaning",
    "section": "Convert the numbers",
    "text": "Convert the numbers\nOur various readings come in as text. We convert them to numbers.\n\n\nExpand this to see code\nlogs_numbs &lt;- logs_dates |&gt; \n  mutate(\n    across(c(temp, humid, wind, hi_wc, hi_wc_n), parse_number)\n  )\n\n\nWarning: There were 5 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(c(temp, humid, wind, hi_wc, hi_wc_n), parse_number)`.\nCaused by warning:\n! 3 parsing failures.\n row col expected actual\n2451  -- a number    N/A\n2476  -- a number    N/A\n2478  -- a number    N/A\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\n\nExpand this to see code\nlogs_numbs |&gt; slice_sample(n = 10)\n\n\n\n  \n\n\n\n\n\nExpand this to see code\nlogs_numbs |&gt; filter(str_detect(unit, \"Moore\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#fix-names",
    "href": "01-outdoor-cleaning.html#fix-names",
    "title": "Outdoor logs cleaning",
    "section": "Fix names",
    "text": "Fix names\nIn some cases the name on the outdoor log sheet did not match our tracking sheet, which means we have trouble later when we need to match that information.\n\n\nExpand this to see code\nlogs_fixes &lt;- logs_numbs |&gt; \n  mutate(\n    unit = case_match(\n      unit,\n      \"Travis State Jail\" ~ \"Travis County\",\n      \"Choice Moore\" ~ \"Moore, C.\",\n      \"Jester 3\" ~ \"Jester III\",\n      \"Lane Murray\" ~ \"Murray\",\n      \"Wallace Pack\" ~ \"Pack\",\n      \"Mt. View\" ~ \"Woodman\",\n      .default = unit)\n  )\n\nlogs_fixes |&gt; filter(str_detect(unit, \"Jordan\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#add-the-region",
    "href": "01-outdoor-cleaning.html#add-the-region",
    "title": "Outdoor logs cleaning",
    "section": "Add the region",
    "text": "Add the region\nIt’s helpful later to have the region of the prison as part of this dataset, so I’m going to add it here from the unit info.\n\n\nExpand this to see code\nregions &lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\") |&gt; \n  select(unit_name, region)\n\nlogs_regions &lt;- logs_fixes |&gt; \n  left_join(regions, by = join_by(unit == unit_name)) |&gt; \n  relocate(region, .after = unit)\n\nlogs_regions |&gt; filter(str_detect(unit, \"Jordan\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#export-the-logs",
    "href": "01-outdoor-cleaning.html#export-the-logs",
    "title": "Outdoor logs cleaning",
    "section": "Export the logs",
    "text": "Export the logs\n\n\nExpand this to see code\nlogs_regions |&gt; write_rds(\"data-processed/01-outdoor-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#notes-pages",
    "href": "01-outdoor-cleaning.html#notes-pages",
    "title": "Outdoor logs cleaning",
    "section": "Notes pages",
    "text": "Notes pages\nWhen we transcribed the logs, we also included a notes sheet where we added any overall relavant or interesting information.\nWe will be following the same steps as above, except that we have to piece together the download url from our tracking sheet.\n\n\nExpand this to see code\nnotes_urls &lt;- log_urls |&gt; \n  # filter(region == \"II\") |&gt; # filters to region for testing\n  select(slug, url, notes_gid) |&gt; \n  mutate(\n    pub_url = str_extract(url, \"(.*pub\\\\?)\", group = 1),\n    notes_url = paste0(pub_url, \"gid=\", notes_gid, \"&output=csv\")\n  ) |&gt; \n  filter(!is.na(url)) |&gt; \n  select(slug, notes_url) \n\nnotes_urls\n\n\n\n  \n\n\n\nThen we download all the files. eval: false is set here so files won’t be re-downloaded.\n\n\nExpand this to see code\n# set our urls to df\nnotes_df &lt;- notes_urls\n\n\n# Download loop\nfor (i in seq_len(nrow(notes_df))) {\n  url &lt;- df$notes_url[i]\n  slug &lt;- df$slug[i]\n  file_name &lt;- paste0(\"data-original/2023_notes/\", slug, \".csv\")\n  \n  if (is.na(url)) {\n    # message(paste(\"No URL for slug:\", slug))\n    next\n  } else {\n    tryCatch({\n      download.file(url, destfile = file_name, mode = \"wb\")\n    }, error = function(e) {\n      message(paste(\"Failed to download:\", url, \"-\", e$message, \"\\n\"))\n    })\n  }\n}\n\n\nMake a list of file paths to combine.\n\n\nExpand this to see code\nnotes_list &lt;- list.files(\n  \"data-original/2023_notes\",\n  pattern = \".csv\",\n  full.names = TRUE\n  )\n\nnotes_list\n\n\n [1] \"data-original/2023_notes/r1-byrd.csv\"              \n [2] \"data-original/2023_notes/r1-duncan.csv\"            \n [3] \"data-original/2023_notes/r1-ellis.csv\"             \n [4] \"data-original/2023_notes/r1-estelle.csv\"           \n [5] \"data-original/2023_notes/r1-ferguson.csv\"          \n [6] \"data-original/2023_notes/r1-goodman.csv\"           \n [7] \"data-original/2023_notes/r1-goree.csv\"             \n [8] \"data-original/2023_notes/r1-holliday.csv\"          \n [9] \"data-original/2023_notes/r1-huntsville.csv\"        \n[10] \"data-original/2023_notes/r1-lewis.csv\"             \n[11] \"data-original/2023_notes/r1-polunsky.csv\"          \n[12] \"data-original/2023_notes/r1-wainwright.csv\"        \n[13] \"data-original/2023_notes/r1-wynne.csv\"             \n[14] \"data-original/2023_notes/r2-beto.csv\"              \n[15] \"data-original/2023_notes/r2-boyd.csv\"              \n[16] \"data-original/2023_notes/r2-choice-moore.csv\"      \n[17] \"data-original/2023_notes/r2-cole.csv\"              \n[18] \"data-original/2023_notes/r2-hutchins.csv\"          \n[19] \"data-original/2023_notes/r2-johnston.csv\"          \n[20] \"data-original/2023_notes/r2-michael.csv\"           \n[21] \"data-original/2023_notes/r2-powledge.csv\"          \n[22] \"data-original/2023_notes/r2-telford.csv\"           \n[23] \"data-original/2023_notes/r3-clemens.csv\"           \n[24] \"data-original/2023_notes/r3-gist.csv\"              \n[25] \"data-original/2023_notes/r3-henley.csv\"            \n[26] \"data-original/2023_notes/r3-hightower.csv\"         \n[27] \"data-original/2023_notes/r3-hospital-galveston.csv\"\n[28] \"data-original/2023_notes/r3-jester-3.csv\"          \n[29] \"data-original/2023_notes/r3-kegans.csv\"            \n[30] \"data-original/2023_notes/r3-leblanc.csv\"           \n[31] \"data-original/2023_notes/r3-lychner.csv\"           \n[32] \"data-original/2023_notes/r3-memorial.csv\"          \n[33] \"data-original/2023_notes/r3-plane.csv\"             \n[34] \"data-original/2023_notes/r3-ramsey.csv\"            \n[35] \"data-original/2023_notes/r3-scott.csv\"             \n[36] \"data-original/2023_notes/r3-stiles.csv\"            \n[37] \"data-original/2023_notes/r3-stringfellow.csv\"      \n[38] \"data-original/2023_notes/r3-terrell.csv\"           \n[39] \"data-original/2023_notes/r3-vance.csv\"             \n[40] \"data-original/2023_notes/r3-young.csv\"             \n[41] \"data-original/2023_notes/r4-briscoe.csv\"           \n[42] \"data-original/2023_notes/r4-connally.csv\"          \n[43] \"data-original/2023_notes/r4-cotulla.csv\"           \n[44] \"data-original/2023_notes/r4-dominguez.csv\"         \n[45] \"data-original/2023_notes/r4-fort-stockton.csv\"     \n[46] \"data-original/2023_notes/r4-garza-west.csv\"        \n[47] \"data-original/2023_notes/r4-glossbrenner.csv\"      \n[48] \"data-original/2023_notes/r4-lopez.csv\"             \n[49] \"data-original/2023_notes/r4-lynaugh.csv\"           \n[50] \"data-original/2023_notes/r4-mcconnell.csv\"         \n[51] \"data-original/2023_notes/r4-sanchez.csv\"           \n[52] \"data-original/2023_notes/r4-stevenson.csv\"         \n[53] \"data-original/2023_notes/r4-torres.csv\"            \n[54] \"data-original/2023_notes/r5-dalhart.csv\"           \n[55] \"data-original/2023_notes/r5-daniel.csv\"            \n[56] \"data-original/2023_notes/r5-formby.csv\"            \n[57] \"data-original/2023_notes/r5-jordan.csv\"            \n[58] \"data-original/2023_notes/r5-mechler.csv\"           \n[59] \"data-original/2023_notes/r5-montford.csv\"          \n[60] \"data-original/2023_notes/r5-roach.csv\"             \n[61] \"data-original/2023_notes/r5-smith.csv\"             \n[62] \"data-original/2023_notes/r5-wallace.csv\"           \n[63] \"data-original/2023_notes/r6-crain.csv\"             \n[64] \"data-original/2023_notes/r6-halbert.csv\"           \n[65] \"data-original/2023_notes/r6-hamilton.csv\"          \n[66] \"data-original/2023_notes/r6-havins.csv\"            \n[67] \"data-original/2023_notes/r6-hilltop.csv\"           \n[68] \"data-original/2023_notes/r6-hobby.csv\"             \n[69] \"data-original/2023_notes/r6-hughes.csv\"            \n[70] \"data-original/2023_notes/r6-luther.csv\"            \n[71] \"data-original/2023_notes/r6-marlin.csv\"            \n[72] \"data-original/2023_notes/r6-middleton.csv\"         \n[73] \"data-original/2023_notes/r6-o-daniel.csv\"          \n[74] \"data-original/2023_notes/r6-pack.csv\"              \n[75] \"data-original/2023_notes/r6-robertson.csv\"         \n[76] \"data-original/2023_notes/r6-san-saba.csv\"          \n[77] \"data-original/2023_notes/r6-sayle.csv\"             \n[78] \"data-original/2023_notes/r6-travis-county.csv\"     \n[79] \"data-original/2023_notes/r6-woodman.csv\"           \n\n\nNow we put those files together into a single tibble.\n\n\nExpand this to see code\nnotes_raw &lt;- notes_list |&gt; \n  set_names(basename) |&gt;\n  map(\n  read_csv,\n  col_types = cols(.default = col_character()),\n  col_names = c(\"notes\")\n) |&gt; list_rbind(names_to = \"unit\") |&gt;\n  clean_names()\n\nnotes_raw |&gt; glimpse()\n\n\nRows: 268\nColumns: 4\n$ unit  &lt;chr&gt; \"r1-byrd.csv\", \"r1-byrd.csv\", \"r1-duncan.csv\", \"r1-duncan.csv\", …\n$ notes &lt;chr&gt; \"Name we code as Komomzy is a difficult signature to comprehend\"…\n$ x2    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ x3    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\nWe have a little cleanup here for the unit and region. We drop the x columns, which are empty.\n\n\nExpand this to see code\nnotes_cleaned &lt;- notes_raw |&gt; \n  filter(!is.na(notes)) |&gt; \n  mutate(\n    region = str_extract(unit, pattern = \"(.*)-\", group = 1),\n    unit = str_extract(unit, pattern = \".*-(.*).csv\", group = 1)\n  ) |&gt; \n  select(unit, region, notes)\n\nnotes_cleaned |&gt; head()\n\n\n\n  \n\n\n\nWe’ll export this as both a csv and an rds because we may just put these into a spreadsheet for review.\n\n\nExpand this to see code\nnotes_cleaned |&gt; write_rds(\"data-processed/01-outdoor-notes.rds\")\nnotes_cleaned |&gt; write_csv(\"data-processed/01-outdoor-notes.csv\")",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html",
    "href": "01-unit-info-cleaning.html",
    "title": "Unit info cleaning",
    "section": "",
    "text": "This data piece will help us join together our unit-focused data and our weather station-focused data. The data was compiled by Media Innovation Group data fellows. The information came from the Texas Department of Criminal Justice website, a TDCJ unit prototype list and the National Weather Service website.",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#goals-of-this-notebook",
    "href": "01-unit-info-cleaning.html#goals-of-this-notebook",
    "title": "Unit info cleaning",
    "section": "Goals of this notebook",
    "text": "Goals of this notebook\nClean up our column names so we can easily combine with our other data.",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#loading-libraries",
    "href": "01-unit-info-cleaning.html#loading-libraries",
    "title": "Unit info cleaning",
    "section": "Loading libraries",
    "text": "Loading libraries\n\nlibrary(tidyverse)\nlibrary(janitor)",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#download-unit-info",
    "href": "01-unit-info-cleaning.html#download-unit-info",
    "title": "Unit info cleaning",
    "section": "Download unit info",
    "text": "Download unit info\n\nOption eval: fasle set to avoid re-download. Change to true for updates.\n\n\n# download.file(\n#   \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRebFs9O2i0HoygXTtIvuzdVTmyrjX3MeUorU9d4fpYkGX7Bb026OradFQ1MMk2ltcGnyILih6ow4F4/pub?gid=1653039441&single=true&output=csv\",\n#   \"data-original/unit-nws-info.csv\")",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#importing-unit-info",
    "href": "01-unit-info-cleaning.html#importing-unit-info",
    "title": "Unit info cleaning",
    "section": "Importing unit info",
    "text": "Importing unit info\nLet’s bring in our unit info sheet. This was compiled by us to be used in this project.\n\nnws_unit_raw &lt;- read_csv(\"data-original/unit-nws-info.csv\") |&gt; clean_names()\n\nRows: 101 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): Unit Name, Region, Data Fellow, NWS, NWS ID, Unit Code, Type, Stre...\ndbl  (1): Driving miles\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnws_unit_raw |&gt; head()",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#select-columns",
    "href": "01-unit-info-cleaning.html#select-columns",
    "title": "Unit info cleaning",
    "section": "Select columns",
    "text": "Select columns\nI’m gonna change column names and only keep the columns we need.\n\nunit_info_clean &lt;- nws_unit_raw |&gt; \n  select(unit_name, region, unit_code, type, nws, nws_id, county)\n\nunit_info_clean",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#export-data",
    "href": "01-unit-info-cleaning.html#export-data",
    "title": "Unit info cleaning",
    "section": "Export data",
    "text": "Export data\nOur cleaning was pretty easy! Let’s export the data and put it into our processed folder now.\n\nunit_info_clean |&gt; write_rds(\"data-processed/01-unit-info-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01.1-indoor-combining.html",
    "href": "01.1-indoor-combining.html",
    "title": "Combine indoor logs",
    "section": "",
    "text": "I’ve created station files that already have the protocol flags. This notebook could bring those in, but I’m not sure it is any more clear. I might trash those protocol data files."
  },
  {
    "objectID": "01.1-indoor-combining.html#to-do",
    "href": "01.1-indoor-combining.html#to-do",
    "title": "Combine indoor logs",
    "section": "",
    "text": "I’ve created station files that already have the protocol flags. This notebook could bring those in, but I’m not sure it is any more clear. I might trash those protocol data files."
  },
  {
    "objectID": "01.1-indoor-combining.html#overview",
    "href": "01.1-indoor-combining.html#overview",
    "title": "Combine indoor logs",
    "section": "Overview",
    "text": "Overview\nHere we bring together our indoor readings together with our station readings, and then create our protocol flags.\nWe’ll compare two ways to handle the station data:\n\nFind the 3 p.m. hours to match when the prison is supposed to record their records.\nThe daily summaries that have the hottest temp/heat index."
  },
  {
    "objectID": "01.1-indoor-combining.html#load-our-libraries",
    "href": "01.1-indoor-combining.html#load-our-libraries",
    "title": "Combine indoor logs",
    "section": "Load our libraries",
    "text": "Load our libraries\nlibrary(data.table)\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "01.1-indoor-combining.html#downloading-weather-logs-files",
    "href": "01.1-indoor-combining.html#downloading-weather-logs-files",
    "title": "Combine indoor logs",
    "section": "Downloading weather-logs files",
    "text": "Downloading weather-logs files\nWe’ll download a copy of our cleaned weather-logs into this repo so we don’t have to rely on the original.\nThe station files come from the weather-logs repo. That repo is currently private, so we’ll manually download the files into data-processed/weather-logs."
  },
  {
    "objectID": "01.1-indoor-combining.html#import-our-files",
    "href": "01.1-indoor-combining.html#import-our-files",
    "title": "Combine indoor logs",
    "section": "Import our files",
    "text": "Import our files\n\nstations_daily &lt;- read_rds(\"data-processed/weather-logs/01-station-daily-summary.rds\")\nstations_hourly &lt;- read_rds(\"data-processed/weather-logs/01-station-hourly-readings.rds\")\nheat_warnings &lt;- read_rds(\"data-processed/weather-logs/01-heat-warning-cleaned.rds\")\nactivation &lt;- read_rds(\"data-processed/01-activation-cleaned.rds\")\nindoor &lt;-  read_rds(\"data-processed/01-indoor-cleaned.rds\")\nunit_info&lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\")"
  },
  {
    "objectID": "01.1-indoor-combining.html#combine-data-files",
    "href": "01.1-indoor-combining.html#combine-data-files",
    "title": "Combine indoor logs",
    "section": "Combine data files",
    "text": "Combine data files\nNow that we’ve brought all of the files into our environment, it’s time to put them all together!\nThe process we’ll use is this:\n\nstart with indoor readings\njoin with unit-info to get the station ids\njoin with the stations to get that information\ncreate our protocol flags\nexport\n\nFirst we’ll combine our unit temperature logs with our activation status. We’ll also take this time to fill in our inactives because our original file only denoted activation."
  },
  {
    "objectID": "01.1-indoor-combining.html#indoor-unit-info",
    "href": "01.1-indoor-combining.html#indoor-unit-info",
    "title": "Combine indoor logs",
    "section": "Indoor + unit info",
    "text": "Indoor + unit info\n\nindoor_unit &lt;- indoor |&gt; \n  left_join(unit_info, by = join_by(unit == unit_name)) |&gt; \n  select(unit, date, unit_temp, nws_id, county) |&gt; \n  arrange(unit, date)\n\n# sample of data\nindoor_unit |&gt; slice_sample(n = 10)\n\n\n  \n\n\n# total row count\nindoor_unit |&gt; nrow()\n\n[1] 12323\n\n# missing join count\nindoor_unit |&gt; filter(is.na(nws_id))"
  },
  {
    "objectID": "01.1-indoor-combining.html#add-stations",
    "href": "01.1-indoor-combining.html#add-stations",
    "title": "Combine indoor logs",
    "section": "Add stations",
    "text": "Add stations\nI did check and the daily summaries have more possible matches than pulling 3p-only values. Here we join by both the station id and date.\n\nindoor_stations &lt;- indoor_unit |&gt; \n  left_join(stations_daily, by = join_by(nws_id == station_id, date))\n\nindoor_stations |&gt; nrow()\n\n[1] 12323\n\nindoor_stations |&gt; head()\n\n\n  \n\n\n\nUnfortunately there are There are 1580 indoor records (out of 12323) without a matching station record.\n\nindoor_stations_peek &lt;- indoor_stations |&gt; \n  group_by(unit, nws_id) |&gt;\n  summarize(\n    total_cnt = n(),\n    na_cnt = sum(is.na(name))\n  ) |&gt; \n  ungroup() |&gt; \n  adorn_totals(\"row\") |&gt; \n  tibble()\n\n`summarise()` has grouped output by 'unit'. You can override using the\n`.groups` argument.\n\nindoor_stations_peek\n\n\n  \n\n\n\nIn some cases there are just missing days of readings, which could be for any number of reasons, like the observation station was down.\nBut there are a number of prison units where we could not find a NWS weather data within 40 miles. These fit into that category.\n\nindoor_stations_peek |&gt; \n  filter(na_cnt &gt; 0,\n         total_cnt == na_cnt) |&gt; \n  arrange(unit)\n\n\n  \n\n\n\n\nRemove non-matches\nWe’ll remove indoor records where we don’t have a matching weather temperature since we can’t do any comparison here. (May decide later we need this, but later code might have to change to deal with missing variables.)\n\nindoor_clipped &lt;- indoor_stations |&gt; \n  filter(!is.na(name))\n\nindoor_clipped |&gt; nrow()\n\n[1] 10743"
  },
  {
    "objectID": "01.1-indoor-combining.html#create-a-flag",
    "href": "01.1-indoor-combining.html#create-a-flag",
    "title": "Combine indoor logs",
    "section": "Create a flag",
    "text": "Create a flag\nWe know that one form of criteria for our heat protocol is if the feels like is greater than or equal to 113 for three days. Let’s create a flag for days greater than or equal to 113 and then count the number of consecutive days.\n\nindoor_flags &lt;- indoor_clipped |&gt; \n  mutate(\n    tmp_flag = (case_when(\n      tmp_high &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi_high &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == \"TRUE\" | hi_flag == \"TRUE\" ~ TRUE,\n      .default = FALSE)\n    )\n  \nindoor_flags"
  },
  {
    "objectID": "01.1-indoor-combining.html#look-for-runs",
    "href": "01.1-indoor-combining.html#look-for-runs",
    "title": "Combine indoor logs",
    "section": "Look for runs",
    "text": "Look for runs\nCreate if either are true column Add another column that showsd if true is along with active\n\nindoor_protocol &lt;- indoor_flags |&gt; \n  group_by(unit) |&gt; \n  mutate(\n    protocol_run = accumulate(protocol_flag, ~if_else(.y, .x + 1, 0))\n  ) |&gt; \n  mutate(\n    protocol_met = case_when(\n    protocol_run &gt;= 3 ~ TRUE,\n    .default = FALSE\n  )) |&gt; \n  ungroup()\n\nindoor_protocol |&gt; \n  select(unit, date, starts_with(\"protocol\")) |&gt; \n  # skipping lines to get to true values\n  slice(80:100)"
  },
  {
    "objectID": "01.1-indoor-combining.html#add-activation-days",
    "href": "01.1-indoor-combining.html#add-activation-days",
    "title": "Combine indoor logs",
    "section": "Add activation days",
    "text": "Add activation days\nNow we’ll add a column that shows which prisons were actively under heat protocols.\n\nindoor_active &lt;- indoor_protocol |&gt; \n  left_join(activation) |&gt; \n  mutate(protocol_active = if_else(is.na(protocol_active), F, T),\n         protocol_fail = case_when(\n           protocol_met == T & protocol_active == F ~ T,\n           .default = F\n         ))\n\nJoining with `by = join_by(unit, date)`\n\nindoor_active |&gt; filter(protocol_met == T)"
  },
  {
    "objectID": "01.1-indoor-combining.html#consider-heat-warnings",
    "href": "01.1-indoor-combining.html#consider-heat-warnings",
    "title": "Combine indoor logs",
    "section": "Consider heat warnings",
    "text": "Consider heat warnings\nThere are only six cases where we’ve found a heat warning in effect. This could be because we don’t have a good match on these warnings unless it is set for the whole county.\nWe won’t save this join since it is of little use.\n\n# add later\nindoor_active |&gt;\n  left_join(heat_warnings, by = join_by(nws_id == station_code, date)) |&gt; \n  filter(ehw_active == T)"
  },
  {
    "objectID": "01.1-indoor-combining.html#export",
    "href": "01.1-indoor-combining.html#export",
    "title": "Combine indoor logs",
    "section": "Export",
    "text": "Export\nWe’ve put everything together! Now let’s export it into data-processed.\n\nindoor_protocol |&gt;\n  write_rds(\"data-processed/02-indoor-combined.rds\")"
  },
  {
    "objectID": "02-outdoor-protocols.html",
    "href": "02-outdoor-protocols.html",
    "title": "Precautionary measures",
    "section": "",
    "text": "We have eight days of hand-written weather logs from 82 prisons that we have translated into data. The date range was 2023-07-24 to 2023-07-31.\nIn this analysis our aim is to compare those local weather logs to TDCJ heat protocol implementations. The protocols are based on the EXCESSIVE AND EXTREME TEMPERATURE CONDITIONS IN THE TDCJ (AD-10.64 (rev. 10)).\n\n\nThese come from AD-10.64:\n\n“Excessive Heat” occurs from a combination of significantly higher than normal temperatures and high humidity.\n“Excessive Heat Warning” is issued by the National Weather Service within 12 hours of the onset of the following criteria: temperature of at least 105ºF for more than three hours per day for two consecutive days, or heat index of 113ºF or greater for any period of time.\n“Heat Wave” is a prolonged period (three or more days) of excessively hot and unusually humid weather that meets the following criteria: temperature of at least 105ºF or heat index of 113ºF.\n\n\n\n\nWhen the National Weather Service issues an excessive heat warning or notice of an impending heat wave, the TDCJ Office of Emergency Management shall send the applicable division directors an email notification. When excessive heat conditions last for more than three consecutive days, the division directors and warden(s) of units in the affected area(s) shall immediately implement additional precautionary measures, as outlined in Section IV.I of this directive.\nNOTE: The TDCJ protocol activation records we have relate to these “precautionary measures” mentioned here.\n\n\n\nThe definition of excessive heat above is a challenge as it is not clearly defined with data points to measure against, yet seems to be referred to in the III.A.2 section of the directive.\nExcessive heat warning is clearly defined, but the III.A.2 section does not use the term “warning”, just the ambiguous “excessive heat conditions” terminology.\nDoes this mean that an “excessive heat warning” must be in effect for more than 3 days before implementing “precautionary measure” protocols? Or that the conditions that contribute to that heat warning have to be in place?\n\n\n\nIn communications with the TDCJ Director of Communications, they stated that “ICS is initiated when temperatures are 105+ degrees or heat index is 113+ for three or more consecutive days.” This interpretation is less strict than the “excessive heat warning” definition in the AD-10.64 directive.\nOur understanding is the “ICS” (Incident Command System) is the precautionary measures outlined in III.A.2 of the directive.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#overview",
    "href": "02-outdoor-protocols.html#overview",
    "title": "Precautionary measures",
    "section": "",
    "text": "We have eight days of hand-written weather logs from 82 prisons that we have translated into data. The date range was 2023-07-24 to 2023-07-31.\nIn this analysis our aim is to compare those local weather logs to TDCJ heat protocol implementations. The protocols are based on the EXCESSIVE AND EXTREME TEMPERATURE CONDITIONS IN THE TDCJ (AD-10.64 (rev. 10)).\n\n\nThese come from AD-10.64:\n\n“Excessive Heat” occurs from a combination of significantly higher than normal temperatures and high humidity.\n“Excessive Heat Warning” is issued by the National Weather Service within 12 hours of the onset of the following criteria: temperature of at least 105ºF for more than three hours per day for two consecutive days, or heat index of 113ºF or greater for any period of time.\n“Heat Wave” is a prolonged period (three or more days) of excessively hot and unusually humid weather that meets the following criteria: temperature of at least 105ºF or heat index of 113ºF.\n\n\n\n\nWhen the National Weather Service issues an excessive heat warning or notice of an impending heat wave, the TDCJ Office of Emergency Management shall send the applicable division directors an email notification. When excessive heat conditions last for more than three consecutive days, the division directors and warden(s) of units in the affected area(s) shall immediately implement additional precautionary measures, as outlined in Section IV.I of this directive.\nNOTE: The TDCJ protocol activation records we have relate to these “precautionary measures” mentioned here.\n\n\n\nThe definition of excessive heat above is a challenge as it is not clearly defined with data points to measure against, yet seems to be referred to in the III.A.2 section of the directive.\nExcessive heat warning is clearly defined, but the III.A.2 section does not use the term “warning”, just the ambiguous “excessive heat conditions” terminology.\nDoes this mean that an “excessive heat warning” must be in effect for more than 3 days before implementing “precautionary measure” protocols? Or that the conditions that contribute to that heat warning have to be in place?\n\n\n\nIn communications with the TDCJ Director of Communications, they stated that “ICS is initiated when temperatures are 105+ degrees or heat index is 113+ for three or more consecutive days.” This interpretation is less strict than the “excessive heat warning” definition in the AD-10.64 directive.\nOur understanding is the “ICS” (Incident Command System) is the precautionary measures outlined in III.A.2 of the directive.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#other-known-issues",
    "href": "02-outdoor-protocols.html#other-known-issues",
    "title": "Precautionary measures",
    "section": "Other known issues",
    "text": "Other known issues\nBecause of the nature of the original records, there are missing data that affect our analysis.\n\nThere are 8 records where we don’t have a temperature value. We dropped those records.\nThere are 3088 records (out of 15,744) where we don’t have a heat index value. That limits their value, but we still have temperatures, so we keep them.\n\nThere are 7 units that have one or more days where we can’t find a max heat index value. Typically because there are no heat index values recorded for a given date. 8 days for Dalhart, 7 for Formby, 3 for Wheeler. The others only have one day missing.\nIn the original written logs, some units (Dalhart, for example) used category designations instead of heat index values, which means they will not translate into numbers for data. Category 3 is the “Danger” area according to the NOAA’s National Weather Service Heat and Humidity Index included in the TDCJ directive, and that range includes heat indexes of 113 degrees and higher. Category 4 is “Extreme Danger”. This means in some cases we could have heat conditions based on heat index, but they are not included.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#what-well-find",
    "href": "02-outdoor-protocols.html#what-well-find",
    "title": "Precautionary measures",
    "section": "What we’ll find",
    "text": "What we’ll find\nBased on our communications with TDCJ, we’ll use the following definition as “meeting protocol” to go into precautionary measures: When temperatures are 105+ degrees or heat index is 113+ for three or more consecutive days.\nWe’ll find …\n\nWhich units met the protocol definition outlined above.\nWhich units had ICS “precautionary measures” implemented by TDCJ.\nSetting aside the consecutive days criteria, we’ll find any day it reached 105 or a heat index of 113.\nWe’ll find how many times units reached a heat index of 90 or greater, as the directive uses that measure for some heat mitigation measures, though TDCJ does not track their use.\nJust for comparison, we’ll calculate the more strict “excessive heat warning” criteria and see when units reached it.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#setup",
    "href": "02-outdoor-protocols.html#setup",
    "title": "Precautionary measures",
    "section": "Setup",
    "text": "Setup\nOur libraries.\n\n\nExpand this to see code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(rlang)\n\n\n\nStreaks function\nThis function creates a new column that counts how many consecutive days another column has been true, within each group (by unit). It’s used several times throughout the analysis, anytime we want to count a streak of a flag value. The code was written with help from ChatGPT. Full explanation in the resources folder.\n\n\nExpand this to see code\nadd_streaks &lt;- function(df, new_col, flag_col) {\n  # helps new col work if quoted\n  new_col &lt;- rlang::ensym(new_col)\n\n  df |&gt;\n  arrange(unit, date) |&gt; \n  group_by(unit) |&gt; \n  # Counts consecutive flags by unit\n  mutate(\n    !!new_col := {\n      count &lt;- 0L\n      map_int({{ flag_col }}, ~ {\n        if (.x) {\n          count &lt;&lt;- count + 1L\n        } else {\n          count &lt;&lt;- 0L\n        }\n        count\n      })\n    }\n  ) |&gt; \n  ungroup()\n}",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#import",
    "href": "02-outdoor-protocols.html#import",
    "title": "Precautionary measures",
    "section": "Import",
    "text": "Import\nWe need the logs and the activations.\n\n\nExpand this to see code\nlogs_all &lt;- read_rds(\"data-processed/01-outdoor-cleaned.rds\") \nactivations &lt;- read_rds(\"data-processed/01-activation-cleaned.rds\")",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#data-checks",
    "href": "02-outdoor-protocols.html#data-checks",
    "title": "Precautionary measures",
    "section": "Data checks",
    "text": "Data checks\nWe use temperature (tmp) and heat index (hi_wc) values as a basis of everything. Missing those values affect things down the road, so let’s catalog the problems and mitigate them.\n\nMissing both\nHere we find records that have neither value:\n\n\nExpand this to see code\nlogs_all |&gt; filter(is.na(temp), is.na(hi_wc))\n\n\n\n  \n\n\n\nWe’ll drop these records below because they can’t help us. I’ve checked and these are also the only records that are missing temp.\n\n\nExpand this to see code\nlogs_clipped &lt;- logs_all |&gt; \n  drop_na(temp) # drops 8 records that don't have a temperature recorded\n\n\n\n\nMissing heat index\nHowever, there are some records with temps that are missing heat index/wind chill hi_wc values.\n\n\nExpand this to see code\nlogs_missing_hi &lt;- logs_clipped |&gt; filter(is.na(hi_wc))\n\nlogs_missing_hi |&gt; nrow()\n\n\n[1] 3080\n\n\nMissing the heat index value is not necessarily a problem. Temperatures and humidity could be out of range for a valid calculation.\nWe keep these because we can still use the temp values.\n\n\nRisk category\nThat said, of the records without heat index values, there are some that do have the hi_wc_n value, which is the “Risk Category” in the NOAA Heat and Humidity Index.\n\n\n\nCategory\nRating\n\n\n\n\n1\nCaution\n\n\n2\nExtreme Caution\n\n\n3\nDanger\n\n\n4\nExtreme Danger\n\n\n\nHere I filter to find the units that have the category listed. Since these are hourly readings, I use distinct to find the dates when a unit recorded a specific category. We then filter those units that recorded cat 3 or higher.\n\n\nExpand this to see code\nlogs_missing_hi |&gt;\n  filter(!is.na(hi_wc_n)) |&gt; \n  distinct(unit, date, hi_wc_n) |&gt;\n  filter(hi_wc_n &gt;= 3)\n\n\n\n  \n\n\n\nThis means there are 15 “unit days” where it is possible the heat index has reached 113, but we don’t know for sure. Here we show which units have a missing heat index and category 3 risk category.\nIn the end, we are keeping records with no heat index value because they still have temperature, and we are ignoring the risk category because we don’t know the exact heat index value.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#preparing-the-protocol-flags",
    "href": "02-outdoor-protocols.html#preparing-the-protocol-flags",
    "title": "Precautionary measures",
    "section": "Preparing the protocol flags",
    "text": "Preparing the protocol flags\nHere we summarize the hourly logs to see if certain conditions exist in a given day.\n\nWe find the maximum heat index value.\nWe find the maximum temperatures for a day.\nWe find how many hours within a day the temperature was above 105. (This is for strict Excessive Heat Warning calculations.)\n\nWe peek at a sample to check results.\n\n\nExpand this to see code\nlogs_values &lt;- logs_clipped |&gt; \n  group_by(unit, date) |&gt; \n  # summarize measurements\n  summarise(\n    u_hi_max = max(hi_wc, na.rm = T),\n    u_tmp_max = max(temp, na.rm = T),\n    u_hrs_105 = sum(temp &gt;= 105),\n    .groups = \"drop\"\n  )\n\n\nWarning: There were 22 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `u_hi_max = max(hi_wc, na.rm = T)`.\nℹ In group 81: `unit = \"Dalhart\"` `date = 2023-07-24`.\nCaused by warning in `max()`:\n! no non-missing arguments to max; returning -Inf\nℹ Run `dplyr::last_dplyr_warnings()` to see the 21 remaining warnings.\n\n\nExpand this to see code\nlogs_values |&gt; slice_sample(n = 20)\n\n\n\n  \n\n\n\n\nWarnings and -Inf values\nWe do get some warnings here “no non-missing arguments to max; returning -Inf” when we calculate maximum heat index values. These are cases where the hi_wc was always NA within a unit day, so there was no max value to find. In 22 cases it gives us an -Inf which gets treated as NA going forward and we can’t consider heat index values for those unit days.\n\n\nAdd hi, tmp flags\nHere we add some flags based on the excessive heat warning definitions:\n\nIf heat index reached 113\nIf the temperature reached 105\n\nWe peek at a sample to check the results.\n\n\nExpand this to see code\nlogs_flags &lt;- logs_values |&gt; \n  mutate(\n    u_hi_flag = if_else(u_hi_max &gt;= 113, T, F),\n    u_tmp_flag = if_else(u_tmp_max &gt;= 105, T, F),\n    )\n\n# view a sample\nlogs_flags |&gt; slice_sample(n = 20)\n\n\n\n  \n\n\n\n\n\nSet protocol flags\nHere we set more flags when certain definitions have been met.\n\nFlag TRUE if either the temperature reached 105 or the heat index reached 113: u_hi_tmp_proto.\nCreate a consecutive day count for when u_hi_tmp_proto is true: u_hi_tmp_proto_cnt.\nFlag TRUE if u_hi_tmp_proto_cnt is more than three days: u_proto_flag.\n\nWe peek at a sample of rows to check logic.\n\n\nExpand this to see code\nlogs_protos &lt;- logs_flags |&gt; \n  mutate(\n    u_hi_tmp_proto = if_else(u_tmp_flag == T | u_hi_flag == T, T, F),\n  ) |&gt; \n  add_streaks(\"u_hi_tmp_proto_cnt\", u_hi_tmp_proto) |&gt; \n  mutate(\n    u_proto_flag = if_else(u_hi_tmp_proto_cnt &gt; 3, T, F)\n  )\n\nlogs_protos |&gt; slice_sample(n = 20)\n\n\n\n  \n\n\n\nAt this point we have all the conditions counted and flags created.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#join-with-activations",
    "href": "02-outdoor-protocols.html#join-with-activations",
    "title": "Precautionary measures",
    "section": "Join with activations",
    "text": "Join with activations\nHere we take our log files and join them with the activation dates so we can see where/when they match, if at all.\nWe glimpse the new table to review the column names.\n\n\nExpand this to see code\nlogs_activations &lt;- logs_protos |&gt;\n  left_join(activations, join_by(unit, date)) |&gt; \n  mutate(protocol_active = if_else(is.na(protocol_active), F, protocol_active))\n\nlogs_activations |&gt; glimpse()\n\n\nRows: 656\nColumns: 11\n$ unit               &lt;chr&gt; \"Allred\", \"Allred\", \"Allred\", \"Allred\", \"Allred\", \"…\n$ date               &lt;date&gt; 2023-07-24, 2023-07-25, 2023-07-26, 2023-07-27, 20…\n$ u_hi_max           &lt;dbl&gt; 105, 107, 105, 105, 110, 105, 104, 108, 109, 107, 1…\n$ u_tmp_max          &lt;dbl&gt; 106, 108, 103, 105, 104, 105, 105, 107, 103, 100, 9…\n$ u_hrs_105          &lt;int&gt; 3, 4, 0, 1, 0, 1, 2, 5, 0, 0, 0, 0, 0, 0, 1, 2, 0, …\n$ u_hi_flag          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ u_tmp_flag         &lt;lgl&gt; TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, F…\n$ u_hi_tmp_proto     &lt;lgl&gt; TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, F…\n$ u_hi_tmp_proto_cnt &lt;int&gt; 1, 2, 0, 1, 0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 1, 2, 0, …\n$ u_proto_flag       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ protocol_active    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n\n\n\nChecks on activations\nWe have a multi-step bit here to make sure that all the activations have been added to our logs data.\nHere we look at the original data to see which rows match our log time frame.\n\n\nExpand this to see code\nactivations_active &lt;- activations |&gt;\n  filter(date &gt;= \"2023-07-24\" & date &lt;= \"2023-07-31\") |&gt; \n  arrange(date, unit)\n\nactivations_active\n\n\n\n  \n\n\n\nThere are 24 activations in our time period. Now let’s check our joined data …\n\n\nExpand this to see code\nlogs_activations_active &lt;- logs_activations |&gt; \n  filter(!is.na(protocol_active)) |&gt; \n  arrange(date, unit)\n\nlogs_activations_active\n\n\n\n  \n\n\n\nThere are also 24 days here. Let’s anti-join them to see if there are any non-matches.\nThis should result in 0 rows if we’ve done things right.\n\n\nExpand this to see code\nactivations_active |&gt; \n  anti_join(logs_activations_active, join_by(unit, date))\n\n\n\n  \n\n\n\nThis has been overkill, but I’m certain that we have captured all of the activations in our time period and those have been properly added to our logs.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#answer-the-questions",
    "href": "02-outdoor-protocols.html#answer-the-questions",
    "title": "Precautionary measures",
    "section": "Answer the questions",
    "text": "Answer the questions\n\n1. Units meeting criteria\n\nWhich units met the protocol definition outlined above.\n\n\n\nExpand this to see code\ncriteria_met &lt;- logs_activations |&gt; \n  filter(u_proto_flag == T) |&gt; \n  select(unit, date, u_hi_max, u_tmp_max, protocol_active) |&gt; \n  arrange(unit, date)\n\ncriteria_met\n\n\n\n  \n\n\n\nWhich units were involved?\n\n\nExpand this to see code\ncriteria_met |&gt; count(unit)\n\n\n\n  \n\n\n\nWere ICS protocols activated?\n\n\nExpand this to see code\ncriteria_met |&gt; filter(protocol_active == T)\n\n\n\n  \n\n\n\n\n\nTakeaway 1\nSeven units met the ICS criteria for a total of 17 “unit days” based on the definition given to us by the TDCJ communications department. None of those had active ICS protocols in place.\nNote that this count could be low, as it’s possible units met protocol before the first day of our sample.\n\n\n2. Days in ICS\nHere we look at the days that units within our time frame where TDCJ activated its Incident Command System, i.e., precautionary measures based on heat.\nWe include our u_proto_flag created based on the handwritten weather logs.\n\n\nExpand this to see code\nlogs_ics &lt;- logs_activations |&gt; \n  filter(protocol_active == T) |&gt; \n  select(unit, date, u_hi_max, u_tmp_max, u_proto_flag)\n\nlogs_ics\n\n\n\n  \n\n\n\nHowever, 12 of these activation days are within the first two days of our period, meaning they could’ve met the criteria based on data not in our review.\n\n\nExpand this to see code\nlogs_ics |&gt; filter(date &lt; \"2023-07-26\")\n\n\n\n  \n\n\n\nOne more look … Which of these days did not meet the criteria for a the given day.\n\n\nExpand this to see code\nlogs_ics |&gt; \n  filter(u_hi_max &lt; 113 & u_tmp_max &lt; 105)\n\n\n\n  \n\n\n\nHalf of these units did not reach a heat index of 113 or temperature of 105 on the day of the active protocol. Being under ICS is likely good for inmates, but it perhaps shows inconsistency in implementation.\n\n\nTakeaway 2\nAccording to the activation data, there were 24 days in our time period where ICS heat mitigation protocols were put in place. On those activation days, none of these units met the criteria we’ve been given by TDCJ communications based on the weather logs kept by units. That said, half of the activations are within the first two days of our study, meaning they could have met the criteria in the days before our time frame.\n\n\n3. Any day meeting condition\nThese precautionary protocols are mandated only after heat conditions exists for three consecutive days. But any day these conditions exist can be dangerous. Let’s see how many times this condition was true for these units in our time period.\n\n\nExpand this to see code\nlogs_activations |&gt; \n  filter(u_hi_tmp_proto == T) |&gt; \n  count(unit) |&gt; \n  adorn_totals(\"row\") |&gt; \n  tibble()\n\n\n\n  \n\n\n\nHow many of these were in active ICS protocol?\n\n\nExpand this to see code\nlogs_activations |&gt; \n  filter(u_hi_tmp_proto == T) |&gt; \n  count(protocol_active)\n\n\n\n  \n\n\n\n\n\nTakeaway 3\nOut of 656 “unit days” possible (82 units, 8 days), excessive heat conditions were reached 127 times. Half of the units reached the criteria at least once. In only 12 of those instances was a unit in active precautionary measures.\n\n\n4. Days of 90 heat index\nThere is a section in the directive (III.A.5) where heat precautions must be taken when in “apparent air temperatures above 90 degrees”, like frequent water breaks. TDCJ does not record when such conditions exist, but we wanted to get an idea.\n\n\nExpand this to see code\nlogs_activations |&gt; \n  filter(u_hi_max &gt;= 90) |&gt; \n  count(unit)\n\n\n\n  \n\n\n\n\n\nTakeaway 4\nThese 90 degree heat index conditions existed nearly every day for all 82 units within our time period. Only five units didn’t meet that criteria every day, and for four of those days it didn’t meet for only a single day.\n\n\n5. Excessive Heat Warnings\nWithin the directive, TDCJ outlines a definition of “Excessive Heat Warning” as “temperature of at least 105ºF for more than three hours per day for two consecutive days, or heat index of 113ºF or greater for any period of time.” This is more strict than the definition we use above. Let’s just see how many of our units would meet this criteria based on the handwritten weather logs data.\n\n\nPrep the data\nWe have to calculate quite a few things to make this definition. Comments in code.\n\n\nExpand this to see code\nlogs_ehw_protos &lt;- logs_values |&gt; \n  # 105ºF for more than three hours per day\n  mutate(\n    u_tmp_3hrs = if_else(u_hrs_105 &gt;= 3, T, F)\n  ) |&gt; \n  # add count to find consecutive days 105 met\n  add_streaks(\"u_tmp_flag_cnt\", u_tmp_3hrs) |&gt; \n  mutate(\n    # flag if 105 was 2 or more days\n    u_tmp_2days = if_else(u_tmp_flag_cnt &gt;= 2, T, F),\n    # flag if either tmp conditions or heat index conditions are met\n    u_ehw_flag = if_else(u_hi_max &gt;= 113 | u_tmp_2days == T, T, F)\n  ) |&gt; \n  # Add count to find consecutive days either condition met\n  add_streaks(\"u_ehw_flag_cnt\", u_ehw_flag) |&gt; \n  # add activations\n  left_join(activations, join_by(unit, date)) |&gt; \n  mutate(protocol_active = if_else(is.na(protocol_active), F, protocol_active))\n\nlogs_ehw_protos\n\n\n\n  \n\n\n\n\n\nEHW streaks\nIn III.A.2 the directive says “When excessive heat conditions last for more than three consecutive days, the division directors and warden(s) of units in the affected area(s) shall immediately implement additional precautionary measures …”\nEmphasis is mine. That portion could be read as four or more days, which is different than the “three consecutive days” value we’ve been working with in our basic definition above.\nHere we show which units met the EHW criteria for three or more days.\nOF NOTE: Since we only have eight days of readings, we potentially miss streaks early in our time period. Conditions could have been met on July 22-23, but we don’t know that.\n\n\nExpand this to see code\nlogs_ehw_protos |&gt; \n  filter(u_ehw_flag_cnt &gt;= 3) |&gt; \n  select(unit, date, u_hi_max, u_tmp_max, u_ehw_flag_cnt, protocol_active)\n\n\n\n  \n\n\n\nThe emphasis is mine. This could be read as these conditions being in place for more than three days, so four or greater. So here check that.\n\n\nExpand this to see code\nlogs_ehw_protos |&gt; \n  filter(u_ehw_flag_cnt &gt;= 4) |&gt; \n  select(unit, date, u_hi_max, u_tmp_max, u_ehw_flag_cnt, protocol_active)\n\n\n\n  \n\n\n\n\n\nTakeaway 5\nThere were 12 “unit days” where strict Excessive Heat Warning conditions were in effect for three or more days during our eight-day period. Official “Incident Command System” protocols were not in active on any of those days.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html",
    "href": "02-outdoor-reliability.html",
    "title": "Outdoor log reliability",
    "section": "",
    "text": "Can we find a NWS for Jordan that is close but has readings? Are there other cases like this were we don’t have readings, but can find an alternative station? Let’s try using virtualcrossing.\n\nQuestions for LM:\n\nWe filter to 1 week in July, but have 8 days. Should we include the 8th day?",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#reliability-questions",
    "href": "02-outdoor-reliability.html#reliability-questions",
    "title": "Outdoor log reliability",
    "section": "Reliability questions",
    "text": "Reliability questions\n\nHow do log temperature readings compare to the nearest weather station?\n\ndiff in temp\ndiff in heat index",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#setup-import-combine",
    "href": "02-outdoor-reliability.html#setup-import-combine",
    "title": "Outdoor log reliability",
    "section": "Setup, Import & Combine",
    "text": "Setup, Import & Combine\n\n\nExpand this to see code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(DT)\n\n\nWe’re bringing in:\n\nOur coded and cleaned outdoor log data\nHourly weather readings\nPrison unit information\n\n\n\nExpand this to see code\nlogs_all &lt;- read_rds(\"data-processed/01-outdoor-cleaned.rds\") # come from 01-outdoor-cleaning\nhourly &lt;- read_rds(\"data-processed/01-station-hourly-protocols.rds\")\nunits &lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\")\n\n\n\nClip log data\nWe have eight days of data, but we can clip it to a “week” as the last seven seven days of July, 2023. As of July 2025, we are not clipping.\n\n\nExpand this to see code\nlogs &lt;- logs_all\n# |&gt; filter(date != \"2023-07-24\")\n\nlogs |&gt; count(date) |&gt; adorn_totals() |&gt; tibble()\n\n\n\n  \n\n\n\n\n\nCombining files\nHere we bring in the unit info and hourly logs.\n\n\nExpand this to see code\n# getting station from units\nlogs_expanded &lt;- logs |&gt; \n  left_join(units, by = join_by(unit == unit_name, region)) |&gt; \n  # removing some unneeded cols\n  select(!c(unit_code, type, county, nws))\n\n# joining to get weather info\nlogs_nws &lt;- logs_expanded |&gt; \n  left_join(hourly, by = join_by(nws_id == station_id, date == date, hour == hr))\n\nlogs_expanded |&gt; filter(str_detect(unit, \"Jordan\"))\n\n\n\n  \n\n\n\n\n\nContext on matches\nHere we find the percentage of records with no NWS data.\n\n\nExpand this to see code\nmatch_checks &lt;- logs_nws |&gt; \n  mutate(match_null = if_else(is.na(name), T, F))\n\nmatch_checks |&gt; \n  tabyl(match_null) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nAbout 15 percent of our hourly logs don’t have a station match. Here TRUE values means there is missing station data.\nLet’s take a look at which ones are at issue:\n\n\nExpand this to see code\nmatch_checks |&gt; \n  tabyl(unit, match_null) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting() |&gt; \n  adorn_ns() |&gt; \n  tibble()\n\n\n\n  \n\n\n\nIn some cases we didn’t have a prison close enough. In other cases – like Jordan – we had a station, but there were no recordings for the time period.\nHere is a filtered list to more easily see units that have no NWS readings:\n\n\nExpand this to see code\nno_compare &lt;- match_checks |&gt; \n  tabyl(unit, match_null) |&gt; \n  filter(`FALSE` == 0) |&gt; \n  tibble()\n\nno_compare\n\n\n\n  \n\n\n\nWe’re going to remove these units going forward, leaving us with 82 units.\n\n\nExpand this to see code\nno_compare_units &lt;- no_compare |&gt; pull(unit)\n\nlogs_nws_compare &lt;- logs_nws |&gt; \n  filter(!unit %in% no_compare_units)\n\nlogs_nws_compare |&gt; count(unit) |&gt; nrow()\n\n\n\n\nTA: Missing weather stations\nWe don’t have a matching weather station for 11 of the 82 unites. Using visualcrossing we might be able to do a better job finding weather stations, and also to keep a better record of how far away these stations are from the units. We will do this, but later.\nALSO: In May25 work we removed stations with no matching weather station because we were looking at reliability. But if we want instead to look at protocols based on internal records, we shouldn’t remove them.",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#reliability-answers",
    "href": "02-outdoor-reliability.html#reliability-answers",
    "title": "Outdoor log reliability",
    "section": "Reliability answers",
    "text": "Reliability answers\nWe must remember these weather stations may be up to 40 miles away from the unit.\n\nHow do log temperature readings compare to the nearest weather station?\n\ndiff in temp\ndiff in heat index\n\n\nHere we find the difference in the prison recorded temp and heat index compared to the nearest station, if we have one. In some cases the diffs are NA because we don’t have a nearby station, or one of the calculating numbers is missing for whatever reason.\nWe only do this heat index calculation if one of the temperatures if above 80 degrees, because heat indexes get wonky when it is below 80.\n\n\nExpand this to see code\nlogs_diffs &lt;- logs_nws_compare |&gt; \n  mutate(\n    tmp_diff = temp - tmp,\n    hi_diff = case_when(\n      temp &gt;= 80 | tmp &gt;= 80 ~ hi_wc - hi,\n      .default = NA\n    )\n  )\n\nlogs_diffs |&gt; slice_sample(n = 20)",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#temperature-differences",
    "href": "02-outdoor-reliability.html#temperature-differences",
    "title": "Outdoor log reliability",
    "section": "Temperature differences",
    "text": "Temperature differences\n\nAverage temp difference\nHere we look across the dataset at the diff_tmp (unit temperature - weather staiton temperaure).\n\n\nExpand this to see code\nlogs_diffs |&gt; \n  summarise(\n    max_tmp_diff = max(tmp_diff, na.rm = T),\n    avg_tmp_diff = (mean(tmp_diff, na.rm = T) * 100) |&gt; round(1),\n    med_tmp_diff = median(tmp_diff, na.rm = T)\n  )\n\n\n\n  \n\n\n\n\n\nDistribution temp difference\nLet’s do a quick plot of these to see how the are distributed. i.e., how many rows are within x degrees of the weather station temperature. Note: there are 248 blank rows where we could not determine the difference because of a missing value.\n\n\nExpand this to see code\nggplot(logs_diffs, aes(x = tmp_diff)) +\n  geom_histogram(binwidth = 1, fill = \"lightblue\", color = \"black\") +\n  labs(\n    title = \"Compare temp draft\",\n    subtitle = str_wrap(\"Each bar is how many unit records are within x degrees of the closest weather station.\"))\n\n\nWarning: Removed 250 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nI can see that there are a just a couple of outliers Let’s consider removing those and see the spread mo betta.\nThere are only five records (out of 11,928) where the temp is 20 or more degrees off.\n\n\nExpand this to see code\n# the outliers\nlogs_diffs |&gt; \n  filter(abs(tmp_diff) &gt;= 20)\n\n\n\n  \n\n\n\nExpand this to see code\n# chart it\nlogs_diffs |&gt; \n  filter(abs(tmp_diff) &lt; 20) |&gt; \n  ggplot() +\n  aes(x = tmp_diff) +\n  geom_histogram(binwidth = 2, fill = \"lightblue\", color = \"black\") +\n  scale_x_continuous(breaks = seq(-20, 20, 2)) +\n  theme(panel.grid.minor.x = element_blank()) +\n  labs(\n    title = \"Most temps within about 4 degrees\",\n    subtitle = str_wrap(\"Each bar is how many unit records are within x degrees of the closest weather station. We've removed any rows that are more the 20 degrees off (of which there are five).\")\n  )\n\n\n\n\n\n\n\n\n\nHow many rows negative vs positive?\n\n\nExpand this to see code\nlogs_diffs |&gt; filter(tmp_diff &lt;= 0) |&gt; nrow() # unit is lower\n\n\n[1] 7053\n\n\nExpand this to see code\nlogs_diffs |&gt; filter(tmp_diff &gt; 0) |&gt; nrow() # unit is higher\n\n\n[1] 6329\n\n\n\n\nStandard deviation temp difference\nThis might be too technical or unneeded, but let’s look at this using the standard deviation. The standard deviation describes how much the values in the dataset typically vary from the mean (average). Like how whacked are they.\n\n\nExpand this to see code\n# The standard deviation of tmp_diff\n# logs_diffs |&gt; summarise(td_sd = sd(tmp_diff, na.rm = T))\ntmp_diff_sd &lt;- \n  logs_diffs |&gt;\n  filter(abs(tmp_diff) &lt; 20) |&gt;\n  pull(tmp_diff) |&gt; sd(na.rm = T)\n\ntmp_diff_sd\n\n\n[1] 4.131502\n\n\nExpand this to see code\n# percent outside the sd\nlogs_diffs |&gt; \n  filter(abs(tmp_diff) &lt; 20) |&gt; \n  mutate(\n    td_sd_out = case_when(\n      abs(tmp_diff) &gt; tmp_diff_sd ~ T,\n      .default = F\n    )\n  ) |&gt; \n  tabyl(td_sd_out) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nHere about 1/4 of the rows are outside the standard deviation.\n\n\nPercent temp within range\nThis might make more sense to a human: What percentage of the overall records fall within 2 degrees of the nearest weather station. How about within 4 degrees?\nWe are NOT removing outliers here.\nThe TRUE value here is the percentage of records within 2 or 4 degrees, respectively. To be clear, within 4 degrees also includes those within 2 degrees.\n\n\nExpand this to see code\ntmp_diff_2_4 &lt;- logs_diffs |&gt;\n  # filter(abs(tmp_diff) &lt; 20) |&gt; \n  filter(!is.na(tmp_diff)) |&gt; \n  mutate(in_2d = abs(tmp_diff) &lt;= 2,\n         in_4d = abs(tmp_diff) &lt;= 4)\n\ntmp_diff_2_4 |&gt; \n  tabyl(in_2d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nExpand this to see code\ntmp_diff_2_4 |&gt; \n  tabyl(in_4d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nNow that seems easier to understand than standard deviation.\n\n\nWithin range degrees by unit\nLet’s see how this differs by unit.\nin_2d_prc is the percentage of records for that unit within 2 degrees the closest weather station. in_4d_prc is the same for within 4 degrees.\n\n\nExpand this to see code\ntmp_diff_2_4 |&gt; \n  group_by(unit) |&gt; \n  summarize(\n    cnt = n(),\n    in_2d_true = sum(in_2d == T),\n    in_2d_prc = ((in_2d_true / cnt) * 100) |&gt; round(1),\n    in_4d_true = sum(in_4d == T),\n    in_4d_prc = ((in_4d_true / cnt) * 100) |&gt; round(1)\n  ) |&gt; \n  # removes the cnt true rows from display\n  select(unit, ends_with(\"prc\")) |&gt; \n  arrange(in_2d_prc)\n\n\n\n  \n\n\n\nSorted above based on lowest percentage within 2 degrees.\nNow, these numbers could really depend on how close the weather station is to the unit. For instance, I’m not happy with the choice of a station at Pearland Regional Airport for the Ramsey unit (maybe 30 miles?). The Angleton/Brazoria airport is closer (13 miles), as is Houston Southwest Airport (14 miles).\n\nWe will check all the weather stations and add distance between unit and weather station at a future date.",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#heat-index",
    "href": "02-outdoor-reliability.html#heat-index",
    "title": "Outdoor log reliability",
    "section": "Heat index",
    "text": "Heat index\n\nHI diff distribution\nLet’s do the same for heat index. We’ll skip the standard deviations and just look at the distribution and percentages.\nThere are about 50 records outside a 20 degree difference that we show here, but remove for the plot.\n\n\nExpand this to see code\nlogs_diffs |&gt; \n  filter(abs(hi_diff) &gt;= 20)\n\n\n\n  \n\n\n\nExpand this to see code\nlogs_diffs |&gt; \n  filter(abs(hi_diff) &lt; 20) |&gt;\n  ggplot() +\n  aes(x = hi_diff) +\n  geom_histogram(binwidth = 2, fill = \"lightblue\", color = \"black\") +\n  scale_x_continuous(breaks = seq(-20, 20, 2)) +\n  theme(panel.grid.minor.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nWithin HI range\nHere we find the percentage of heat index records within 2 or 4 degrees. We are NOT removing outliers here.\nTRUE means within 2 or 4 degrees, respectively.\n\n\nExpand this to see code\nhi_diff_2_4 &lt;- logs_diffs |&gt;\n  filter(!is.na(hi_diff)) |&gt; \n  mutate(in_2d = abs(hi_diff) &lt;= 2,\n         in_4d = abs(hi_diff) &lt;= 4)\n\nhi_diff_2_4 |&gt; \n  tabyl(in_2d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nExpand this to see code\nhi_diff_2_4 |&gt; \n  tabyl(in_4d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\n\n\nWithin HI range by unit\nAnd now to do that by unit. We have more cases where we don’t have the heat index for both the unit and the weather station, so we include the count of records we are comparing here.\n\n\nExpand this to see code\nhi_diff_2_4 |&gt; \n  group_by(unit) |&gt; \n  summarize(\n    cnt = n(),\n    in_2d_true = sum(in_2d == T),\n    in_2d_prc = ((in_2d_true / cnt) * 100) |&gt; round(1),\n    in_4d_true = sum(in_4d == T),\n    in_4d_prc = ((in_4d_true / cnt) * 100) |&gt; round(1)\n  ) |&gt; \n  # removes the cnt true rows from display\n  select(unit, cnt, ends_with(\"prc\")) |&gt; \n  arrange(in_2d_prc)",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  }
]