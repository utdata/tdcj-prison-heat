[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TDCJ Prison Heat",
    "section": "",
    "text": "This is for a group project under UT’s Media Innovation Group.\nIt is an examination of weather conditions recorded in prisons across the state of Texas using documents obtained through the Texas Department of Criminal Justice. More extensive sourcing information can be found on the project’s cleaning files.\nLooking at the navigation of this site, you’ll mainly want to look at the files under the Analysis heading.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#outdoor-logs",
    "href": "index.html#outdoor-logs",
    "title": "TDCJ Prison Heat",
    "section": "Outdoor logs",
    "text": "Outdoor logs\nLauren McGaughy of the Texas Newsroom public radio collaborative used a public information act to obtain the outdoor temperature logs kept by Texas prisons. The logs are hand-written and the data could not be accurately pulled through mechanical means, so a group of Media Innovation Group fellows transcribed a sample set of logs from each prison, from July 24, 2023 through July 31, 2023.\nHere is what we found:\n\nBasics and legibility\nMay 2025\n\nWe transcribed 8 days of logs for 82 units, making up about 15,700 rows of data.\nIf we found a row of the data was corrected or there was a question about the legibility, we included a note within that row. We included some consistent-worded notes so we could count them, but also had some more free-form when those categories didn’t fit. We tried to be consistent, but different humans were involved, so variability is inevitable. These based on the last seven days of July 2023.\n\nAbout 15% of the records had notes.\nAbout 1,300 records (7.5%) had corrections.\nAbout 1,000 records (6.5%) we marked with legibility issues.\n\nOf those with notes, we also counted what was at issue.\n\nThere were about 650 notes concerning the heat index/wind chill column. That column is challenging because it captures two different things (though not at the same time). Some records also include what we determined was a heat risk category category, which we recorded in a separate column.\nThere were about 590 notes about the temperature column.\n\nWe wanted to see for which units we added notes.\n\nMore than 60% of the records for the Stevenson unit had notes, and many of those were flagged as “corrections”. However, when you look at the original document, it appears the unit reviews the logs and clears up any legibility issues. i.e. having more corrections could be a positive thing.\nIn some cases we recorded an overall note about the unit as opposed to notes for each individual line. They are printed in browsable form here.\n\n\n\n\nPrecautionary measures\nAugust 2025 update\nHere we wanted to find which units should be under heat-related “precautionary measures” based on written data logs, and how that compared to actual Incident Command System activations.\nThis was challenging because the TDCJ definitions and directives to define if the unit should be in “precautionary measures” are ambiguous and confusing. See the Precautionary measures notebook for a full discussion.\nWe ended up using the following definition: When temperatures are 105+ degrees or heat index is 113+ for three or more consecutive days.\nWe use the term “unit day” to describe when any of the 82 units meets a criteria on a single day.\nOf our eight days of records from 82 prisons …\n\nSeven units met the ICS criteria on some days for a total of 17 unit days, based on the definition given to us by the TDCJ communications department. None of those units had active ICS protocols in place on those days. This count could be low since units could have met protocol in the days before our time period.\nAccording to the activation data, there were 24 days in our time period where ICS heat mitigation protocols were put in place. On those activation days, none of these units met the criteria we’ve been given by TDCJ communications based on the weather logs kept by units. That said, half of the activations are within the first two days of our study, meaning they could have met the criteria in the days before our time frame.\nOut of 656 “unit days” possible (82 units, 8 days), excessive heat conditions (105 degrees or 113 heat index) were reached 127 times. Half of the units reached the criteria at least once. In only 12 of those instances was a unit in active precautionary measures.\n\nThere are also 15 “unit days” where there was no heat index values for that day, but the unit did include a heat index “risk category” that was Cat 3 or higher. Those are potentially 113 degrees or greater and would put the day in precautionary measures territory.\n\n\nLog accuracy vs weather stations\nMay 2025\n\nI’d like to take another crack at pairing these weather stations and include the distance the weather station is from the prison unit. I’ve found a few issues, along with some solutions to them, but they’ve yet to be implemented. For now, consider these stats with caution.\n\nTo gauge the accuracy of recorded temperature and humidity/heat index we paired each prison unit with a “nearby” weather station so we could compare the temperatures.\n\nWe don’t have a good match for about 15% of the records.\nOn average, temperatures are recorded -2.9 degrees lower than the closest weather station.\nAbout half of the temperatures are within 2 degrees, and 3/4ths are within 4 degrees.\n\nYou can peruse these ranges by unit.\n\nThe heat index differences are a little harder because of so much missing data, but we’ve charted them.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#indoor-temperatures",
    "href": "index.html#indoor-temperatures",
    "title": "TDCJ Prison Heat",
    "section": "Indoor temperatures",
    "text": "Indoor temperatures\nJuly 2025 update\nWhile we did some earlier analysis of indoor temperatures in the spring, there were enough changes in the companion data that we need to check and rework it.\nHere is what we’d like to find:\n\nWhat was the temperature inside on the days that units were under active ICS protocol?\nAre there days where inside a prison it was hotter than on the hottest day under protocol?\nHow many days were above 85 degrees inside? 95? 100?",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#collaborators",
    "href": "index.html#collaborators",
    "title": "TDCJ Prison Heat",
    "section": "Collaborators",
    "text": "Collaborators\nWorking on weather station data and analysis for indoor logs:\n\nPearson Neal\nJohan Villatoro\nAli Juell\nKarina Kumar\n\nAli Juell served as project lead and Christian McDonald mentored and edited.\nThose transcribing the outdoor logs included:\n\nEmily deMotte\nTeresa Do\nNicolas Pinto\nDiego Torrealba",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html",
    "href": "02-outdoor-readability.html",
    "title": "Outdoor log readability",
    "section": "",
    "text": "We hand-coded eight days of temperatures logs (7/24/2023 to 7/31/2023) for 82 Texas prisons. Here we take those logs and try to answer the following questions.",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#questions-to-answer",
    "href": "02-outdoor-readability.html#questions-to-answer",
    "title": "Outdoor log readability",
    "section": "Questions to answer",
    "text": "Questions to answer\n\nIn the logs, how many cases did we find where the records were hard to read or had problems?\nWhat were those problems, generally?\nDo certain units have more legibility problems?\n\n\nSomething we’ve yet to look at: there are cases where the log does not have a heat index hi_wc1 recorded, but there is a temperature and humidity. We could calculate that heat index when we have the data (and then compare to what the prison log has when it is present.)\n\n\nAlso, we perhaps need to go back to cleaning to see when units recorded hi_wc as categories, which would",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#setup",
    "href": "02-outdoor-readability.html#setup",
    "title": "Outdoor log readability",
    "section": "Setup",
    "text": "Setup\n\n\nExpand this to see code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(DT)",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#import",
    "href": "02-outdoor-readability.html#import",
    "title": "Outdoor log readability",
    "section": "Import",
    "text": "Import\nWe’re bringing in:\n\nOur coded and cleaned outdoor log data\nActivation records\nHourly weather readings\nPrison unit information\n\n\n\nExpand this to see code\nlogs_all &lt;- read_rds(\"data-processed/01-outdoor-cleaned.rds\")\nactivations &lt;- read_rds(\"data-processed/01-activation-cleaned.rds\")\nhourly &lt;- read_rds(\"data-processed/01-station-hourly-protocols.rds\")\nunits &lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\")",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#basic-information",
    "href": "02-outdoor-readability.html#basic-information",
    "title": "Outdoor log readability",
    "section": "Basic information",
    "text": "Basic information\nPeek at a sample\n\n\nExpand this to see code\nlogs_all |&gt; slice_sample(n = 5)\n\n\n\n  \n\n\n\nand glimpse the columns …\n\n\nExpand this to see code\nlogs_all |&gt; glimpse()\n\n\nRows: 15,744\nColumns: 13\n$ unit     &lt;chr&gt; \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd…\n$ region   &lt;chr&gt; \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"…\n$ date     &lt;date&gt; 2023-07-24, 2023-07-24, 2023-07-24, 2023-07-24, 2023-07-24, …\n$ rec      &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"…\n$ datetime &lt;dttm&gt; 2023-07-24 00:30:00, 2023-07-24 01:30:00, 2023-07-24 02:30:0…\n$ hour     &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ temp     &lt;dbl&gt; 83, 82, 81, 81, 80, 79, 79, 80, 83, 86, 90, 91, 92, 96, 100, …\n$ humid    &lt;dbl&gt; 54, 62, 67, 71, 74, 76, NA, 79, 72, 69, 61, 57, 53, 44, 36, 3…\n$ wind     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hi_wc    &lt;dbl&gt; 65, 68, 69, 71, 71, 71, NA, 84, 86, 95, 100, 100, 100, 103, 1…\n$ hi_wc_n  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ person   &lt;chr&gt; \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. J…\n$ notes    &lt;chr&gt; NA, NA, NA, NA, \"hi_wc corrected\", \"humid corrected\", NA, NA,…\n\n\nDate range of the data\n\n\nExpand this to see code\nlogs_all$date |&gt; summary()\n\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2023-07-24\" \"2023-07-25\" \"2023-07-27\" \"2023-07-27\" \"2023-07-29\" \"2023-07-31\" \n\n\n\nCount units\n\n\nExpand this to see code\nlogs_all |&gt; count(unit)\n\n\n\n  \n\n\n\n\n\nClip log data\nWe’ll remove July 24th so we have the last seven days of July, 2023.\n\n\nExpand this to see code\nlogs &lt;- logs_all |&gt; filter(date != \"2023-07-24\")\n\nlogs |&gt; count(date) |&gt; adorn_totals() |&gt; tibble()\n\n\n\n  \n\n\n\n\n\nTA: Basics\nWe transcribed outdoor temperature logs from 82 different units within the Texas prison system. Each log had 192 entries (24 hours for 8 days). We’ve clipped these records to be the last 7 days of July 2023, for a total of 13,776 records.",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#readability-answers",
    "href": "02-outdoor-readability.html#readability-answers",
    "title": "Outdoor log readability",
    "section": "Readability answers",
    "text": "Readability answers\nWhen we transcribed the outdoor temperature logs, we added notes when something was illegible or corrected on the form. Here we analyze those notes.\n\nRecords with notes\nHere we set flags if a record (an hour within a log) had a note, along with some categories. The result here is just a record sample to check our work.\n\n\nExpand this to see code\nlogs_notes &lt;- logs |&gt;\n  mutate(\n    notes_a = if_else(is.na(notes), F, T),\n    notes_c = case_when(str_detect(notes, \"correct\") ~ T, .default = F),\n    notes_l = case_when(str_detect(notes, \"legi\") ~ T, .default = F),\n  )\n\nlogs_notes |&gt; \n  select(unit, date, starts_with(\"notes\")) |&gt; \n  slice_sample(n = 20)\n\n\n\n  \n\n\n\nThis is the total percentage of records where we included some kind of note. TRUE means we included a note.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  tabyl(notes_a) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nWhen we recorded notes, we had some standardization. We included the term “corrected” if a record was scratched out and replaced with a new value or otherwise amended.\nThis is the percentage of records where something was corrected, per our notes.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  tabyl(notes_c) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nIf there was a legibility problem, we included the term “legibility”. This is the percentage of records where we noted some kind of legibility problem.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  tabyl(notes_l) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\n\n\nTA: Notes statistics\nWe have some kind of note in about 15% of all records. About 7.5% of records had a correction of some kind, and about 6.5% had legibility issues. (Some records might have both.). We should remember that data fellows had to use personal judgement on what to record and how, and that four different individuals performed the transcriptions. We tried our best to be consistent, but we are humans.\n\n\nTypes of notes\nSome records have more than one note. Here we “explode” those to count the notes individually. In some cases we had some standard notes, in other cases we didn’t.\nThe result here is just a sample to check our work.\n\n\nExpand this to see code\nnotes_exploded &lt;- logs |&gt; \n  select(unit, date, notes) |&gt; \n  filter(!is.na(notes)) |&gt; \n  group_by(unit, date) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  ungroup()\n\n# number of rows in new tibble\nnotes_exploded |&gt; nrow()\n\n\n[1] 2652\n\n\nExpand this to see code\n# sample rows\nnotes_exploded |&gt; slice_sample(n = 10)\n\n\n\n  \n\n\n\nLet’s look at the kinds of notes we recorded.\n\n\nExpand this to see code\nnotes_exploded_cnts &lt;- notes_exploded |&gt; count(notes, sort = T)\n\nnotes_exploded_cnts\n\n\n\n  \n\n\n\nHere we count how many individual records we labeled as “corrected” or had a “legibility” concern. I also counted a couple of other instances I saw, like where a “double” or “range” of values were recorded.\n\n\nExpand this to see code\nnotes_exploded_cnts |&gt;\n  # filter(str_detect(notes, \"corrected\")) |&gt; \n  summarise(\n    total_indiv_corrected = sum(n[str_detect(notes, \"correct\")]),\n    total_indiv_legibility = sum(n[str_detect(notes, \"legib\")]),\n    total_indiv_double = sum(n[str_detect(notes, \"double\")]),\n    total_indiv_range = sum(n[str_detect(notes, \"range\")]),\n    ) |&gt; \n  pivot_longer(cols = everything())\n\n\n\n  \n\n\n\n\n\nTA: Types statistics\nMost of the individual notes identified (about 1,300) were corrections of some kind. About 1,000 of them concerned legibility.\n\n\nVariable counts\nHere we try to get a handle on which variables recorded had the most notes (i.e., temp vs wind, etc.). We are counting how many times our variable terms were included in individual notes.\n\n\nExpand this to see code\nnotes_exploded_cnts |&gt;\n  # filter(str_detect(notes, \"corrected\")) |&gt; \n  summarise(\n    total_indiv_temp = sum(n[str_detect(notes, \"temp\")]),\n    total_indiv_humid = sum(n[str_detect(notes, \"humid\")]),\n    total_indiv_wind = sum(n[str_detect(notes, \"wind\")]),\n    total_indiv_hi_wc = sum(n[str_detect(notes, \"hi_wc\")]),\n    total_indiv_hi_wc_n = sum(n[str_detect(notes, \"hi_wc_n\")]),\n    total_indiv_person = sum(n[str_detect(notes, \"person\")]),\n  ) |&gt; \n  pivot_longer(everything()) |&gt; \n  arrange(value |&gt; desc())\n\n\n\n  \n\n\n\nWe had more notes on the hi_wc variable (Heat index/Wind chill) than any other, followed by temperature.\nThe heat index/wind chill record is already challenging because it measures two different things. Some records would also sometimes include what we determined was a heat index “category”, which we recorded in a separate column.\nHere we see how much that field was at issue by counting any notes that included hi_wc. The heat index category notes show up in this list, too.\n\n\nExpand this to see code\nnotes_exploded_cnts |&gt; \n  filter(str_detect(notes, \"hi_wc\")) |&gt; \n  adorn_totals() |&gt; tibble()\n\n\n\n  \n\n\n\n\n\nTA: Variable statistics\nThe heat index/wind chill columns was at issue about 650 times, with about half of these being corrections.\n\n\nNotes by unit\n\nUnits with most notes\nThis looks at which units had the most notes of any kind. The pct_notes is the percetage of rows that had a note of any kind.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  count(unit, notes_a) |&gt; \n  pivot_wider(names_from = notes_a, values_from = n) |&gt; \n  mutate(pct_notes = ((`TRUE` / (`FALSE` + `TRUE`)) * 100) |&gt; round(1)) |&gt; \n  arrange(pct_notes |&gt; desc())\n\n\n\n  \n\n\n\nTo gain some insight on what these might be, let’s look at these notes by Stevenson unit.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  filter(unit == \"Stevenson\" & !is.na(notes)) |&gt; \n  select(unit, region, date, hour, notes)\n\n\n\n  \n\n\n\nIt looks like there are many “corrections”. When you look at the original documents, it appears the unit must review the logs and regularly clears up any legibility issues. The logs are also signed. i.e. having more corrections could be a positive thing.\n\n\nUnit corrections, legibilty\nHere we explode all the notes for all the units and count how many are for corrections and legibility.\nWe sort the same list twice … once by corrections and once by legibility.\n\n\nExpand this to see code\nunits_cor_leg &lt;- logs_notes |&gt; \n  filter(!is.na(notes)) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  count(unit, region, notes, sort = T) |&gt; \n  group_by(unit, region) |&gt; \n  summarise(\n    total_indiv_correct = sum(n[str_detect(notes, \"correct\")]),\n    total_indiv_legibility = sum(n[str_detect(notes, \"legib\")]),\n    .groups = \"drop\"\n    )\n\nunits_cor_leg |&gt; \n  arrange(total_indiv_correct |&gt; desc())\n\n\n\n  \n\n\n\nExpand this to see code\nunits_cor_leg |&gt; \n  arrange(total_indiv_legibility |&gt; desc())\n\n\n\n  \n\n\n\nLet’s look at bit more at Telford to see the legibility issues.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  filter(unit == \"Telford\" & !is.na(notes)) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  count(notes, sort = T)\n\n\n\n  \n\n\n\nAnd then Glossbrenner …\n\n\nExpand this to see code\nlogs_notes |&gt; \n  filter(unit == \"Glossbrenner\" & !is.na(notes)) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  count(notes, sort = T)\n\n\n\n  \n\n\n\n\n\n\nTA: Notes by unit\nThe units with the most correction notes include Garza West, Briscoe, Stevenson, Connally, Dominguez. Given what we found with Stevenson, this may mean they are more accurate, but they should be reviewed.\nWhen it comes to legibility, the Telford unit stands out. Most of the issues are around the signature of the person recording the record.",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#unit-notes",
    "href": "02-outdoor-readability.html#unit-notes",
    "title": "Outdoor log readability",
    "section": "Unit notes",
    "text": "Unit notes\nIn some cases we recorded an overall note about the unit as opposed to notes for each individual line. I’m just printing all of these out for perusal.\n\n\nExpand this to see code\noverall_notes &lt;- read_rds(\"data-processed/01-outdoor-notes.rds\")\n\noverall_notes |&gt; datatable()",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "01.1-indoor-combining.html",
    "href": "01.1-indoor-combining.html",
    "title": "Combine indoor logs",
    "section": "",
    "text": "I’ve created station files that already have the protocol flags. This notebook could bring those in, but I’m not sure it is any more clear. I might trash those protocol data files."
  },
  {
    "objectID": "01.1-indoor-combining.html#to-do",
    "href": "01.1-indoor-combining.html#to-do",
    "title": "Combine indoor logs",
    "section": "",
    "text": "I’ve created station files that already have the protocol flags. This notebook could bring those in, but I’m not sure it is any more clear. I might trash those protocol data files."
  },
  {
    "objectID": "01.1-indoor-combining.html#overview",
    "href": "01.1-indoor-combining.html#overview",
    "title": "Combine indoor logs",
    "section": "Overview",
    "text": "Overview\nHere we bring together our indoor readings together with our station readings, and then create our protocol flags.\nWe’ll compare two ways to handle the station data:\n\nFind the 3 p.m. hours to match when the prison is supposed to record their records.\nThe daily summaries that have the hottest temp/heat index."
  },
  {
    "objectID": "01.1-indoor-combining.html#load-our-libraries",
    "href": "01.1-indoor-combining.html#load-our-libraries",
    "title": "Combine indoor logs",
    "section": "Load our libraries",
    "text": "Load our libraries\nlibrary(data.table)\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "01.1-indoor-combining.html#downloading-weather-logs-files",
    "href": "01.1-indoor-combining.html#downloading-weather-logs-files",
    "title": "Combine indoor logs",
    "section": "Downloading weather-logs files",
    "text": "Downloading weather-logs files\nWe’ll download a copy of our cleaned weather-logs into this repo so we don’t have to rely on the original.\nThe station files come from the weather-logs repo. That repo is currently private, so we’ll manually download the files into data-processed/weather-logs."
  },
  {
    "objectID": "01.1-indoor-combining.html#import-our-files",
    "href": "01.1-indoor-combining.html#import-our-files",
    "title": "Combine indoor logs",
    "section": "Import our files",
    "text": "Import our files\n\nstations_daily &lt;- read_rds(\"data-processed/weather-logs/01-station-daily-summary.rds\")\nstations_hourly &lt;- read_rds(\"data-processed/weather-logs/01-station-hourly-readings.rds\")\nheat_warnings &lt;- read_rds(\"data-processed/weather-logs/01-heat-warning-cleaned.rds\")\nactivation &lt;- read_rds(\"data-processed/01-activation-cleaned.rds\")\nindoor &lt;-  read_rds(\"data-processed/01-indoor-cleaned.rds\")\nunit_info&lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\")"
  },
  {
    "objectID": "01.1-indoor-combining.html#combine-data-files",
    "href": "01.1-indoor-combining.html#combine-data-files",
    "title": "Combine indoor logs",
    "section": "Combine data files",
    "text": "Combine data files\nNow that we’ve brought all of the files into our environment, it’s time to put them all together!\nThe process we’ll use is this:\n\nstart with indoor readings\njoin with unit-info to get the station ids\njoin with the stations to get that information\ncreate our protocol flags\nexport\n\nFirst we’ll combine our unit temperature logs with our activation status. We’ll also take this time to fill in our inactives because our original file only denoted activation."
  },
  {
    "objectID": "01.1-indoor-combining.html#indoor-unit-info",
    "href": "01.1-indoor-combining.html#indoor-unit-info",
    "title": "Combine indoor logs",
    "section": "Indoor + unit info",
    "text": "Indoor + unit info\n\nindoor_unit &lt;- indoor |&gt; \n  left_join(unit_info, by = join_by(unit == unit_name)) |&gt; \n  select(unit, date, unit_temp, nws_id, county) |&gt; \n  arrange(unit, date)\n\n# sample of data\nindoor_unit |&gt; slice_sample(n = 10)\n\n\n  \n\n\n# total row count\nindoor_unit |&gt; nrow()\n\n[1] 12323\n\n# missing join count\nindoor_unit |&gt; filter(is.na(nws_id))"
  },
  {
    "objectID": "01.1-indoor-combining.html#add-stations",
    "href": "01.1-indoor-combining.html#add-stations",
    "title": "Combine indoor logs",
    "section": "Add stations",
    "text": "Add stations\nI did check and the daily summaries have more possible matches than pulling 3p-only values. Here we join by both the station id and date.\n\nindoor_stations &lt;- indoor_unit |&gt; \n  left_join(stations_daily, by = join_by(nws_id == station_id, date))\n\nindoor_stations |&gt; nrow()\n\n[1] 12323\n\nindoor_stations |&gt; head()\n\n\n  \n\n\n\nUnfortunately there are There are 1580 indoor records (out of 12323) without a matching station record.\n\nindoor_stations_peek &lt;- indoor_stations |&gt; \n  group_by(unit, nws_id) |&gt;\n  summarize(\n    total_cnt = n(),\n    na_cnt = sum(is.na(name))\n  ) |&gt; \n  ungroup() |&gt; \n  adorn_totals(\"row\") |&gt; \n  tibble()\n\n`summarise()` has grouped output by 'unit'. You can override using the\n`.groups` argument.\n\nindoor_stations_peek\n\n\n  \n\n\n\nIn some cases there are just missing days of readings, which could be for any number of reasons, like the observation station was down.\nBut there are a number of prison units where we could not find a NWS weather data within 40 miles. These fit into that category.\n\nindoor_stations_peek |&gt; \n  filter(na_cnt &gt; 0,\n         total_cnt == na_cnt) |&gt; \n  arrange(unit)\n\n\n  \n\n\n\n\nRemove non-matches\nWe’ll remove indoor records where we don’t have a matching weather temperature since we can’t do any comparison here. (May decide later we need this, but later code might have to change to deal with missing variables.)\n\nindoor_clipped &lt;- indoor_stations |&gt; \n  filter(!is.na(name))\n\nindoor_clipped |&gt; nrow()\n\n[1] 10743"
  },
  {
    "objectID": "01.1-indoor-combining.html#create-a-flag",
    "href": "01.1-indoor-combining.html#create-a-flag",
    "title": "Combine indoor logs",
    "section": "Create a flag",
    "text": "Create a flag\nWe know that one form of criteria for our heat protocol is if the feels like is greater than or equal to 113 for three days. Let’s create a flag for days greater than or equal to 113 and then count the number of consecutive days.\n\nindoor_flags &lt;- indoor_clipped |&gt; \n  mutate(\n    tmp_flag = (case_when(\n      tmp_high &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi_high &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == \"TRUE\" | hi_flag == \"TRUE\" ~ TRUE,\n      .default = FALSE)\n    )\n  \nindoor_flags"
  },
  {
    "objectID": "01.1-indoor-combining.html#look-for-runs",
    "href": "01.1-indoor-combining.html#look-for-runs",
    "title": "Combine indoor logs",
    "section": "Look for runs",
    "text": "Look for runs\nCreate if either are true column Add another column that showsd if true is along with active\n\nindoor_protocol &lt;- indoor_flags |&gt; \n  group_by(unit) |&gt; \n  mutate(\n    protocol_run = accumulate(protocol_flag, ~if_else(.y, .x + 1, 0))\n  ) |&gt; \n  mutate(\n    protocol_met = case_when(\n    protocol_run &gt;= 3 ~ TRUE,\n    .default = FALSE\n  )) |&gt; \n  ungroup()\n\nindoor_protocol |&gt; \n  select(unit, date, starts_with(\"protocol\")) |&gt; \n  # skipping lines to get to true values\n  slice(80:100)"
  },
  {
    "objectID": "01.1-indoor-combining.html#add-activation-days",
    "href": "01.1-indoor-combining.html#add-activation-days",
    "title": "Combine indoor logs",
    "section": "Add activation days",
    "text": "Add activation days\nNow we’ll add a column that shows which prisons were actively under heat protocols.\n\nindoor_active &lt;- indoor_protocol |&gt; \n  left_join(activation) |&gt; \n  mutate(protocol_active = if_else(is.na(protocol_active), F, T),\n         protocol_fail = case_when(\n           protocol_met == T & protocol_active == F ~ T,\n           .default = F\n         ))\n\nJoining with `by = join_by(unit, date)`\n\nindoor_active |&gt; filter(protocol_met == T)"
  },
  {
    "objectID": "01.1-indoor-combining.html#consider-heat-warnings",
    "href": "01.1-indoor-combining.html#consider-heat-warnings",
    "title": "Combine indoor logs",
    "section": "Consider heat warnings",
    "text": "Consider heat warnings\nThere are only six cases where we’ve found a heat warning in effect. This could be because we don’t have a good match on these warnings unless it is set for the whole county.\nWe won’t save this join since it is of little use.\n\n# add later\nindoor_active |&gt;\n  left_join(heat_warnings, by = join_by(nws_id == station_code, date)) |&gt; \n  filter(ehw_active == T)"
  },
  {
    "objectID": "01.1-indoor-combining.html#export",
    "href": "01.1-indoor-combining.html#export",
    "title": "Combine indoor logs",
    "section": "Export",
    "text": "Export\nWe’ve put everything together! Now let’s export it into data-processed.\n\nindoor_protocol |&gt;\n  write_rds(\"data-processed/02-indoor-combined.rds\")"
  },
  {
    "objectID": "01-station-protocols.html",
    "href": "01-station-protocols.html",
    "title": "Stations cleaning",
    "section": "",
    "text": "THIS HAS SOME ISSUES. IF WE ARE FOLLOWING THE EXCESSIVE HEAT WARNING CRITERIA, THEN WE ARE NOT LOOKING AT 105 FOR 2 DAYS.\nHere we enhance our weather station data to include prison protocol data. We do this both for our hourly readings and our daily summaries.\nWill decide later if we add the prison info. That may need to be in more specific analysis.\nlibrary(tidyverse)\nlibrary(janitor)",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html#flag-function",
    "href": "01-station-protocols.html#flag-function",
    "title": "Stations cleaning",
    "section": "Flag function",
    "text": "Flag function\nHere we want a function that will create the flags, once fed a tmp and heat index.\n\nset_protocol_flag &lt;- function(df, tmp_name, hi_name) {\n    df |&gt; mutate(\n    tmp_flag = (case_when(\n      tmp_name &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi_name &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == TRUE | hi_flag == TRUE ~ TRUE,\n      .default = FALSE)\n    )\n}",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html#hourly-data",
    "href": "01-station-protocols.html#hourly-data",
    "title": "Stations cleaning",
    "section": "Hourly data",
    "text": "Hourly data\n\nImport hourly data\n\nstations_hourly &lt;- read_rds(\"data-processed/weather-logs/01-station-hourly-readings.rds\")\n\n\n\nAdd protocol flags\n\nstations_hourly_protocol &lt;- stations_hourly |&gt; \n   mutate(\n    tmp_flag = (case_when(\n      tmp &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == TRUE | hi_flag == TRUE ~ TRUE,\n      .default = FALSE)\n    )\n\n# check results\nstations_hourly_protocol |&gt;\n  select(-name) |&gt; \n  filter(tmp &gt; 100) |&gt; \n  slice_sample(n = 10)\n\n\n  \n\n\n\n\n\nExport hourly\n\nstations_hourly_protocol |&gt; \n  write_rds(\"data-processed/01-station-hourly-protocols.rds\")",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html#daily-summarized-data",
    "href": "01-station-protocols.html#daily-summarized-data",
    "title": "Stations cleaning",
    "section": "Daily summarized data",
    "text": "Daily summarized data\n\nImport daily summaries\n\nstations_daily &lt;- read_rds(\"data-processed/weather-logs/01-station-daily-summary.rds\")\n\nstations_daily |&gt; glimpse()\n\nRows: 17,453\nColumns: 5\n$ station_id &lt;chr&gt; \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KA…\n$ name       &lt;chr&gt; \"ABILENE REGIONAL AIRPORT, TX US\", \"ABILENE REGIONAL AIRPOR…\n$ date       &lt;date&gt; 2023-01-01, 2023-01-02, 2023-01-03, 2023-01-04, 2023-01-05…\n$ tmp_high   &lt;dbl&gt; 78, 76, 60, 64, 63, 77, 63, 63, 77, 83, 83, 60, 64, 67, 67,…\n$ hi_high    &lt;dbl&gt; 76.7, 74.8, 56.9, 61.0, 60.2, 76.1, 62.1, 60.5, 75.3, 80.4,…\n\n\n\n\nAdd daily protocols\n\nstations_daily_protocol &lt;- stations_daily |&gt; \n  mutate(\n    tmp_flag = (case_when(\n      tmp_high &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi_high &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == \"TRUE\" | hi_flag == \"TRUE\" ~ TRUE,\n      .default = FALSE)\n  )\n\n# check results\nstations_daily_protocol |&gt; \n  select(-name) |&gt; \n  filter(tmp_high &gt; 100) |&gt; \n  slice_sample(n = 10)\n\n\n  \n\n\n\n\n\nExport daily\n\nstations_daily_protocol |&gt; write_rds(\"data-processed/01-station-daily-protocols.rds\")",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html",
    "href": "01-indoor-cleaning.html",
    "title": "Indoor logs cleaning",
    "section": "",
    "text": "This data is the record indoor temperatures at each unit from April to September. This information was acquired by Lauren McGaughy at KUT through a public information request to the Texas Department of Criminal Justice.\nThe original PDFs were transcribed into data using Google Pinpoint, resulting in the csv files found in data-original as Indoor-30days-edited.csv and Indoor-31days-edited.csv. The original documents are found in data-original/2023_indoor.",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#goals-of-this-notebook",
    "href": "01-indoor-cleaning.html#goals-of-this-notebook",
    "title": "Indoor logs cleaning",
    "section": "Goals of this notebook",
    "text": "Goals of this notebook\nWhat we’ll do to prepare the data:\n\nDownload the data\nImport it into our notebook\nClean up data types and columns\nExport data into our next notebook",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#setup",
    "href": "01-indoor-cleaning.html#setup",
    "title": "Indoor logs cleaning",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(janitor)",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#downloading-data",
    "href": "01-indoor-cleaning.html#downloading-data",
    "title": "Indoor logs cleaning",
    "section": "Downloading data",
    "text": "Downloading data\nThis data comes from a public information request to the Texas Department of Criminal Justice. We have two seperate sheets with our 30-day months and our 31-day months, so we’ll read each of those in separately.\n\nindoor_temps &lt;- read_csv(\"data-original/Indoor-30days-edited.csv\")\n\nRows: 201 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): File Name, Unit, Validation Link\ndbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nindoor_temps |&gt; glimpse()\n\nRows: 201\nColumns: 33\n$ `File Name`       &lt;chr&gt; \"SB1R56 - April 2023_1.pdf\", \"SB1R56 - April 2023_1.…\n$ Unit              &lt;chr&gt; \"Allred\", \"Beto\", \"Boyd\", \"Bradshaw\", \"Briscoe\", \"By…\n$ `1`               &lt;dbl&gt; 75.5, 73.4, 76.6, 80.1, 84.0, 76.9, 77.5, 71.4, 76.4…\n$ `2`               &lt;dbl&gt; 79.0, 73.9, 76.5, 82.1, 88.0, 74.0, 79.1, 72.8, 76.1…\n$ `3`               &lt;dbl&gt; 78.1, 78.6, 81.3, 86.3, 91.2, 81.1, 78.6, 74.7, 80.8…\n$ `4`               &lt;dbl&gt; 80.5, 79.4, 82.5, 80.5, 88.0, 81.2, 75.6, 73.3, 82.2…\n$ `5`               &lt;dbl&gt; 74.3, 74.2, 77.0, 73.8, 76.4, 75.4, 81.5, 71.4, 73.8…\n$ `6`               &lt;dbl&gt; 74.2, 69.9, 68.7, 68.5, 67.7, 72.7, 73.2, 72.3, 66.3…\n$ `7`               &lt;dbl&gt; 76.1, 69.6, 72.1, 70.2, 69.1, 65.1, 67.0, 73.3, 76.4…\n$ `8`               &lt;dbl&gt; 77.9, 71.3, 74.2, 78.4, 73.1, 72.3, 67.5, 72.5, 78.5…\n$ `9`               &lt;dbl&gt; 79.0, 73.2, 74.2, 73.0, 76.5, 69.8, 72.0, 73.2, 75.7…\n$ `10`              &lt;dbl&gt; 78.4, 71.6, 73.1, 79.1, 75.6, 75.8, 72.2, 73.3, 72.1…\n$ `11`              &lt;dbl&gt; 78.9, 72.1, 78.8, 78.5, 80.6, 76.9, 71.1, 74.4, 77.1…\n$ `12`              &lt;dbl&gt; 78.3, 72.5, 77.0, 78.3, 80.0, 73.0, 72.4, 75.0, 77.2…\n$ `13`              &lt;dbl&gt; 79.7, 72.6, 76.5, 75.8, 81.0, 76.0, 74.7, 74.9, 77.2…\n$ `14`              &lt;dbl&gt; 80.4, 73.4, 77.2, 85.3, 88.6, 74.0, 70.9, 74.4, 75.8…\n$ `15`              &lt;dbl&gt; 77.1, 79.2, 80.9, 84.9, 90.6, 73.1, 76.4, 69.8, 79.9…\n$ `16`              &lt;dbl&gt; 76.3, 70.9, 73.3, 82.0, 88.2, 72.0, 72.6, 67.0, 71.0…\n$ `17`              &lt;dbl&gt; 77.8, 70.8, 76.1, 82.2, 79.4, 77.2, 73.1, 69.3, 77.0…\n$ `18`              &lt;dbl&gt; 82.2, 71.9, 72.3, 73.7, 83.0, 73.0, 76.5, 77.6, 72.6…\n$ `19`              &lt;dbl&gt; 81.3, 74.9, 77.8, 82.1, 80.0, 72.5, 75.6, 72.5, 76.5…\n$ `20`              &lt;dbl&gt; 76.9, 75.8, 78.9, 80.1, 85.0, 76.0, 75.3, 69.6, 79.8…\n$ `21`              &lt;dbl&gt; 74.4, 75.9, 74.8, 78.1, 82.5, 73.0, 75.5, 74.1, 76.6…\n$ `22`              &lt;dbl&gt; 75.4, 72.3, 75.6, 80.7, 81.0, 71.0, 74.2, 73.5, 73.8…\n$ `23`              &lt;dbl&gt; 77.0, 65.3, 65.0, 72.6, 67.9, 76.8, 66.1, 61.0, 76.4…\n$ `24`              &lt;dbl&gt; 71.6, 69.1, 73.0, 78.2, 72.0, 76.0, 67.3, 61.7, 76.8…\n$ `25`              &lt;dbl&gt; 77.1, 70.1, 73.3, 79.7, 77.0, 72.6, 72.9, 64.7, 75.7…\n$ `26`              &lt;dbl&gt; 76.3, 71.2, 73.1, 74.0, 83.6, 71.5, 77.1, 62.2, 76.7…\n$ `27`              &lt;dbl&gt; 76.9, 71.4, 73.5, 74.8, 83.2, 67.9, 70.3, 61.9, 78.5…\n$ `28`              &lt;dbl&gt; 77.2, 71.6, 78.6, 80.5, 86.3, 74.5, 78.9, 60.7, 77.4…\n$ `29`              &lt;dbl&gt; 71.8, 67.7, 73.0, 76.0, 77.1, 67.1, 68.8, 61.9, 76.3…\n$ `30`              &lt;dbl&gt; 77.0, 71.7, 81.3, 79.1, 84.3, 69.6, 74.9, 63.9, 78.5…\n$ `Validation Link` &lt;chr&gt; \"https://journaliststudio.google.com/pinpoint-extrac…\n\n\n\nother_indoor_temps &lt;- read_csv(\"data-original/Indoor-31days-edited.csv\")\n\nRows: 203 Columns: 34\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): File Name, Unit, Validation Link\ndbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nother_indoor_temps |&gt; glimpse()\n\nRows: 203\nColumns: 34\n$ `File Name`       &lt;chr&gt; \"SB1R56 - August 2023_1.pdf\", \"SB1R56 - August 2023_…\n$ Unit              &lt;chr&gt; \"Allred\", \"Beto\", \"Boyd\", \"Bradshaw\", \"Briscoe\", \"By…\n$ `1`               &lt;dbl&gt; 97.7, 91.9, 95.1, 98.2, 96.0, 93.8, 84.3, 89.3, 97.0…\n$ `2`               &lt;dbl&gt; 99.7, 94.4, 97.0, 97.7, 94.1, 94.2, 87.4, 89.0, 93.8…\n$ `3`               &lt;dbl&gt; 95.6, 92.8, 97.4, 98.4, 94.9, 93.4, 85.6, 88.8, 92.7…\n$ `4`               &lt;dbl&gt; 97.6, 91.1, 96.0, 97.6, 91.7, 93.7, 83.0, 88.0, 94.0…\n$ `5`               &lt;dbl&gt; 100.5, 93.8, 96.4, 97.8, 95.1, 93.9, 84.2, 89.3, 92.…\n$ `6`               &lt;dbl&gt; 97.1, 94.7, 94.9, 97.4, 95.3, 90.1, 87.2, 85.1, 93.7…\n$ `7`               &lt;dbl&gt; 93.8, 92.3, 94.5, 94.5, 95.5, 95.2, 84.9, 82.5, 93.5…\n$ `8`               &lt;dbl&gt; 92.3, 93.1, 95.3, 95.3, 95.4, 94.0, 89.4, 83.5, 93.1…\n$ `9`               &lt;dbl&gt; 95.8, 91.3, 94.4, 92.8, 98.1, 92.5, 89.9, 83.9, 94.5…\n$ `10`              &lt;dbl&gt; 93.0, 93.9, 95.3, 95.9, 97.5, 92.9, 91.7, 84.0, 96.2…\n$ `11`              &lt;dbl&gt; 99.1, 92.7, 93.2, 95.4, 96.1, 92.8, 90.2, 87.3, 94.2…\n$ `12`              &lt;dbl&gt; 100.1, 93.7, 95.4, 96.3, 97.2, 93.0, 88.6, 87.2, 94.…\n$ `13`              &lt;dbl&gt; 97.5, 92.4, 95.6, 95.0, 98.1, 92.6, 86.2, 86.0, 93.8…\n$ `14`              &lt;dbl&gt; 83.3, 93.2, 95.0, 94.2, 96.3, 92.3, 88.6, 81.4, 94.9…\n$ `15`              &lt;dbl&gt; 85.9, 89.0, 93.7, 92.1, 94.6, 91.2, 87.9, 81.0, 91.1…\n$ `16`              &lt;dbl&gt; 87.5, 86.8, 91.1, 89.7, 94.5, 90.2, 89.5, 83.5, 88.8…\n$ `17`              &lt;dbl&gt; 94.3, 90.3, 94.8, 94.6, 96.7, 90.4, 90.5, 87.9, 89.6…\n$ `18`              &lt;dbl&gt; 97.9, 93.2, 98.0, 89.7, 94.1, 91.8, 90.1, 82.0, 95.1…\n$ `19`              &lt;dbl&gt; 95.0, 91.9, 94.9, 95.2, 95.0, 91.9, 89.4, 85.7, 96.5…\n$ `20`              &lt;dbl&gt; 96.3, 94.9, 97.6, 96.1, 98.2, 92.9, 88.7, 85.9, 99.1…\n$ `21`              &lt;dbl&gt; 93.4, 94.2, 98.2, 96.1, 93.7, 93.3, 87.2, 87.0, 96.6…\n$ `22`              &lt;dbl&gt; 95.7, 92.3, 96.1, 98.1, 85.6, 92.1, 84.4, 89.9, 93.6…\n$ `23`              &lt;dbl&gt; 97.8, 94.3, 96.2, 98.6, 89.5, 91.4, 89.1, 89.4, 94.8…\n$ `24`              &lt;dbl&gt; 99.1, 95.7, 98.9, 98.5, 92.2, 94.2, 91.7, 86.0, 99.5…\n$ `25`              &lt;dbl&gt; 95.6, 96.2, 98.0, 98.9, 96.2, 92.2, 90.9, 87.8, 95.6…\n$ `26`              &lt;dbl&gt; 99.3, 96.3, 98.7, 98.6, 93.2, 95.4, 90.7, 84.8, 99.1…\n$ `27`              &lt;dbl&gt; 97.3, 96.5, 99.6, 98.1, 96.3, 93.2, 85.7, 83.8, 101.…\n$ `28`              &lt;dbl&gt; 88.0, 86.3, 89.9, 84.3, 97.1, 86.2, 86.9, 84.0, 86.2…\n$ `29`              &lt;dbl&gt; 87.7, 87.1, 90.7, 89.3, 93.8, 88.2, 88.9, 81.7, 88.6…\n$ `30`              &lt;dbl&gt; 87.8, 87.3, 91.8, 89.1, 95.4, 86.5, 88.8, 81.6, 88.8…\n$ `31`              &lt;dbl&gt; 88.6, 87.6, 91.8, 91.2, 97.2, 81.9, 87.9, 81.0, 88.0…\n$ `Validation Link` &lt;chr&gt; \"https://journaliststudio.google.com/pinpoint-extrac…",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#pivot-longer",
    "href": "01-indoor-cleaning.html#pivot-longer",
    "title": "Indoor logs cleaning",
    "section": "Pivot Longer",
    "text": "Pivot Longer\nRight now each day is a column, let’s change it so days are all under one column and clean up the names of our columns.\nWe’ll do this individually before we join the columns so we don’t have to remove the 31st rows for months with only 30 days.\n\nother_temps_raw &lt;- other_indoor_temps |&gt; \n  pivot_longer(\n    cols = \"1\":\"31\",\n    names_to = \"day\",\n    values_to = \"temperature\"\n  )\n\nother_temps_raw\n\n\n  \n\n\n\n\nindoor_temps_raw &lt;- indoor_temps |&gt; \n  pivot_longer(\n    cols = \"1\":\"30\",\n    names_to = \"day\",\n    values_to = \"temperature\"\n  )\n\nindoor_temps_raw",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#combine-data-frames",
    "href": "01-indoor-cleaning.html#combine-data-frames",
    "title": "Indoor logs cleaning",
    "section": "Combine data frames",
    "text": "Combine data frames\nLet’s combine both of our datasets to make a single table.\n\nindoor_temps_combined &lt;- bind_rows(other_temps_raw, indoor_temps_raw)\n\nindoor_temps_combined",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#clean-names",
    "href": "01-indoor-cleaning.html#clean-names",
    "title": "Indoor logs cleaning",
    "section": "Clean names",
    "text": "Clean names\nOne column we’ll have to convert is our file_name column into a month/year column. In order to do that, we first have to create a value with all of the characters we want to remove.\n\nword_list &lt;- c(\"SB1R56 - |_1.pdf|_2.pdf\")\n\nNow that we’ve done that, let’s clean names and make our new column.\n\ndaily_indoor &lt;- indoor_temps_combined |&gt; \n  clean_names() |&gt; \n  mutate(\n    month_year = str_remove_all(file_name, word_list) \n  ) \n\ndaily_indoor",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#add-date-column",
    "href": "01-indoor-cleaning.html#add-date-column",
    "title": "Indoor logs cleaning",
    "section": "Add date column",
    "text": "Add date column\nWe’ll need to clean up the dates so they’re standardized and readable. We’ll combine our month/year column with the day column to put through the lubridate function and, while we’re at it, remove unnecessary columns.\n\ndaily_indoor_dated &lt;- daily_indoor |&gt; \n    # distinct() |&gt;\n    unite(\n    col = \"temp_date\", \n    day, month_year,\n    sep = \" \"\n    ) |&gt; \n  mutate(\n    date = dmy(temp_date)\n  ) |&gt; \n  select(!c(file_name, validation_link, temp_date)) |&gt; \n  mutate(unit_temp = temperature) |&gt; \n  select(!temperature) \n\n\ndaily_indoor_dated |&gt; glimpse()\n\nRows: 12,323\nColumns: 3\n$ unit      &lt;chr&gt; \"Allred\", \"Allred\", \"Allred\", \"Allred\", \"Allred\", \"Allred\", …\n$ date      &lt;date&gt; 2023-08-01, 2023-08-02, 2023-08-03, 2023-08-04, 2023-08-05,…\n$ unit_temp &lt;dbl&gt; 97.7, 99.7, 95.6, 97.6, 100.5, 97.1, 93.8, 92.3, 95.8, 93.0,…\n\n\n\nCheck dates\n\ndaily_indoor_dated",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#clean-some-unit-names",
    "href": "01-indoor-cleaning.html#clean-some-unit-names",
    "title": "Indoor logs cleaning",
    "section": "Clean some unit names",
    "text": "Clean some unit names\nIt looks like the Ramsey unit was recorded with slightly different names in each csv. Let’s combine both of those columns so all of the data is together and then remove any leftover NA’s.\nMcconnell 183\nO’Daniel 91\nWainwright 183\n\ndaily_indoor_named &lt;- daily_indoor_dated |&gt; \n  mutate(\n    unit = case_match(\n      unit,\n      \"Mcconnell\" ~ \"McConnell\",\n      \"O’Daniel\" ~ \"O'Daniel\",\n      .default = unit\n    )\n  )\n\nWe don’t have recordings for every unit. These are the units we do not have readings for: Baten, Bell, Bridgeport, Coleman, Cotulla, Diboll, Duncan, East Texas, Estes, Fort Stockton, Garza East, Glossbrenner, Halbert, Hamilton, Havins, Henley, Hodge, Hospital Galveston, Kegans, Kyle, LeBlanc, Lindsey, Marlin, Mechler, Moore, B., Ney, Pack, San Saba, Sayle, Scott, Skyview, Travis, Willacy.",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#export-the-data",
    "href": "01-indoor-cleaning.html#export-the-data",
    "title": "Indoor logs cleaning",
    "section": "Export the data",
    "text": "Export the data\nWe did it! Now let’s export our data and place it in our processed data folder.\n\ndaily_indoor_named |&gt; write_rds(\"data-processed/01-indoor-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html",
    "href": "01-activation-cleaning.html",
    "title": "Activations cleaning",
    "section": "",
    "text": "Things to check\n\n\n\nI need to check some things with Lauren:\n\nWhere is the original file from TDCJ with the activations data?\nConfirm what these dates are for.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#overview",
    "href": "01-activation-cleaning.html#overview",
    "title": "Activations cleaning",
    "section": "Overview",
    "text": "Overview\nThese are the days that a unit was under ICS protocols like providing extra ice water and access to cooler areas during extreme heat events. This data was acquired by Lauren McGaughy of The Texas Newsroom through a public information request to the Texas Department of Criminal Justice.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#possible-update",
    "href": "01-activation-cleaning.html#possible-update",
    "title": "Activations cleaning",
    "section": "Possible update",
    "text": "Possible update\nWe don’t have a complete record of where the activations CSV comes from. Pretty sure this came from Lauren.\nIn our tracking Google sheet we do have the same data, except there are errors on the last line.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#set-up",
    "href": "01-activation-cleaning.html#set-up",
    "title": "Activations cleaning",
    "section": "Set up",
    "text": "Set up\nYou know the drill!\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(data.table)",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#comparison-activation-log",
    "href": "01-activation-cleaning.html#comparison-activation-log",
    "title": "Activations cleaning",
    "section": "Comparison activation log",
    "text": "Comparison activation log\nWe’ll check about the original CSV with Lauren, but in the meantime we’ll compare that with the Activation records we also have in our tracking sheet. This copy was downloaded from our MIG = TDCJ Basic Information on the “Activation” tab. The Google Sheet has errors on the last record for 9/5/2023, but that record is in full in our csv log.\n\n# download.file(\n#   \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRebFs9O2i0HoygXTtIvuzdVTmyrjX3MeUorU9d4fpYkGX7Bb026OradFQ1MMk2ltcGnyILih6ow4F4/pub?gid=1968960884&single=true&output=csv\",\n#   \"data-original/unit-activation-direct.csv\"\n#       )\n\nImport direct log\n\nraw_activation_direct &lt;- read_csv(\"data-original/unit-activation-direct.csv\") |&gt; \n  clean_names()\n\nRows: 102 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Initial Date of Extreme Temperature, ICS Implementation Date, Count...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nraw_activation_direct\n\n\n  \n\n\n\nThis original files lacks on activation that is in our already-downloaded unit-activation-log.csv. We’ll continue to use the -log version while we track down an original.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#import-activation-log",
    "href": "01-activation-cleaning.html#import-activation-log",
    "title": "Activations cleaning",
    "section": "Import activation log",
    "text": "Import activation log\nLet’s add our activation log file in. (This file was downloaded directly into the repo, but we lost track of the original file. It has an additional record or other copies don’t have.)\n\nraw_activation_log &lt;- read_csv(\"data-original/unit-activation-log.csv\", skip = 1) |&gt; \n  clean_names()\n\nNew names:\nRows: 102 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): Initial Date of Extreme Temperature, ICS Implementation Date, Count... dbl\n(2): Time...6, Time...8 lgl (2): ...9, ...10\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `Date` -&gt; `Date...5`\n• `Time` -&gt; `Time...6`\n• `Date` -&gt; `Date...7`\n• `Time` -&gt; `Time...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n\nraw_activation_log",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#clean-column-names",
    "href": "01-activation-cleaning.html#clean-column-names",
    "title": "Activations cleaning",
    "section": "Clean column names",
    "text": "Clean column names\nOur column names didn’t translate perfectly, so let’s work on cleaning them up and remove the last two unnecessary columns.\n\nclean_activation_log &lt;- raw_activation_log |&gt; \n  select(\n    initial_extreme_temp = initial_date_of_extreme_temperature,\n    initiation_date = ics_implementation_date,\n    county,\n    unit = unit_affected,\n    activation_date = date_5, \n    activation_time = time_6,\n    deactivation_date = date_7,\n    deactivation_time = time_8 \n  )",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#change-date-columns",
    "href": "01-activation-cleaning.html#change-date-columns",
    "title": "Activations cleaning",
    "section": "Change date columns",
    "text": "Change date columns\nWe need our dates to be date columns instead of character columns. Let’s adjust.\n\nactivation_log_dates &lt;- clean_activation_log |&gt; \n  mutate(\n    initial_extreme_temp = mdy(initial_extreme_temp),\n    initiation_date = mdy(initiation_date),\n    activation_date = mdy(activation_date),\n    deactivation_date = mdy(deactivation_date)\n  ) \n\nactivation_log_dates",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#document-active-days",
    "href": "01-activation-cleaning.html#document-active-days",
    "title": "Activations cleaning",
    "section": "Document active days",
    "text": "Document active days\nWe need to turn our initial_extreme_temp column, implementation_date column and our deactivation_date column into date ranges.\n\ndirty_actives &lt;- activation_log_dates |&gt; \n  group_by(unit) |&gt; \n  mutate(occurence = dense_rank(initial_extreme_temp)) |&gt; \n  mutate(start = as.Date(activation_date), end = as.Date(deactivation_date)) |&gt; \n  mutate(\n    activated = map2(start, end, ~seq(from = .x, to = .y, by = \"day\"))\n    ) |&gt; \n  unnest(activated) \n\ndirty_actives",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#clean-up-our-active-dates",
    "href": "01-activation-cleaning.html#clean-up-our-active-dates",
    "title": "Activations cleaning",
    "section": "Clean up our active dates",
    "text": "Clean up our active dates\nWe need to create a status column, so let’s remove everything other than unit and active dates.\n\nclean_actives &lt;- dirty_actives |&gt; \n  select(c(unit, activated)) |&gt; \n  mutate(protocol_active = T) |&gt; \n  mutate(date = activated) |&gt; \n  select(!activated)\n\nclean_actives |&gt; \n  filter(unit == \"Byrd\")",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#export",
    "href": "01-activation-cleaning.html#export",
    "title": "Activations cleaning",
    "section": "Export",
    "text": "Export\n\nclean_actives |&gt; write_rds(\"data-processed/01-activation-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html",
    "href": "01-outdoor-cleaning.html",
    "title": "Outdoor logs cleaning",
    "section": "",
    "text": "This notebook compiles data collected from one week of outdoor temperature logs collected by the Texas Department of Criminal Justice. The dates considered were July 24, 2023 through July 31, 2023. The logs are hand-written and data could not be accurately pulled from them using AI, so a group of Media Innovaiton Group fellows transcribed the week of logs for each prison we had records for, XXX in total. Those records were acquired through a public information request by Lauren McGaughey of the Texas Newsroom public radio collaborative.\nWe created a Google Spreadsheet file for each prison, recording the temperatures for each hour as best as we could decipher them. Here we download a tracking spreadsheet, which is then used to download all of the individual log sheets, combining them into a single file. When then prepare that file for analysis.",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#setup",
    "href": "01-outdoor-cleaning.html#setup",
    "title": "Outdoor logs cleaning",
    "section": "Setup",
    "text": "Setup",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#download-the-tracking-sheet",
    "href": "01-outdoor-cleaning.html#download-the-tracking-sheet",
    "title": "Outdoor logs cleaning",
    "section": "Download the tracking sheet",
    "text": "Download the tracking sheet\nThis sheet is in MIG Data &gt; Jail Logs &gt; Outdoor Logs Data. It is called Outdoor Logs Tracking Sheet.\n\nOptions are set to eval: false to avoid re-downloading. Set to true to update.\n\n\n\nExpand this to see code\n# download.file(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRQBIKas7s5Wwe4bd9tMlAheCsjqeFU2JMZfk6h8Hc_QH6Dx02SEZmP6ieny13hCyZQosmsRirDEu9P/pub?output=csv\", \"data-original/outdoor-jail-logs-urls.csv\")\n\n\nRead in the tracking sheet url\n\n\nExpand this to see code\nlog_urls &lt;- read_csv(\"data-original/outdoor-jail-logs-urls.csv\") |&gt; clean_names()\n\nlog_urls |&gt; glimpse()\n\n\nRows: 101\nColumns: 7\n$ unit_name &lt;chr&gt; \"Bell\", \"Byrd\", \"Diboll\", \"Duncan\", \"Ellis\", \"Estelle\", \"Fer…\n$ unit_code &lt;chr&gt; \"CV\", \"DU\", \"DO\", \"N6\", \"E1\", \"E2\", \"FE\", \"GG\", \"GR\", \"NF\", …\n$ region    &lt;chr&gt; \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", …\n$ slug      &lt;chr&gt; NA, \"r1-byrd\", NA, \"r1-duncan\", \"r1-ellis\", \"r1-estelle\", \"r…\n$ url       &lt;chr&gt; NA, \"https://docs.google.com/spreadsheets/d/e/2PACX-1vROqTyA…\n$ notes_gid &lt;dbl&gt; NA, 1133970569, NA, 535687266, 1770803795, 1551327169, 96070…\n$ notes     &lt;chr&gt; NA, NA, \"closely linked to Duncan, pre-release\", NA, NA, NA,…\n\n\nIn some cases we had the urls formatted incorrectly. Result should be 0 rows.\n\n\nExpand this to see code\nlog_urls |&gt; \n  filter(str_detect(url, \"pubhtml\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#download-all-the-files",
    "href": "01-outdoor-cleaning.html#download-all-the-files",
    "title": "Outdoor logs cleaning",
    "section": "Download all the files",
    "text": "Download all the files\nThis code below was courtesy of ChatGPT, based on the following prompt: “I’m using R. I have a data frame that has a list of urls and slugs, which are short file names. Please write me a loop or other code that will download all the files at those urls, with their names as the slug. These are all csv files.” There were some edits.\n\nOptions are set to eval: false to avoid re-downloading. Set to true to update.\n\n\n\nExpand this to see code\n# set our urls to df\ndf &lt;- log_urls\n\n# Download loop\nfor (i in seq_len(nrow(df))) {\n  url &lt;- df$url[i]\n  slug &lt;- df$slug[i]\n  file_name &lt;- paste0(\"data-original/2023_logs/\", slug, \".csv\")\n  \n  if (is.na(url)) {\n    # message(paste(\"No URL for slug:\", slug))\n    next\n  } else {\n    tryCatch({\n      download.file(url, destfile = file_name, mode = \"wb\")\n    }, error = function(e) {\n      message(paste(\"Failed to download:\", url, \"-\", e$message, \"\\n\"))\n    })\n  }\n}\n\n\n\nMake a files list\nMake a list of files:\n\n\nExpand this to see code\nlogs_list &lt;- list.files(\n  \"data-original/2023_logs\",\n  pattern = \".csv\",\n  full.names = TRUE\n  )\n\nlogs_list\n\n\n [1] \"data-original/2023_logs/r1-byrd.csv\"              \n [2] \"data-original/2023_logs/r1-duncan.csv\"            \n [3] \"data-original/2023_logs/r1-ellis.csv\"             \n [4] \"data-original/2023_logs/r1-estelle.csv\"           \n [5] \"data-original/2023_logs/r1-ferguson.csv\"          \n [6] \"data-original/2023_logs/r1-goodman.csv\"           \n [7] \"data-original/2023_logs/r1-goree.csv\"             \n [8] \"data-original/2023_logs/r1-holliday.csv\"          \n [9] \"data-original/2023_logs/r1-huntsville.csv\"        \n[10] \"data-original/2023_logs/r1-lewis.csv\"             \n[11] \"data-original/2023_logs/r1-polunsky.csv\"          \n[12] \"data-original/2023_logs/r1-wainwright.csv\"        \n[13] \"data-original/2023_logs/r1-wynne.csv\"             \n[14] \"data-original/2023_logs/r2-beto.csv\"              \n[15] \"data-original/2023_logs/r2-boyd.csv\"              \n[16] \"data-original/2023_logs/r2-choice-moore.csv\"      \n[17] \"data-original/2023_logs/r2-cole.csv\"              \n[18] \"data-original/2023_logs/r2-hutchins.csv\"          \n[19] \"data-original/2023_logs/r2-johnston.csv\"          \n[20] \"data-original/2023_logs/r2-michael.csv\"           \n[21] \"data-original/2023_logs/r2-powledge.csv\"          \n[22] \"data-original/2023_logs/r2-skyview.csv\"           \n[23] \"data-original/2023_logs/r2-telford.csv\"           \n[24] \"data-original/2023_logs/r3-clemens.csv\"           \n[25] \"data-original/2023_logs/r3-gist.csv\"              \n[26] \"data-original/2023_logs/r3-henley.csv\"            \n[27] \"data-original/2023_logs/r3-hightower.csv\"         \n[28] \"data-original/2023_logs/r3-hospital-galveston.csv\"\n[29] \"data-original/2023_logs/r3-jester-3.csv\"          \n[30] \"data-original/2023_logs/r3-kegans.csv\"            \n[31] \"data-original/2023_logs/r3-leblanc.csv\"           \n[32] \"data-original/2023_logs/r3-lychner.csv\"           \n[33] \"data-original/2023_logs/r3-memorial.csv\"          \n[34] \"data-original/2023_logs/r3-plane.csv\"             \n[35] \"data-original/2023_logs/r3-ramsey.csv\"            \n[36] \"data-original/2023_logs/r3-scott.csv\"             \n[37] \"data-original/2023_logs/r3-stiles.csv\"            \n[38] \"data-original/2023_logs/r3-stringfellow.csv\"      \n[39] \"data-original/2023_logs/r3-terrell.csv\"           \n[40] \"data-original/2023_logs/r3-vance.csv\"             \n[41] \"data-original/2023_logs/r3-young.csv\"             \n[42] \"data-original/2023_logs/r4-briscoe.csv\"           \n[43] \"data-original/2023_logs/r4-connally.csv\"          \n[44] \"data-original/2023_logs/r4-cotulla.csv\"           \n[45] \"data-original/2023_logs/r4-dominguez.csv\"         \n[46] \"data-original/2023_logs/r4-fort-stockton.csv\"     \n[47] \"data-original/2023_logs/r4-garza-west.csv\"        \n[48] \"data-original/2023_logs/r4-glossbrenner.csv\"      \n[49] \"data-original/2023_logs/r4-lopez.csv\"             \n[50] \"data-original/2023_logs/r4-lynaugh.csv\"           \n[51] \"data-original/2023_logs/r4-mcconnell.csv\"         \n[52] \"data-original/2023_logs/r4-sanchez.csv\"           \n[53] \"data-original/2023_logs/r4-stevenson.csv\"         \n[54] \"data-original/2023_logs/r4-torres.csv\"            \n[55] \"data-original/2023_logs/r5-allred.csv\"            \n[56] \"data-original/2023_logs/r5-dalhart.csv\"           \n[57] \"data-original/2023_logs/r5-daniel.csv\"            \n[58] \"data-original/2023_logs/r5-formby.csv\"            \n[59] \"data-original/2023_logs/r5-jordan.csv\"            \n[60] \"data-original/2023_logs/r5-mechler.csv\"           \n[61] \"data-original/2023_logs/r5-montford.csv\"          \n[62] \"data-original/2023_logs/r5-roach.csv\"             \n[63] \"data-original/2023_logs/r5-smith.csv\"             \n[64] \"data-original/2023_logs/r5-wallace.csv\"           \n[65] \"data-original/2023_logs/r5-wheeler.csv\"           \n[66] \"data-original/2023_logs/r6-crain.csv\"             \n[67] \"data-original/2023_logs/r6-halbert.csv\"           \n[68] \"data-original/2023_logs/r6-hamilton.csv\"          \n[69] \"data-original/2023_logs/r6-havins.csv\"            \n[70] \"data-original/2023_logs/r6-hilltop.csv\"           \n[71] \"data-original/2023_logs/r6-hobby.csv\"             \n[72] \"data-original/2023_logs/r6-hughes.csv\"            \n[73] \"data-original/2023_logs/r6-luther.csv\"            \n[74] \"data-original/2023_logs/r6-marlin.csv\"            \n[75] \"data-original/2023_logs/r6-middleton.csv\"         \n[76] \"data-original/2023_logs/r6-o-daniel.csv\"          \n[77] \"data-original/2023_logs/r6-pack.csv\"              \n[78] \"data-original/2023_logs/r6-robertson.csv\"         \n[79] \"data-original/2023_logs/r6-san-saba.csv\"          \n[80] \"data-original/2023_logs/r6-sayle.csv\"             \n[81] \"data-original/2023_logs/r6-travis-county.csv\"     \n[82] \"data-original/2023_logs/r6-woodman.csv\"           \n\n\n\n\nFile checks\nA little ChatGPT help here to find a way to count the number of lines in each file. There should be 193 in each one. In some cases our spreadsheets included blank rows, which we fixed. There is still a blank column, but we take care of that later.\nThis takes the list of files above and counts the lines. When then filter the list for any that aren’t 193 lines long. The result should be 0 rows.\n\n\nExpand this to see code\ncount_lines &lt;- function(file) {\n  length(readLines(file, warn = FALSE))\n}\n\nline_counts &lt;- sapply(logs_list, count_lines)\nline_counts_data &lt;- tibble(file = basename(names(line_counts)), lines = line_counts)\n\nline_counts_data |&gt; filter(lines != 193)\n\n\n\n  \n\n\n\nSometimes we needed to check a specific file being downloaded.\n\n\nExpand this to see code\n# download.file(\n#   \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRPdRSPyFwnYPgrLkJktmtg2hy4BmWl0eILzwO7KLalYxjPF2xYMZj4kSTuo3u7wwUckskou4-Dm4zF/pub?gid=0&single=true&output=csv\",\n#   \"data-original/2023_logs/r1-estelle.csv\"\n# )",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#combine-the-files",
    "href": "01-outdoor-cleaning.html#combine-the-files",
    "title": "Outdoor logs cleaning",
    "section": "Combine the files",
    "text": "Combine the files\nAgain uses the logs_list from above.\n\n\nExpand this to see code\nlogs_raw &lt;- logs_list |&gt;  #set_names(basename) |&gt;\n  map(\n  read_csv,\n  col_types = cols(.default = col_character())\n) |&gt; list_rbind() |&gt;\n  clean_names()\n\n\nNew names:\nNew names:\nNew names:\n• `` -&gt; `...12`\n\n\nExpand this to see code\nlogs_raw |&gt; glimpse()\n\n\nRows: 15,744\nColumns: 12\n$ unit    &lt;chr&gt; \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\"…\n$ date    &lt;chr&gt; \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023…\n$ rec     &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",…\n$ time    &lt;chr&gt; \"12:30 a.m.\", \"1:30 a.m.\", \"2:30 a.m.\", \"3:30 a.m.\", \"4:30 a.m…\n$ temp    &lt;chr&gt; \"83\", \"82\", \"81\", \"81\", \"80\", \"79\", \"79\", \"80\", \"83\", \"86\", \"9…\n$ humid   &lt;chr&gt; \"54\", \"62\", \"67\", \"71\", \"74\", \"76\", \"N/A\", \"79\", \"72\", \"69\", \"…\n$ wind    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hi_wc   &lt;chr&gt; \"65\", \"68\", \"69\", \"71\", \"71\", \"71\", \"N/A\", \"84\", \"86\", \"95\", \"…\n$ hi_wc_n &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ person  &lt;chr&gt; \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Jo…\n$ notes   &lt;chr&gt; NA, NA, NA, NA, \"hi_wc corrected\", \"humid corrected\", NA, NA, …\n$ x12     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\nCheck and remove extra column\n\n\nExpand this to see code\nlogs_raw |&gt; filter(!is.na(x12))\n\n\n\n  \n\n\n\n\n\nExpand this to see code\nlogs_tight &lt;- logs_raw |&gt; select(-x12)\n\nlogs_tight |&gt; glimpse()\n\n\nRows: 15,744\nColumns: 11\n$ unit    &lt;chr&gt; \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\"…\n$ date    &lt;chr&gt; \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023…\n$ rec     &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",…\n$ time    &lt;chr&gt; \"12:30 a.m.\", \"1:30 a.m.\", \"2:30 a.m.\", \"3:30 a.m.\", \"4:30 a.m…\n$ temp    &lt;chr&gt; \"83\", \"82\", \"81\", \"81\", \"80\", \"79\", \"79\", \"80\", \"83\", \"86\", \"9…\n$ humid   &lt;chr&gt; \"54\", \"62\", \"67\", \"71\", \"74\", \"76\", \"N/A\", \"79\", \"72\", \"69\", \"…\n$ wind    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hi_wc   &lt;chr&gt; \"65\", \"68\", \"69\", \"71\", \"71\", \"71\", \"N/A\", \"84\", \"86\", \"95\", \"…\n$ hi_wc_n &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ person  &lt;chr&gt; \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Jo…\n$ notes   &lt;chr&gt; NA, NA, NA, NA, \"hi_wc corrected\", \"humid corrected\", NA, NA, …",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#fix-date-time-rec",
    "href": "01-outdoor-cleaning.html#fix-date-time-rec",
    "title": "Outdoor logs cleaning",
    "section": "Fix date, time, rec",
    "text": "Fix date, time, rec\nTwo decisions in the formatting of our log necessitate this:\n\nWe configured our date and time columns like the form itself instead of datetime formats that easily import.\nWe originally numbered the rec column from 1 to 24 thinking we needed to label each reading, but the weather station data will be labeled 0 to 23 because it is the hour of time.\n\nHere we create a real datetime, date and proper hour.\nAlso, we remove artifact columns and any blank columns by specifying the columns we do need.\n\n\nExpand this to see code\nlogs_dates &lt;- logs_tight |&gt; \n  mutate(\n    new_time = case_when(\n      time == \"12:30 a.m.\" ~ \"00:30:00\",\n      time == \"12:30 p.m.\" ~ \"12:30:00\",\n      str_sub(time, -4, -1) == \"a.m.\" ~ paste0(str_extract(time, \"^(\\\\d+)\"),\":30:00\"),\n      str_sub(time, -4, -1) == \"p.m.\" ~ paste0(str_extract(time, \"^(\\\\d+)\") |&gt; as.numeric() + 12, \":30:00\"),\n      .default = NA\n    )\n  ) |&gt; \n  mutate(\n    datetime = mdy_hms(paste(date, new_time)),\n    date = mdy(date),\n    hour = hour(datetime),\n    .after = time\n  ) |&gt; \n  select(\n    unit,\n    date,\n    rec,\n    datetime,\n    hour,\n    temp,\n    humid,\n    wind,\n    hi_wc,\n    hi_wc_n,\n    person,\n    notes\n  )\n\nlogs_dates |&gt; slice_sample(n = 10)",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#convert-the-numbers",
    "href": "01-outdoor-cleaning.html#convert-the-numbers",
    "title": "Outdoor logs cleaning",
    "section": "Convert the numbers",
    "text": "Convert the numbers\nOur various readings come in as text. We convert them to numbers.\n\n\nExpand this to see code\nlogs_numbs &lt;- logs_dates |&gt; \n  mutate(\n    across(c(temp, humid, wind, hi_wc, hi_wc_n), parse_number)\n  )\n\n\nWarning: There were 5 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(c(temp, humid, wind, hi_wc, hi_wc_n), parse_number)`.\nCaused by warning:\n! 3 parsing failures.\n row col expected actual\n2451  -- a number    N/A\n2476  -- a number    N/A\n2478  -- a number    N/A\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\n\nExpand this to see code\nlogs_numbs |&gt; slice_sample(n = 10)\n\n\n\n  \n\n\n\n\n\nExpand this to see code\nlogs_numbs |&gt; filter(str_detect(unit, \"Moore\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#fix-names",
    "href": "01-outdoor-cleaning.html#fix-names",
    "title": "Outdoor logs cleaning",
    "section": "Fix names",
    "text": "Fix names\nIn some cases the name on the outdoor log sheet did not match our tracking sheet, which means we have trouble later when we need to match that information.\n\n\nExpand this to see code\nlogs_fixes &lt;- logs_numbs |&gt; \n  mutate(\n    unit = case_match(\n      unit,\n      \"Travis State Jail\" ~ \"Travis County\",\n      \"Choice Moore\" ~ \"Moore, C.\",\n      \"Jester 3\" ~ \"Jester III\",\n      \"Lane Murray\" ~ \"Murray\",\n      \"Wallace Pack\" ~ \"Pack\",\n      \"Mt. View\" ~ \"Woodman\",\n      .default = unit)\n  )\n\nlogs_fixes |&gt; filter(str_detect(unit, \"Jordan\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#add-the-region",
    "href": "01-outdoor-cleaning.html#add-the-region",
    "title": "Outdoor logs cleaning",
    "section": "Add the region",
    "text": "Add the region\nIt’s helpful later to have the region of the prison as part of this dataset, so I’m going to add it here from the unit info.\n\n\nExpand this to see code\nregions &lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\") |&gt; \n  select(unit_name, region)\n\nlogs_regions &lt;- logs_fixes |&gt; \n  left_join(regions, by = join_by(unit == unit_name)) |&gt; \n  relocate(region, .after = unit)\n\nlogs_regions |&gt; filter(str_detect(unit, \"Jordan\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#export-the-logs",
    "href": "01-outdoor-cleaning.html#export-the-logs",
    "title": "Outdoor logs cleaning",
    "section": "Export the logs",
    "text": "Export the logs\n\n\nExpand this to see code\nlogs_regions |&gt; write_rds(\"data-processed/01-outdoor-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#notes-pages",
    "href": "01-outdoor-cleaning.html#notes-pages",
    "title": "Outdoor logs cleaning",
    "section": "Notes pages",
    "text": "Notes pages\nWhen we transcribed the logs, we also included a notes sheet where we added any overall relavant or interesting information.\nWe will be following the same steps as above, except that we have to piece together the download url from our tracking sheet.\n\n\nExpand this to see code\nnotes_urls &lt;- log_urls |&gt; \n  # filter(region == \"II\") |&gt; # filters to region for testing\n  select(slug, url, notes_gid) |&gt; \n  mutate(\n    pub_url = str_extract(url, \"(.*pub\\\\?)\", group = 1),\n    notes_url = paste0(pub_url, \"gid=\", notes_gid, \"&output=csv\")\n  ) |&gt; \n  filter(!is.na(url)) |&gt; \n  select(slug, notes_url) \n\nnotes_urls\n\n\n\n  \n\n\n\nThen we download all the files. eval: false is set here so files won’t be re-downloaded.\n\n\nExpand this to see code\n# set our urls to df\nnotes_df &lt;- notes_urls\n\n\n# Download loop\nfor (i in seq_len(nrow(notes_df))) {\n  url &lt;- df$notes_url[i]\n  slug &lt;- df$slug[i]\n  file_name &lt;- paste0(\"data-original/2023_notes/\", slug, \".csv\")\n  \n  if (is.na(url)) {\n    # message(paste(\"No URL for slug:\", slug))\n    next\n  } else {\n    tryCatch({\n      download.file(url, destfile = file_name, mode = \"wb\")\n    }, error = function(e) {\n      message(paste(\"Failed to download:\", url, \"-\", e$message, \"\\n\"))\n    })\n  }\n}\n\n\nMake a list of file paths to combine.\n\n\nExpand this to see code\nnotes_list &lt;- list.files(\n  \"data-original/2023_notes\",\n  pattern = \".csv\",\n  full.names = TRUE\n  )\n\nnotes_list\n\n\n [1] \"data-original/2023_notes/r1-byrd.csv\"              \n [2] \"data-original/2023_notes/r1-duncan.csv\"            \n [3] \"data-original/2023_notes/r1-ellis.csv\"             \n [4] \"data-original/2023_notes/r1-estelle.csv\"           \n [5] \"data-original/2023_notes/r1-ferguson.csv\"          \n [6] \"data-original/2023_notes/r1-goodman.csv\"           \n [7] \"data-original/2023_notes/r1-goree.csv\"             \n [8] \"data-original/2023_notes/r1-holliday.csv\"          \n [9] \"data-original/2023_notes/r1-huntsville.csv\"        \n[10] \"data-original/2023_notes/r1-lewis.csv\"             \n[11] \"data-original/2023_notes/r1-polunsky.csv\"          \n[12] \"data-original/2023_notes/r1-wainwright.csv\"        \n[13] \"data-original/2023_notes/r1-wynne.csv\"             \n[14] \"data-original/2023_notes/r2-beto.csv\"              \n[15] \"data-original/2023_notes/r2-boyd.csv\"              \n[16] \"data-original/2023_notes/r2-choice-moore.csv\"      \n[17] \"data-original/2023_notes/r2-cole.csv\"              \n[18] \"data-original/2023_notes/r2-hutchins.csv\"          \n[19] \"data-original/2023_notes/r2-johnston.csv\"          \n[20] \"data-original/2023_notes/r2-michael.csv\"           \n[21] \"data-original/2023_notes/r2-powledge.csv\"          \n[22] \"data-original/2023_notes/r2-telford.csv\"           \n[23] \"data-original/2023_notes/r3-clemens.csv\"           \n[24] \"data-original/2023_notes/r3-gist.csv\"              \n[25] \"data-original/2023_notes/r3-henley.csv\"            \n[26] \"data-original/2023_notes/r3-hightower.csv\"         \n[27] \"data-original/2023_notes/r3-hospital-galveston.csv\"\n[28] \"data-original/2023_notes/r3-jester-3.csv\"          \n[29] \"data-original/2023_notes/r3-kegans.csv\"            \n[30] \"data-original/2023_notes/r3-leblanc.csv\"           \n[31] \"data-original/2023_notes/r3-lychner.csv\"           \n[32] \"data-original/2023_notes/r3-memorial.csv\"          \n[33] \"data-original/2023_notes/r3-plane.csv\"             \n[34] \"data-original/2023_notes/r3-ramsey.csv\"            \n[35] \"data-original/2023_notes/r3-scott.csv\"             \n[36] \"data-original/2023_notes/r3-stiles.csv\"            \n[37] \"data-original/2023_notes/r3-stringfellow.csv\"      \n[38] \"data-original/2023_notes/r3-terrell.csv\"           \n[39] \"data-original/2023_notes/r3-vance.csv\"             \n[40] \"data-original/2023_notes/r3-young.csv\"             \n[41] \"data-original/2023_notes/r4-briscoe.csv\"           \n[42] \"data-original/2023_notes/r4-connally.csv\"          \n[43] \"data-original/2023_notes/r4-cotulla.csv\"           \n[44] \"data-original/2023_notes/r4-dominguez.csv\"         \n[45] \"data-original/2023_notes/r4-fort-stockton.csv\"     \n[46] \"data-original/2023_notes/r4-garza-west.csv\"        \n[47] \"data-original/2023_notes/r4-glossbrenner.csv\"      \n[48] \"data-original/2023_notes/r4-lopez.csv\"             \n[49] \"data-original/2023_notes/r4-lynaugh.csv\"           \n[50] \"data-original/2023_notes/r4-mcconnell.csv\"         \n[51] \"data-original/2023_notes/r4-sanchez.csv\"           \n[52] \"data-original/2023_notes/r4-stevenson.csv\"         \n[53] \"data-original/2023_notes/r4-torres.csv\"            \n[54] \"data-original/2023_notes/r5-dalhart.csv\"           \n[55] \"data-original/2023_notes/r5-daniel.csv\"            \n[56] \"data-original/2023_notes/r5-formby.csv\"            \n[57] \"data-original/2023_notes/r5-jordan.csv\"            \n[58] \"data-original/2023_notes/r5-mechler.csv\"           \n[59] \"data-original/2023_notes/r5-montford.csv\"          \n[60] \"data-original/2023_notes/r5-roach.csv\"             \n[61] \"data-original/2023_notes/r5-smith.csv\"             \n[62] \"data-original/2023_notes/r5-wallace.csv\"           \n[63] \"data-original/2023_notes/r6-crain.csv\"             \n[64] \"data-original/2023_notes/r6-halbert.csv\"           \n[65] \"data-original/2023_notes/r6-hamilton.csv\"          \n[66] \"data-original/2023_notes/r6-havins.csv\"            \n[67] \"data-original/2023_notes/r6-hilltop.csv\"           \n[68] \"data-original/2023_notes/r6-hobby.csv\"             \n[69] \"data-original/2023_notes/r6-hughes.csv\"            \n[70] \"data-original/2023_notes/r6-luther.csv\"            \n[71] \"data-original/2023_notes/r6-marlin.csv\"            \n[72] \"data-original/2023_notes/r6-middleton.csv\"         \n[73] \"data-original/2023_notes/r6-o-daniel.csv\"          \n[74] \"data-original/2023_notes/r6-pack.csv\"              \n[75] \"data-original/2023_notes/r6-robertson.csv\"         \n[76] \"data-original/2023_notes/r6-san-saba.csv\"          \n[77] \"data-original/2023_notes/r6-sayle.csv\"             \n[78] \"data-original/2023_notes/r6-travis-county.csv\"     \n[79] \"data-original/2023_notes/r6-woodman.csv\"           \n\n\nNow we put those files together into a single tibble.\n\n\nExpand this to see code\nnotes_raw &lt;- notes_list |&gt; \n  set_names(basename) |&gt;\n  map(\n  read_csv,\n  col_types = cols(.default = col_character()),\n  col_names = c(\"notes\")\n) |&gt; list_rbind(names_to = \"unit\") |&gt;\n  clean_names()\n\nnotes_raw |&gt; glimpse()\n\n\nRows: 268\nColumns: 4\n$ unit  &lt;chr&gt; \"r1-byrd.csv\", \"r1-byrd.csv\", \"r1-duncan.csv\", \"r1-duncan.csv\", …\n$ notes &lt;chr&gt; \"Name we code as Komomzy is a difficult signature to comprehend\"…\n$ x2    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ x3    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\nWe have a little cleanup here for the unit and region. We drop the x columns, which are empty.\n\n\nExpand this to see code\nnotes_cleaned &lt;- notes_raw |&gt; \n  filter(!is.na(notes)) |&gt; \n  mutate(\n    region = str_extract(unit, pattern = \"(.*)-\", group = 1),\n    unit = str_extract(unit, pattern = \".*-(.*).csv\", group = 1)\n  ) |&gt; \n  select(unit, region, notes)\n\nnotes_cleaned |&gt; head()\n\n\n\n  \n\n\n\nWe’ll export this as both a csv and an rds because we may just put these into a spreadsheet for review.\n\n\nExpand this to see code\nnotes_cleaned |&gt; write_rds(\"data-processed/01-outdoor-notes.rds\")\nnotes_cleaned |&gt; write_csv(\"data-processed/01-outdoor-notes.csv\")",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html",
    "href": "01-unit-info-cleaning.html",
    "title": "Unit info cleaning",
    "section": "",
    "text": "This data piece will help us join together our unit-focused data and our weather station-focused data. The data was compiled by Media Innovation Group data fellows. The information came from the Texas Department of Criminal Justice website, a TDCJ unit prototype list and the National Weather Service website.",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#goals-of-this-notebook",
    "href": "01-unit-info-cleaning.html#goals-of-this-notebook",
    "title": "Unit info cleaning",
    "section": "Goals of this notebook",
    "text": "Goals of this notebook\nClean up our column names so we can easily combine with our other data.",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#loading-libraries",
    "href": "01-unit-info-cleaning.html#loading-libraries",
    "title": "Unit info cleaning",
    "section": "Loading libraries",
    "text": "Loading libraries\n\nlibrary(tidyverse)\nlibrary(janitor)",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#download-unit-info",
    "href": "01-unit-info-cleaning.html#download-unit-info",
    "title": "Unit info cleaning",
    "section": "Download unit info",
    "text": "Download unit info\n\nOption eval: fasle set to avoid re-download. Change to true for updates.\n\n\n# download.file(\n#   \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRebFs9O2i0HoygXTtIvuzdVTmyrjX3MeUorU9d4fpYkGX7Bb026OradFQ1MMk2ltcGnyILih6ow4F4/pub?gid=1653039441&single=true&output=csv\",\n#   \"data-original/unit-nws-info.csv\")",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#importing-unit-info",
    "href": "01-unit-info-cleaning.html#importing-unit-info",
    "title": "Unit info cleaning",
    "section": "Importing unit info",
    "text": "Importing unit info\nLet’s bring in our unit info sheet. This was compiled by us to be used in this project.\n\nnws_unit_raw &lt;- read_csv(\"data-original/unit-nws-info.csv\") |&gt; clean_names()\n\nRows: 101 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): Unit Name, Region, Data Fellow, NWS, NWS ID, Unit Code, Type, Stre...\ndbl  (1): Driving miles\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnws_unit_raw |&gt; head()",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#select-columns",
    "href": "01-unit-info-cleaning.html#select-columns",
    "title": "Unit info cleaning",
    "section": "Select columns",
    "text": "Select columns\nI’m gonna change column names and only keep the columns we need.\n\nunit_info_clean &lt;- nws_unit_raw |&gt; \n  select(unit_name, region, unit_code, type, nws, nws_id, county)\n\nunit_info_clean",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#export-data",
    "href": "01-unit-info-cleaning.html#export-data",
    "title": "Unit info cleaning",
    "section": "Export data",
    "text": "Export data\nOur cleaning was pretty easy! Let’s export the data and put it into our processed folder now.\n\nunit_info_clean |&gt; write_rds(\"data-processed/01-unit-info-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html",
    "href": "02-outdoor-protocols.html",
    "title": "Precautionary measures",
    "section": "",
    "text": "We have eight days of hand-written weather logs from 82 prisons that we have translated into data. The date range was 2023-07-24 to 2023-07-31.\nIn this analysis our aim is to compare those local weather logs to TDCJ heat protocol implementations. The protocols are based on the EXCESSIVE AND EXTREME TEMPERATURE CONDITIONS IN THE TDCJ (AD-10.64 (rev. 10)).\n\n\nThese come from AD-10.64:\n\n“Excessive Heat” occurs from a combination of significantly higher than normal temperatures and high humidity.\n“Excessive Heat Warning” is issued by the National Weather Service within 12 hours of the onset of the following criteria: temperature of at least 105ºF for more than three hours per day for two consecutive days, or heat index of 113ºF or greater for any period of time.\n“Heat Wave” is a prolonged period (three or more days) of excessively hot and unusually humid weather that meets the following criteria: temperature of at least 105ºF or heat index of 113ºF.\n\n\n\n\nWhen the National Weather Service issues an excessive heat warning or notice of an impending heat wave, the TDCJ Office of Emergency Management shall send the applicable division directors an email notification. When excessive heat conditions last for more than three consecutive days, the division directors and warden(s) of units in the affected area(s) shall immediately implement additional precautionary measures, as outlined in Section IV.I of this directive.\nNOTE: The TDCJ protocol activation records we have relate to these “precautionary measures” mentioned here.\n\n\n\nThe definition of excessive heat above is a challenge as it is not clearly defined with data points to measure against, yet seems to be referred to in the III.A.2 section of the directive.\nExcessive heat warning is clearly defined, but the III.A.2 section does not use the term “warning”, just the ambiguous “excessive heat conditions” terminology.\nDoes this mean that an “excessive heat warning” must be in effect for more than 3 days before implementing “precautionary measure” protocols? Or that the conditions that contribute to that heat warning have to be in place?\n\n\n\nIn communications with the TDCJ Director of Communications, they stated that “ICS is initiated when temperatures are 105+ degrees or heat index is 113+ for three or more consecutive days.” This interpretation is less strict than the “excessive heat warning” definition in the AD-10.64 directive.\nOur understanding is the “ICS” (Incident Command System) is the precautionary measures outlined in III.A.2 of the directive.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#overview",
    "href": "02-outdoor-protocols.html#overview",
    "title": "Precautionary measures",
    "section": "",
    "text": "We have eight days of hand-written weather logs from 82 prisons that we have translated into data. The date range was 2023-07-24 to 2023-07-31.\nIn this analysis our aim is to compare those local weather logs to TDCJ heat protocol implementations. The protocols are based on the EXCESSIVE AND EXTREME TEMPERATURE CONDITIONS IN THE TDCJ (AD-10.64 (rev. 10)).\n\n\nThese come from AD-10.64:\n\n“Excessive Heat” occurs from a combination of significantly higher than normal temperatures and high humidity.\n“Excessive Heat Warning” is issued by the National Weather Service within 12 hours of the onset of the following criteria: temperature of at least 105ºF for more than three hours per day for two consecutive days, or heat index of 113ºF or greater for any period of time.\n“Heat Wave” is a prolonged period (three or more days) of excessively hot and unusually humid weather that meets the following criteria: temperature of at least 105ºF or heat index of 113ºF.\n\n\n\n\nWhen the National Weather Service issues an excessive heat warning or notice of an impending heat wave, the TDCJ Office of Emergency Management shall send the applicable division directors an email notification. When excessive heat conditions last for more than three consecutive days, the division directors and warden(s) of units in the affected area(s) shall immediately implement additional precautionary measures, as outlined in Section IV.I of this directive.\nNOTE: The TDCJ protocol activation records we have relate to these “precautionary measures” mentioned here.\n\n\n\nThe definition of excessive heat above is a challenge as it is not clearly defined with data points to measure against, yet seems to be referred to in the III.A.2 section of the directive.\nExcessive heat warning is clearly defined, but the III.A.2 section does not use the term “warning”, just the ambiguous “excessive heat conditions” terminology.\nDoes this mean that an “excessive heat warning” must be in effect for more than 3 days before implementing “precautionary measure” protocols? Or that the conditions that contribute to that heat warning have to be in place?\n\n\n\nIn communications with the TDCJ Director of Communications, they stated that “ICS is initiated when temperatures are 105+ degrees or heat index is 113+ for three or more consecutive days.” This interpretation is less strict than the “excessive heat warning” definition in the AD-10.64 directive.\nOur understanding is the “ICS” (Incident Command System) is the precautionary measures outlined in III.A.2 of the directive.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#other-known-issues",
    "href": "02-outdoor-protocols.html#other-known-issues",
    "title": "Precautionary measures",
    "section": "Other known issues",
    "text": "Other known issues\nBecause of the nature of the original records, there are missing data that affect our analysis.\n\nThere are 8 records where we don’t have a temperature value. We dropped those records.\nThere are 3088 records (out of 15,744) where we don’t have a heat index value. That limits their value, but we still have temperatures, so we keep them.\n\nThere are 7 units that have one or more days where we can’t find a max heat index value. Typically because there are no heat index values recorded for a given date. 8 days for Dalhart, 7 for Formby, 3 for Wheeler. The others only have one day missing.\nIn the original written logs, some units (Dalhart, for example) used category designations instead of heat index values, which means they will not translate into numbers for data. Category 3 is the “Danger” area according to the NOAA’s National Weather Service Heat and Humidity Index included in the TDCJ directive, and that range includes heat indexes of 113 degrees and higher. Category 4 is “Extreme Danger”. This means in some cases we could have heat conditions based on heat index, but they are not included.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#what-well-find",
    "href": "02-outdoor-protocols.html#what-well-find",
    "title": "Precautionary measures",
    "section": "What we’ll find",
    "text": "What we’ll find\nBased on our communications with TDCJ, we’ll use the following definition as “meeting protocol” to go into precautionary measures: When temperatures are 105+ degrees or heat index is 113+ for three or more consecutive days.\nWe’ll find …\n\nWhich units met the protocol definition outlined above.\nWhich units had ICS “precautionary measures” implemented by TDCJ.\nSetting aside the consecutive days criteria, we’ll find any day it reached 105 or a heat index of 113.\nWe’ll find how many times units reached a heat index of 90 or greater, as the directive uses that measure for some heat mitigation measures, though TDCJ does not track their use.\nJust for comparison, we’ll calculate the more strict “excessive heat warning” criteria and see when units reached it.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#setup",
    "href": "02-outdoor-protocols.html#setup",
    "title": "Precautionary measures",
    "section": "Setup",
    "text": "Setup\nOur libraries.\n\n\nExpand this to see code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(rlang)\n\n\n\nStreaks function\nThis function creates a new column that counts how many consecutive days another column has been true, within each group (by unit). It’s used several times throughout the analysis, anytime we want to count a streak of a flag value. The code was written with help from ChatGPT. Full explanation in the resources folder.\n\n\nExpand this to see code\nadd_streaks &lt;- function(df, new_col, flag_col) {\n  # helps new col work if quoted\n  new_col &lt;- rlang::ensym(new_col)\n\n  df |&gt;\n  arrange(unit, date) |&gt; \n  group_by(unit) |&gt; \n  # Counts consecutive flags by unit\n  mutate(\n    !!new_col := {\n      count &lt;- 0L\n      map_int({{ flag_col }}, ~ {\n        if (.x) {\n          count &lt;&lt;- count + 1L\n        } else {\n          count &lt;&lt;- 0L\n        }\n        count\n      })\n    }\n  ) |&gt; \n  ungroup()\n}",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#import",
    "href": "02-outdoor-protocols.html#import",
    "title": "Precautionary measures",
    "section": "Import",
    "text": "Import\nWe need the logs and the activations.\n\n\nExpand this to see code\nlogs_all &lt;- read_rds(\"data-processed/01-outdoor-cleaned.rds\") \nactivations &lt;- read_rds(\"data-processed/01-activation-cleaned.rds\")",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#data-checks",
    "href": "02-outdoor-protocols.html#data-checks",
    "title": "Precautionary measures",
    "section": "Data checks",
    "text": "Data checks\nWe use temperature (tmp) and heat index (hi_wc) values as a basis of everything. Missing those values affect things down the road, so let’s catalog the problems and mitigate them.\n\nMissing both\nHere we find records that have neither value:\n\n\nExpand this to see code\nlogs_all |&gt; filter(is.na(temp), is.na(hi_wc))\n\n\n\n  \n\n\n\nWe’ll drop these records below because they can’t help us. I’ve checked and these are also the only records that are missing temp.\n\n\nExpand this to see code\nlogs_clipped &lt;- logs_all |&gt; \n  drop_na(temp) # drops 8 records that don't have a temperature recorded\n\n\n\n\nMissing heat index\nHowever, there are some records with temps that are missing heat index/wind chill hi_wc values.\n\n\nExpand this to see code\nlogs_missing_hi &lt;- logs_clipped |&gt; filter(is.na(hi_wc))\n\nlogs_missing_hi |&gt; nrow()\n\n\n[1] 3080\n\n\nMissing the heat index value is not necessarily a problem. Temperatures and humidity could be out of range for a valid calculation.\nWe keep these because we can still use the temp values.\n\n\nRisk category\nThat said, of the records without heat index values, there are some that do have the hi_wc_n value, which is the “Risk Category” in the NOAA Heat and Humidity Index.\n\n\n\nCategory\nRating\n\n\n\n\n1\nCaution\n\n\n2\nExtreme Caution\n\n\n3\nDanger\n\n\n4\nExtreme Danger\n\n\n\nHere I filter to find the units that have the category listed. Since these are hourly readings, I use distinct to find the dates when a unit recorded a specific category. We then filter those units that recorded cat 3 or higher.\n\n\nExpand this to see code\nlogs_missing_hi |&gt;\n  filter(!is.na(hi_wc_n)) |&gt; \n  distinct(unit, date, hi_wc_n) |&gt;\n  filter(hi_wc_n &gt;= 3)\n\n\n\n  \n\n\n\nThis means there are 15 “unit days” where it is possible the heat index has reached 113, but we don’t know for sure. Here we show which units have a missing heat index and category 3 risk category.\nIn the end, we are keeping records with no heat index value because they still have temperature, and we are ignoring the risk category because we don’t know the exact heat index value.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#preparing-the-protocol-flags",
    "href": "02-outdoor-protocols.html#preparing-the-protocol-flags",
    "title": "Precautionary measures",
    "section": "Preparing the protocol flags",
    "text": "Preparing the protocol flags\nHere we summarize the hourly logs to see if certain conditions exist in a given day.\n\nWe find the maximum heat index value.\nWe find the maximum temperatures for a day.\nWe find how many hours within a day the temperature was above 105. (This is for strict Excessive Heat Warning calculations.)\n\nWe peek at a sample to check results.\n\n\nExpand this to see code\nlogs_values &lt;- logs_clipped |&gt; \n  group_by(unit, date) |&gt; \n  # summarize measurements\n  summarise(\n    u_hi_max = max(hi_wc, na.rm = T),\n    u_tmp_max = max(temp, na.rm = T),\n    u_hrs_105 = sum(temp &gt;= 105),\n    .groups = \"drop\"\n  )\n\n\nWarning: There were 22 warnings in `summarise()`.\nThe first warning was:\nℹ In argument: `u_hi_max = max(hi_wc, na.rm = T)`.\nℹ In group 81: `unit = \"Dalhart\"` `date = 2023-07-24`.\nCaused by warning in `max()`:\n! no non-missing arguments to max; returning -Inf\nℹ Run `dplyr::last_dplyr_warnings()` to see the 21 remaining warnings.\n\n\nExpand this to see code\nlogs_values |&gt; slice_sample(n = 20)\n\n\n\n  \n\n\n\n\nWarnings and -Inf values\nWe do get some warnings here “no non-missing arguments to max; returning -Inf” when we calculate maximum heat index values. These are cases where the hi_wc was always NA within a unit day, so there was no max value to find. In 22 cases it gives us an -Inf which gets treated as NA going forward and we can’t consider heat index values for those unit days.\n\n\nAdd hi, tmp flags\nHere we add some flags based on the excessive heat warning definitions:\n\nIf heat index reached 113\nIf the temperature reached 105\n\nWe peek at a sample to check the results.\n\n\nExpand this to see code\nlogs_flags &lt;- logs_values |&gt; \n  mutate(\n    u_hi_flag = if_else(u_hi_max &gt;= 113, T, F),\n    u_tmp_flag = if_else(u_tmp_max &gt;= 105, T, F),\n    )\n\n# view a sample\nlogs_flags |&gt; slice_sample(n = 20)\n\n\n\n  \n\n\n\n\n\nSet protocol flags\nHere we set more flags when certain definitions have been met.\n\nFlag TRUE if either the temperature reached 105 or the heat index reached 113: u_hi_tmp_proto.\nCreate a consecutive day count for when u_hi_tmp_proto is true: u_hi_tmp_proto_cnt.\nFlag TRUE if u_hi_tmp_proto_cnt is more than three days: u_proto_flag.\n\nWe peek at a sample of rows to check logic.\n\n\nExpand this to see code\nlogs_protos &lt;- logs_flags |&gt; \n  mutate(\n    u_hi_tmp_proto = if_else(u_tmp_flag == T | u_hi_flag == T, T, F),\n  ) |&gt; \n  add_streaks(\"u_hi_tmp_proto_cnt\", u_hi_tmp_proto) |&gt; \n  mutate(\n    u_proto_flag = if_else(u_hi_tmp_proto_cnt &gt; 3, T, F)\n  )\n\nlogs_protos |&gt; slice_sample(n = 20)\n\n\n\n  \n\n\n\nAt this point we have all the conditions counted and flags created.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#join-with-activations",
    "href": "02-outdoor-protocols.html#join-with-activations",
    "title": "Precautionary measures",
    "section": "Join with activations",
    "text": "Join with activations\nHere we take our log files and join them with the activation dates so we can see where/when they match, if at all.\nWe glimpse the new table to review the column names.\n\n\nExpand this to see code\nlogs_activations &lt;- logs_protos |&gt;\n  left_join(activations, join_by(unit, date)) |&gt; \n  mutate(protocol_active = if_else(is.na(protocol_active), F, protocol_active))\n\nlogs_activations |&gt; glimpse()\n\n\nRows: 656\nColumns: 11\n$ unit               &lt;chr&gt; \"Allred\", \"Allred\", \"Allred\", \"Allred\", \"Allred\", \"…\n$ date               &lt;date&gt; 2023-07-24, 2023-07-25, 2023-07-26, 2023-07-27, 20…\n$ u_hi_max           &lt;dbl&gt; 105, 107, 105, 105, 110, 105, 104, 108, 109, 107, 1…\n$ u_tmp_max          &lt;dbl&gt; 106, 108, 103, 105, 104, 105, 105, 107, 103, 100, 9…\n$ u_hrs_105          &lt;int&gt; 3, 4, 0, 1, 0, 1, 2, 5, 0, 0, 0, 0, 0, 0, 1, 2, 0, …\n$ u_hi_flag          &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ u_tmp_flag         &lt;lgl&gt; TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, F…\n$ u_hi_tmp_proto     &lt;lgl&gt; TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE, F…\n$ u_hi_tmp_proto_cnt &lt;int&gt; 1, 2, 0, 1, 0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 1, 2, 0, …\n$ u_proto_flag       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ protocol_active    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n\n\n\nChecks on activations\nWe have a multi-step bit here to make sure that all the activations have been added to our logs data.\nHere we look at the original data to see which rows match our log time frame.\n\n\nExpand this to see code\nactivations_active &lt;- activations |&gt;\n  filter(date &gt;= \"2023-07-24\" & date &lt;= \"2023-07-31\") |&gt; \n  arrange(date, unit)\n\nactivations_active\n\n\n\n  \n\n\n\nThere are 24 activations in our time period. Now let’s check our joined data …\n\n\nExpand this to see code\nlogs_activations_active &lt;- logs_activations |&gt; \n  filter(!is.na(protocol_active)) |&gt; \n  arrange(date, unit)\n\nlogs_activations_active\n\n\n\n  \n\n\n\nThere are also 24 days here. Let’s anti-join them to see if there are any non-matches.\nThis should result in 0 rows if we’ve done things right.\n\n\nExpand this to see code\nactivations_active |&gt; \n  anti_join(logs_activations_active, join_by(unit, date))\n\n\n\n  \n\n\n\nThis has been overkill, but I’m certain that we have captured all of the activations in our time period and those have been properly added to our logs.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-protocols.html#answer-the-questions",
    "href": "02-outdoor-protocols.html#answer-the-questions",
    "title": "Precautionary measures",
    "section": "Answer the questions",
    "text": "Answer the questions\n\n1. Units meeting criteria\n\nWhich units met the protocol definition outlined above.\n\n\n\nExpand this to see code\ncriteria_met &lt;- logs_activations |&gt; \n  filter(u_proto_flag == T) |&gt; \n  select(unit, date, u_hi_max, u_tmp_max, protocol_active) |&gt; \n  arrange(unit, date)\n\ncriteria_met\n\n\n\n  \n\n\n\nWhich units were involved?\n\n\nExpand this to see code\ncriteria_met |&gt; count(unit)\n\n\n\n  \n\n\n\nWere ICS protocols activated?\n\n\nExpand this to see code\ncriteria_met |&gt; filter(protocol_active == T)\n\n\n\n  \n\n\n\n\n\nTakeaway 1\nSeven units met the ICS criteria for a total of 17 “unit days” based on the definition given to us by the TDCJ communications department. None of those had active ICS protocols in place.\nNote that this count could be low, as it’s possible units met protocol before the first day of our sample.\n\n\n2. Days in ICS\nHere we look at the days that units within our time frame where TDCJ activated its Incident Command System, i.e., precautionary measures based on heat.\nWe include our u_proto_flag created based on the handwritten weather logs.\n\n\nExpand this to see code\nlogs_ics &lt;- logs_activations |&gt; \n  filter(protocol_active == T) |&gt; \n  select(unit, date, u_hi_max, u_tmp_max, u_proto_flag)\n\nlogs_ics\n\n\n\n  \n\n\n\nHowever, 12 of these activation days are within the first two days of our period, meaning they could’ve met the criteria based on data not in our review.\n\n\nExpand this to see code\nlogs_ics |&gt; filter(date &lt; \"2023-07-26\")\n\n\n\n  \n\n\n\nOne more look … Which of these days did not meet the criteria for a the given day.\n\n\nExpand this to see code\nlogs_ics |&gt; \n  filter(u_hi_max &lt; 113 & u_tmp_max &lt; 105)\n\n\n\n  \n\n\n\nHalf of these units did not reach a heat index of 113 or temperature of 105 on the day of the active protocol. Being under ICS is likely good for inmates, but it perhaps shows inconsistency in implementation.\n\n\nTakeaway 2\nAccording to the activation data, there were 24 days in our time period where ICS heat mitigation protocols were put in place. On those activation days, none of these units met the criteria we’ve been given by TDCJ communications based on the weather logs kept by units. That said, half of the activations are within the first two days of our study, meaning they could have met the criteria in the days before our time frame.\n\n\n3. Any day meeting condition\nThese precautionary protocols are mandated only after heat conditions exists for three consecutive days. But any day these conditions exist can be dangerous. Let’s see how many times this condition was true for these units in our time period.\n\n\nExpand this to see code\nlogs_activations |&gt; \n  filter(u_hi_tmp_proto == T) |&gt; \n  count(unit) |&gt; \n  adorn_totals(\"row\") |&gt; \n  tibble()\n\n\n\n  \n\n\n\nHow many of these were in active ICS protocol?\n\n\nExpand this to see code\nlogs_activations |&gt; \n  filter(u_hi_tmp_proto == T) |&gt; \n  count(protocol_active)\n\n\n\n  \n\n\n\n\n\nTakeaway 3\nOut of 656 “unit days” possible (82 units, 8 days), excessive heat conditions were reached 127 times. Half of the units reached the criteria at least once. In only 12 of those instances was a unit in active precautionary measures.\n\n\n4. Days of 90 heat index\nThere is a section in the directive (III.A.5) where heat precautions must be taken when in “apparent air temperatures above 90 degrees”, like frequent water breaks. TDCJ does not record when such conditions exist, but we wanted to get an idea.\n\n\nExpand this to see code\nlogs_activations |&gt; \n  filter(u_hi_max &gt;= 90) |&gt; \n  count(unit)\n\n\n\n  \n\n\n\n\n\nTakeaway 4\nThese 90 degree heat index conditions existed nearly every day for all 82 units within our time period. Only five units didn’t meet that criteria every day, and for four of those days it didn’t meet for only a single day.\n\n\n5. Excessive Heat Warnings\nWithin the directive, TDCJ outlines a definition of “Excessive Heat Warning” as “temperature of at least 105ºF for more than three hours per day for two consecutive days, or heat index of 113ºF or greater for any period of time.” This is more strict than the definition we use above. Let’s just see how many of our units would meet this criteria based on the handwritten weather logs data.\n\n\nPrep the data\nWe have to calculate quite a few things to make this definition. Comments in code.\n\n\nExpand this to see code\nlogs_ehw_protos &lt;- logs_values |&gt; \n  # 105ºF for more than three hours per day\n  mutate(\n    u_tmp_3hrs = if_else(u_hrs_105 &gt;= 3, T, F)\n  ) |&gt; \n  # add count to find consecutive days 105 met\n  add_streaks(\"u_tmp_flag_cnt\", u_tmp_3hrs) |&gt; \n  mutate(\n    # flag if 105 was 2 or more days\n    u_tmp_2days = if_else(u_tmp_flag_cnt &gt;= 2, T, F),\n    # flag if either tmp conditions or heat index conditions are met\n    u_ehw_flag = if_else(u_hi_max &gt;= 113 | u_tmp_2days == T, T, F)\n  ) |&gt; \n  # Add count to find consecutive days either condition met\n  add_streaks(\"u_ehw_flag_cnt\", u_ehw_flag) |&gt; \n  # add activations\n  left_join(activations, join_by(unit, date)) |&gt; \n  mutate(protocol_active = if_else(is.na(protocol_active), F, protocol_active))\n\nlogs_ehw_protos\n\n\n\n  \n\n\n\n\n\nEHW streaks\nIn III.A.2 the directive says “When excessive heat conditions last for more than three consecutive days, the division directors and warden(s) of units in the affected area(s) shall immediately implement additional precautionary measures …”\nEmphasis is mine. That portion could be read as four or more days, which is different than the “three consecutive days” value we’ve been working with in our basic definition above.\nHere we show which units met the EHW criteria for three or more days.\nOF NOTE: Since we only have eight days of readings, we potentially miss streaks early in our time period. Conditions could have been met on July 22-23, but we don’t know that.\n\n\nExpand this to see code\nlogs_ehw_protos |&gt; \n  filter(u_ehw_flag_cnt &gt;= 3) |&gt; \n  select(unit, date, u_hi_max, u_tmp_max, u_ehw_flag_cnt, protocol_active)\n\n\n\n  \n\n\n\nThe emphasis is mine. This could be read as these conditions being in place for more than three days, so four or greater. So here check that.\n\n\nExpand this to see code\nlogs_ehw_protos |&gt; \n  filter(u_ehw_flag_cnt &gt;= 4) |&gt; \n  select(unit, date, u_hi_max, u_tmp_max, u_ehw_flag_cnt, protocol_active)\n\n\n\n  \n\n\n\n\n\nTakeaway 5\nThere were 12 “unit days” where strict Excessive Heat Warning conditions were in effect for three or more days during our eight-day period. Official “Incident Command System” protocols were not in active on any of those days.",
    "crumbs": [
      "Analysis",
      "Precautionary measures"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html",
    "href": "02-outdoor-reliability.html",
    "title": "Outdoor log reliability",
    "section": "",
    "text": "Can we find a NWS for Jordan that is close but has readings? Are there other cases like this were we don’t have readings, but can find an alternative station? Let’s try using virtualcrossing.\n\nQuestions for LM:\n\nWe filter to 1 week in July, but have 8 days. Should we include the 8th day?",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#reliability-questions",
    "href": "02-outdoor-reliability.html#reliability-questions",
    "title": "Outdoor log reliability",
    "section": "Reliability questions",
    "text": "Reliability questions\n\nHow do log temperature readings compare to the nearest weather station?\n\ndiff in temp\ndiff in heat index",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#setup-import-combine",
    "href": "02-outdoor-reliability.html#setup-import-combine",
    "title": "Outdoor log reliability",
    "section": "Setup, Import & Combine",
    "text": "Setup, Import & Combine\n\n\nExpand this to see code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(DT)\n\n\nWe’re bringing in:\n\nOur coded and cleaned outdoor log data\nHourly weather readings\nPrison unit information\n\n\n\nExpand this to see code\nlogs_all &lt;- read_rds(\"data-processed/01-outdoor-cleaned.rds\") # come from 01-outdoor-cleaning\nhourly &lt;- read_rds(\"data-processed/01-station-hourly-protocols.rds\")\nunits &lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\")\n\n\n\nClip log data\nWe have eight days of data, but we can clip it to a “week” as the last seven seven days of July, 2023. As of July 2025, we are not clipping.\n\n\nExpand this to see code\nlogs &lt;- logs_all\n# |&gt; filter(date != \"2023-07-24\")\n\nlogs |&gt; count(date) |&gt; adorn_totals() |&gt; tibble()\n\n\n\n  \n\n\n\n\n\nCombining files\nHere we bring in the unit info and hourly logs.\n\n\nExpand this to see code\n# getting station from units\nlogs_expanded &lt;- logs |&gt; \n  left_join(units, by = join_by(unit == unit_name, region)) |&gt; \n  # removing some unneeded cols\n  select(!c(unit_code, type, county, nws))\n\n# joining to get weather info\nlogs_nws &lt;- logs_expanded |&gt; \n  left_join(hourly, by = join_by(nws_id == station_id, date == date, hour == hr))\n\nlogs_expanded |&gt; filter(str_detect(unit, \"Jordan\"))\n\n\n\n  \n\n\n\n\n\nContext on matches\nHere we find the percentage of records with no NWS data.\n\n\nExpand this to see code\nmatch_checks &lt;- logs_nws |&gt; \n  mutate(match_null = if_else(is.na(name), T, F))\n\nmatch_checks |&gt; \n  tabyl(match_null) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nAbout 15 percent of our hourly logs don’t have a station match. Here TRUE values means there is missing station data.\nLet’s take a look at which ones are at issue:\n\n\nExpand this to see code\nmatch_checks |&gt; \n  tabyl(unit, match_null) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting() |&gt; \n  adorn_ns() |&gt; \n  tibble()\n\n\n\n  \n\n\n\nIn some cases we didn’t have a prison close enough. In other cases – like Jordan – we had a station, but there were no recordings for the time period.\nHere is a filtered list to more easily see units that have no NWS readings:\n\n\nExpand this to see code\nno_compare &lt;- match_checks |&gt; \n  tabyl(unit, match_null) |&gt; \n  filter(`FALSE` == 0) |&gt; \n  tibble()\n\nno_compare\n\n\n\n  \n\n\n\nWe’re going to remove these units going forward, leaving us with 82 units.\n\n\nExpand this to see code\nno_compare_units &lt;- no_compare |&gt; pull(unit)\n\nlogs_nws_compare &lt;- logs_nws |&gt; \n  filter(!unit %in% no_compare_units)\n\nlogs_nws_compare |&gt; count(unit) |&gt; nrow()\n\n\n\n\nTA: Missing weather stations\nWe don’t have a matching weather station for 11 of the 82 unites. Using visualcrossing we might be able to do a better job finding weather stations, and also to keep a better record of how far away these stations are from the units. We will do this, but later.\nALSO: In May25 work we removed stations with no matching weather station because we were looking at reliability. But if we want instead to look at protocols based on internal records, we shouldn’t remove them.",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#reliability-answers",
    "href": "02-outdoor-reliability.html#reliability-answers",
    "title": "Outdoor log reliability",
    "section": "Reliability answers",
    "text": "Reliability answers\nWe must remember these weather stations may be up to 40 miles away from the unit.\n\nHow do log temperature readings compare to the nearest weather station?\n\ndiff in temp\ndiff in heat index\n\n\nHere we find the difference in the prison recorded temp and heat index compared to the nearest station, if we have one. In some cases the diffs are NA because we don’t have a nearby station, or one of the calculating numbers is missing for whatever reason.\nWe only do this heat index calculation if one of the temperatures if above 80 degrees, because heat indexes get wonky when it is below 80.\n\n\nExpand this to see code\nlogs_diffs &lt;- logs_nws_compare |&gt; \n  mutate(\n    tmp_diff = temp - tmp,\n    hi_diff = case_when(\n      temp &gt;= 80 | tmp &gt;= 80 ~ hi_wc - hi,\n      .default = NA\n    )\n  )\n\nlogs_diffs |&gt; slice_sample(n = 20)",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#temperature-differences",
    "href": "02-outdoor-reliability.html#temperature-differences",
    "title": "Outdoor log reliability",
    "section": "Temperature differences",
    "text": "Temperature differences\n\nAverage temp difference\nHere we look across the dataset at the diff_tmp (unit temperature - weather staiton temperaure).\n\n\nExpand this to see code\nlogs_diffs |&gt; \n  summarise(\n    max_tmp_diff = max(tmp_diff, na.rm = T),\n    avg_tmp_diff = (mean(tmp_diff, na.rm = T) * 100) |&gt; round(1),\n    med_tmp_diff = median(tmp_diff, na.rm = T)\n  )\n\n\n\n  \n\n\n\n\n\nDistribution temp difference\nLet’s do a quick plot of these to see how the are distributed. i.e., how many rows are within x degrees of the weather station temperature. Note: there are 248 blank rows where we could not determine the difference because of a missing value.\n\n\nExpand this to see code\nggplot(logs_diffs, aes(x = tmp_diff)) +\n  geom_histogram(binwidth = 1, fill = \"lightblue\", color = \"black\") +\n  labs(\n    title = \"Compare temp draft\",\n    subtitle = str_wrap(\"Each bar is how many unit records are within x degrees of the closest weather station.\"))\n\n\nWarning: Removed 250 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nI can see that there are a just a couple of outliers Let’s consider removing those and see the spread mo betta.\nThere are only five records (out of 11,928) where the temp is 20 or more degrees off.\n\n\nExpand this to see code\n# the outliers\nlogs_diffs |&gt; \n  filter(abs(tmp_diff) &gt;= 20)\n\n\n\n  \n\n\n\nExpand this to see code\n# chart it\nlogs_diffs |&gt; \n  filter(abs(tmp_diff) &lt; 20) |&gt; \n  ggplot() +\n  aes(x = tmp_diff) +\n  geom_histogram(binwidth = 2, fill = \"lightblue\", color = \"black\") +\n  scale_x_continuous(breaks = seq(-20, 20, 2)) +\n  theme(panel.grid.minor.x = element_blank()) +\n  labs(\n    title = \"Most temps within about 4 degrees\",\n    subtitle = str_wrap(\"Each bar is how many unit records are within x degrees of the closest weather station. We've removed any rows that are more the 20 degrees off (of which there are five).\")\n  )\n\n\n\n\n\n\n\n\n\nHow many rows negative vs positive?\n\n\nExpand this to see code\nlogs_diffs |&gt; filter(tmp_diff &lt;= 0) |&gt; nrow() # unit is lower\n\n\n[1] 7053\n\n\nExpand this to see code\nlogs_diffs |&gt; filter(tmp_diff &gt; 0) |&gt; nrow() # unit is higher\n\n\n[1] 6329\n\n\n\n\nStandard deviation temp difference\nThis might be too technical or unneeded, but let’s look at this using the standard deviation. The standard deviation describes how much the values in the dataset typically vary from the mean (average). Like how whacked are they.\n\n\nExpand this to see code\n# The standard deviation of tmp_diff\n# logs_diffs |&gt; summarise(td_sd = sd(tmp_diff, na.rm = T))\ntmp_diff_sd &lt;- \n  logs_diffs |&gt;\n  filter(abs(tmp_diff) &lt; 20) |&gt;\n  pull(tmp_diff) |&gt; sd(na.rm = T)\n\ntmp_diff_sd\n\n\n[1] 4.131502\n\n\nExpand this to see code\n# percent outside the sd\nlogs_diffs |&gt; \n  filter(abs(tmp_diff) &lt; 20) |&gt; \n  mutate(\n    td_sd_out = case_when(\n      abs(tmp_diff) &gt; tmp_diff_sd ~ T,\n      .default = F\n    )\n  ) |&gt; \n  tabyl(td_sd_out) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nHere about 1/4 of the rows are outside the standard deviation.\n\n\nPercent temp within range\nThis might make more sense to a human: What percentage of the overall records fall within 2 degrees of the nearest weather station. How about within 4 degrees?\nWe are NOT removing outliers here.\nThe TRUE value here is the percentage of records within 2 or 4 degrees, respectively. To be clear, within 4 degrees also includes those within 2 degrees.\n\n\nExpand this to see code\ntmp_diff_2_4 &lt;- logs_diffs |&gt;\n  # filter(abs(tmp_diff) &lt; 20) |&gt; \n  filter(!is.na(tmp_diff)) |&gt; \n  mutate(in_2d = abs(tmp_diff) &lt;= 2,\n         in_4d = abs(tmp_diff) &lt;= 4)\n\ntmp_diff_2_4 |&gt; \n  tabyl(in_2d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nExpand this to see code\ntmp_diff_2_4 |&gt; \n  tabyl(in_4d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nNow that seems easier to understand than standard deviation.\n\n\nWithin range degrees by unit\nLet’s see how this differs by unit.\nin_2d_prc is the percentage of records for that unit within 2 degrees the closest weather station. in_4d_prc is the same for within 4 degrees.\n\n\nExpand this to see code\ntmp_diff_2_4 |&gt; \n  group_by(unit) |&gt; \n  summarize(\n    cnt = n(),\n    in_2d_true = sum(in_2d == T),\n    in_2d_prc = ((in_2d_true / cnt) * 100) |&gt; round(1),\n    in_4d_true = sum(in_4d == T),\n    in_4d_prc = ((in_4d_true / cnt) * 100) |&gt; round(1)\n  ) |&gt; \n  # removes the cnt true rows from display\n  select(unit, ends_with(\"prc\")) |&gt; \n  arrange(in_2d_prc)\n\n\n\n  \n\n\n\nSorted above based on lowest percentage within 2 degrees.\nNow, these numbers could really depend on how close the weather station is to the unit. For instance, I’m not happy with the choice of a station at Pearland Regional Airport for the Ramsey unit (maybe 30 miles?). The Angleton/Brazoria airport is closer (13 miles), as is Houston Southwest Airport (14 miles).\n\nWe will check all the weather stations and add distance between unit and weather station at a future date.",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#heat-index",
    "href": "02-outdoor-reliability.html#heat-index",
    "title": "Outdoor log reliability",
    "section": "Heat index",
    "text": "Heat index\n\nHI diff distribution\nLet’s do the same for heat index. We’ll skip the standard deviations and just look at the distribution and percentages.\nThere are about 50 records outside a 20 degree difference that we show here, but remove for the plot.\n\n\nExpand this to see code\nlogs_diffs |&gt; \n  filter(abs(hi_diff) &gt;= 20)\n\n\n\n  \n\n\n\nExpand this to see code\nlogs_diffs |&gt; \n  filter(abs(hi_diff) &lt; 20) |&gt;\n  ggplot() +\n  aes(x = hi_diff) +\n  geom_histogram(binwidth = 2, fill = \"lightblue\", color = \"black\") +\n  scale_x_continuous(breaks = seq(-20, 20, 2)) +\n  theme(panel.grid.minor.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nWithin HI range\nHere we find the percentage of heat index records within 2 or 4 degrees. We are NOT removing outliers here.\nTRUE means within 2 or 4 degrees, respectively.\n\n\nExpand this to see code\nhi_diff_2_4 &lt;- logs_diffs |&gt;\n  filter(!is.na(hi_diff)) |&gt; \n  mutate(in_2d = abs(hi_diff) &lt;= 2,\n         in_4d = abs(hi_diff) &lt;= 4)\n\nhi_diff_2_4 |&gt; \n  tabyl(in_2d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nExpand this to see code\nhi_diff_2_4 |&gt; \n  tabyl(in_4d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\n\n\nWithin HI range by unit\nAnd now to do that by unit. We have more cases where we don’t have the heat index for both the unit and the weather station, so we include the count of records we are comparing here.\n\n\nExpand this to see code\nhi_diff_2_4 |&gt; \n  group_by(unit) |&gt; \n  summarize(\n    cnt = n(),\n    in_2d_true = sum(in_2d == T),\n    in_2d_prc = ((in_2d_true / cnt) * 100) |&gt; round(1),\n    in_4d_true = sum(in_4d == T),\n    in_4d_prc = ((in_4d_true / cnt) * 100) |&gt; round(1)\n  ) |&gt; \n  # removes the cnt true rows from display\n  select(unit, cnt, ends_with(\"prc\")) |&gt; \n  arrange(in_2d_prc)",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  }
]