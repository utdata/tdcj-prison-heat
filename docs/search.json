[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TDCJ Prison Heat",
    "section": "",
    "text": "This is for a group project under UT’s Media Innovation Group.\nIt is an examination of recorded and actual temperatures in jails across the state of Texas using documents obtained through the Texas Department of Criminal Justice and National Weather Service data. More extensive sourcing information can be found on the project’s cleaning files.\nLooking at the navigation of this site, you’ll mainly want to look at the files under the Analysis heading.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#outdoor-logs",
    "href": "index.html#outdoor-logs",
    "title": "TDCJ Prison Heat",
    "section": "Outdoor logs",
    "text": "Outdoor logs\nLauren McGaughey of the Texas Newsroom public radio collaborative acquired through a public information request the outdoor temperature logs kept by Texas prisons. The logs are hand-written and the data could not be accurately pulled from them using AI, so a group of Media Innovation Group fellows transcribed about a week of logs for each prison, from July 24, 2023 through July 31, 2023.\nWe wanted to explore several angles:\n\nHow well are the logs kept?\n\nAre the legible and accurate?\nAre they accurate?\n\n\nHere is what we found:\n\nBasics and legibility\n\nWe transcribed 8 days of logs for 82 units, making up about 15,700 rows of data. For some analysis we used just the last 7 days of the month.\nIf we a row of the data was corrected or there was a question about the legibility, we included a note within that row. We included some consistent-worded notes so we could count them, but also had some more free-form when those categories didn’t fit. These are 7-day figures. We tried to be consistent, but different humans were involved, so variability is inevitable.\n\nAbout 15% of the records had notes.\nAbout 1,300 records (7.5%) had corrections.\nAbout 1,000 records (6.5%) we marked with legibility issues.\n\nOf those with notes, we also counted what was at issue.\n\nThere were about 650 notes concerning the heat index/wind chill column. That column is challenging because it measures two different things. Some records would also sometimes include what we determined was a heat index category, which we recorded in a separate column.\nThere were about 590 notes about the temperature column.\n\nWe wanted to see for which units we added notes.\n\nMore than 60% of the records for the Stevenson unit had notes, and many of those were flagged as “corrections”. However, when you look at the original document, it appears the unit reviews the logs and clears up any legibility issues. The logs are also signed. i.e. having more corrections could be a positive thing.\nIn some cases we recorded an overall note about the unit as opposed to notes for each individual line. They are printed in browsable form here.\n\n\n\n\nReliability\nThe first thing we wanted to do was calculate a nuanced “protocol flag” which factors into whether prisons put in protocols to mitgate exteme heat. If units have three days of 105 degree temperatures OR a heat index of 113, then they go into protocols to mitigate the heat.\nSince we only have 8 days of logs, we just calculated the protocol flag and didn’t do the run, but you can see them easily enough.\n\nThere were 19 units that had three or more protocol flag days.\nOnly a few of those units ever went into active protocol on those days\n\nIn some cases the flag was true for 7 or 8 days, yet protocols never went active.\n\n\nTo guage accuracy we paired each prison unit with a “nearby” weather station so we could compare the tempartures.\n\nI’d like to take another crack at pairing these weather stations and include the distance the weather station is from the prison unit. I’ve found a few issues, along with some solutions to them, but they’ve yet to be implemented. For now, consider these stats with caution.\n\n\nOn average, temperatures are recorded -2.9 degrees lower than the closest weather station.\nAbout half of the temperatures are within 2 degrees, and 3/4ths are within 4 degrees.\n\nYou can peruse these ranges by unit.\n\nThe heat index differences are a little harder because of so much missing data, but we’ve charted them.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#indoor-temperatures",
    "href": "index.html#indoor-temperatures",
    "title": "TDCJ Prison Heat",
    "section": "Indoor temperatures",
    "text": "Indoor temperatures\nWe do have indoor temperatures but there were a lot of changes in the other data like activations and weather so it needs to be reworked. That will have to come later.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#collaborators",
    "href": "index.html#collaborators",
    "title": "TDCJ Prison Heat",
    "section": "Collaborators",
    "text": "Collaborators\nWorking on the data analysis for indoor logs and weather station data:\n\nPearson Neal\nJohan Villatoro\nAli Juell\nKarina Kumar\n\nAli Juell served as project lead and Christian McDonald mentored and edited.\nThose transcribing the outdoor logs included:\n\nEmily deMotte\nTeresa Do\nNicolas Pinto\nDiego Torrealba",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html",
    "href": "02-outdoor-readability.html",
    "title": "Outdoor log readability",
    "section": "",
    "text": "We hand-coded eight days of temperatures logs (7/24/2023 to 7/31/2023) for 82 Texas prisons. Here we take those logs and try to answer the following questions.",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#questions-to-answer",
    "href": "02-outdoor-readability.html#questions-to-answer",
    "title": "Outdoor log readability",
    "section": "Questions to answer",
    "text": "Questions to answer\n\nIn the logs, how many cases did we find where the records were hard to read or had problems?\nWhat were those problems, generally?\nDo certain units have more legibility problems?\n\n\nSomething we’ve yet to look at: there are cases where the log does not have a heat index hi_wc1 recorded, but there is a temperature and humidity. We could calculate that heat index when we have the data (and then compare to what the prison log has when it is present.)",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#setup",
    "href": "02-outdoor-readability.html#setup",
    "title": "Outdoor log readability",
    "section": "Setup",
    "text": "Setup\n\n\nExpand this to see code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(DT)",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#import",
    "href": "02-outdoor-readability.html#import",
    "title": "Outdoor log readability",
    "section": "Import",
    "text": "Import\nWe’re bringing in:\n\nOur coded and cleaned outdoor log data\nActivation records\nHourly weather readings\nPrison unit information\n\n\n\nExpand this to see code\nlogs_all &lt;- read_rds(\"data-processed/01-outdoor-cleaned.rds\")\nactivations &lt;- read_rds(\"data-processed/01-activation-cleaned.rds\")\nhourly &lt;- read_rds(\"data-processed/01-station-hourly-protocols.rds\")\nunits &lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\")",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#basic-information",
    "href": "02-outdoor-readability.html#basic-information",
    "title": "Outdoor log readability",
    "section": "Basic information",
    "text": "Basic information\nPeek at a sample\n\n\nExpand this to see code\nlogs_all |&gt; slice_sample(n = 5)\n\n\n\n  \n\n\n\nand glimpse the columns …\n\n\nExpand this to see code\nlogs_all |&gt; glimpse()\n\n\nRows: 15,744\nColumns: 13\n$ unit     &lt;chr&gt; \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd…\n$ region   &lt;chr&gt; \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"…\n$ date     &lt;date&gt; 2023-07-24, 2023-07-24, 2023-07-24, 2023-07-24, 2023-07-24, …\n$ rec      &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"…\n$ datetime &lt;dttm&gt; 2023-07-24 00:30:00, 2023-07-24 01:30:00, 2023-07-24 02:30:0…\n$ hour     &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ temp     &lt;dbl&gt; 83, 82, 81, 81, 80, 79, 79, 80, 83, 86, 90, 91, 92, 96, 100, …\n$ humid    &lt;dbl&gt; 54, 62, 67, 71, 74, 76, NA, 79, 72, 69, 61, 57, 53, 44, 36, 3…\n$ wind     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ hi_wc    &lt;dbl&gt; 65, 68, 69, 71, 71, 71, NA, 84, 86, 95, 100, 100, 100, 103, 1…\n$ hi_wc_n  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ person   &lt;chr&gt; \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. J…\n$ notes    &lt;chr&gt; NA, NA, NA, NA, \"hi_wc corrected\", \"humid corrected\", NA, NA,…\n\n\nDate range of the data\n\n\nExpand this to see code\nlogs_all$date |&gt; summary()\n\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2023-07-24\" \"2023-07-25\" \"2023-07-27\" \"2023-07-27\" \"2023-07-29\" \"2023-07-31\" \n\n\n\nCount units\n\n\nExpand this to see code\nlogs_all |&gt; count(unit)\n\n\n\n  \n\n\n\n\n\nClip log data\nWe’ll remove July 24th so we have the last seven days of July, 2023.\n\n\nExpand this to see code\nlogs &lt;- logs_all |&gt; filter(date != \"2023-07-24\")\n\nlogs |&gt; count(date) |&gt; adorn_totals() |&gt; tibble()\n\n\n\n  \n\n\n\n\n\nTA: Basics\nWe transcribed outdoor temperature logs from 82 different units within the Texas prison system. Each log had 192 entries (24 hours for 8 days). We’ve clipped these records to be the last 7 days of July 2023, for a total of 13,776 records.",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#readability-answers",
    "href": "02-outdoor-readability.html#readability-answers",
    "title": "Outdoor log readability",
    "section": "Readability answers",
    "text": "Readability answers\nWhen we transcribed the outdoor temperature logs, we added notes when something was illegible or corrected on the form. Here we analyze those notes.\n\nRecords with notes\nHere we set flags if a record (an hour within a log) had a note, along with some categories. The result here is just a record sample to check our work.\n\n\nExpand this to see code\nlogs_notes &lt;- logs |&gt;\n  mutate(\n    notes_a = if_else(is.na(notes), F, T),\n    notes_c = case_when(str_detect(notes, \"correct\") ~ T, .default = F),\n    notes_l = case_when(str_detect(notes, \"legi\") ~ T, .default = F),\n  )\n\nlogs_notes |&gt; \n  select(unit, date, starts_with(\"notes\")) |&gt; \n  slice_sample(n = 20)\n\n\n\n  \n\n\n\nThis is the total percentage of records where we included some kind of note. TRUE means we included a note.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  tabyl(notes_a) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nWhen we recorded notes, we had some standardization. We included the term “corrected” if a record was scratched out and replaced with a new value or otherwise amended.\nThis is the percentage of records where something was corrected, per our notes.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  tabyl(notes_c) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nIf there was a legibility problem, we included the term “legibility”. This is the percentage of records where we noted some kind of legibility problem.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  tabyl(notes_l) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\n\n\nTA: Notes statistics\nWe have some kind of note in about 15% of all records. About 7.5% of records had a correction of some kind, and about 6.5% had legibility issues. (Some records might have both.). We should remember that data fellows had to use personal judgement on what to record and how, and that four different individuals performed the transcriptions. We tried our best to be consistent, but we are humans.\n\n\nTypes of notes\nSome records have more than one note. Here we “explode” those to count the notes individually. In some cases we had some standard notes, in other cases we didn’t.\nThe result here is just a sample to check our work.\n\n\nExpand this to see code\nnotes_exploded &lt;- logs |&gt; \n  select(unit, date, notes) |&gt; \n  filter(!is.na(notes)) |&gt; \n  group_by(unit, date) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  ungroup()\n\n# number of rows in new tibble\nnotes_exploded |&gt; nrow()\n\n\n[1] 2652\n\n\nExpand this to see code\n# sample rows\nnotes_exploded |&gt; slice_sample(n = 10)\n\n\n\n  \n\n\n\nLet’s look at the kinds of notes we recorded.\n\n\nExpand this to see code\nnotes_exploded_cnts &lt;- notes_exploded |&gt; count(notes, sort = T)\n\nnotes_exploded_cnts\n\n\n\n  \n\n\n\nHere we count how many individual records we labeled as “corrected” or had a “legibility” concern. I also counted a couple of other instances I saw, like where a “double” or “range” of values were recorded.\n\n\nExpand this to see code\nnotes_exploded_cnts |&gt;\n  # filter(str_detect(notes, \"corrected\")) |&gt; \n  summarise(\n    total_indiv_corrected = sum(n[str_detect(notes, \"correct\")]),\n    total_indiv_legibility = sum(n[str_detect(notes, \"legib\")]),\n    total_indiv_double = sum(n[str_detect(notes, \"double\")]),\n    total_indiv_range = sum(n[str_detect(notes, \"range\")]),\n    ) |&gt; \n  pivot_longer(cols = everything())\n\n\n\n  \n\n\n\n\n\nTA: Types statistics\nMost of the individual notes identified (about 1,300) were corrections of some kind. About 1,000 of them concerned legibility.\n\n\nVariable counts\nHere we try to get a handle on which variables recorded had the most notes (i.e., temp vs wind, etc.). We are counting how many times our variable terms were included in individual notes.\n\n\nExpand this to see code\nnotes_exploded_cnts |&gt;\n  # filter(str_detect(notes, \"corrected\")) |&gt; \n  summarise(\n    total_indiv_temp = sum(n[str_detect(notes, \"temp\")]),\n    total_indiv_humid = sum(n[str_detect(notes, \"humid\")]),\n    total_indiv_wind = sum(n[str_detect(notes, \"wind\")]),\n    total_indiv_hi_wc = sum(n[str_detect(notes, \"hi_wc\")]),\n    total_indiv_hi_wc_n = sum(n[str_detect(notes, \"hi_wc_n\")]),\n    total_indiv_person = sum(n[str_detect(notes, \"person\")]),\n  ) |&gt; \n  pivot_longer(everything()) |&gt; \n  arrange(value |&gt; desc())\n\n\n\n  \n\n\n\nWe had more notes on the hi_wc variable (Heat index/Wind chill) than any other, followed by temperature.\nThe heat index/wind chill record is already challenging because it measures two different things. Some records would also sometimes include what we determined was a heat index “category”, which we recorded in a separate column.\nHere we see how much that field was at issue by counting any notes that included hi_wc. The heat index category notes show up in this list, too.\n\n\nExpand this to see code\nnotes_exploded_cnts |&gt; \n  filter(str_detect(notes, \"hi_wc\")) |&gt; \n  adorn_totals() |&gt; tibble()\n\n\n\n  \n\n\n\n\n\nTA: Variable statistics\nThe heat index/wind chill columns was at issue about 650 times, with about half of these being corrections.\n\n\nNotes by unit\n\nUnits with most notes\nThis looks at which units had the most notes of any kind. The pct_notes is the percetage of rows that had a note of any kind.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  count(unit, notes_a) |&gt; \n  pivot_wider(names_from = notes_a, values_from = n) |&gt; \n  mutate(pct_notes = ((`TRUE` / (`FALSE` + `TRUE`)) * 100) |&gt; round(1)) |&gt; \n  arrange(pct_notes |&gt; desc())\n\n\n\n  \n\n\n\nTo gain some insight on what these might be, let’s look at these notes by Stevenson unit.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  filter(unit == \"Stevenson\" & !is.na(notes)) |&gt; \n  select(unit, region, date, hour, notes)\n\n\n\n  \n\n\n\nIt looks like there are many “corrections”. When you look at the original documents, it appears the unit must review the logs and regularly clears up any legibility issues. The logs are also signed. i.e. having more corrections could be a positive thing.\n\n\nUnit corrections, legibilty\nHere we explode all the notes for all the units and count how many are for corrections and legibility.\nWe sort the same list twice … once by corrections and once by legibility.\n\n\nExpand this to see code\nunits_cor_leg &lt;- logs_notes |&gt; \n  filter(!is.na(notes)) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  count(unit, region, notes, sort = T) |&gt; \n  group_by(unit, region) |&gt; \n  summarise(\n    total_indiv_correct = sum(n[str_detect(notes, \"correct\")]),\n    total_indiv_legibility = sum(n[str_detect(notes, \"legib\")]),\n    .groups = \"drop\"\n    )\n\nunits_cor_leg |&gt; \n  arrange(total_indiv_correct |&gt; desc())\n\n\n\n  \n\n\n\nExpand this to see code\nunits_cor_leg |&gt; \n  arrange(total_indiv_legibility |&gt; desc())\n\n\n\n  \n\n\n\nLet’s look at bit more at Telford to see the legibility issues.\n\n\nExpand this to see code\nlogs_notes |&gt; \n  filter(unit == \"Telford\" & !is.na(notes)) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  count(notes, sort = T)\n\n\n\n  \n\n\n\nAnd then Glossbrenner …\n\n\nExpand this to see code\nlogs_notes |&gt; \n  filter(unit == \"Glossbrenner\" & !is.na(notes)) |&gt; \n  separate_longer_delim(notes, delim = \", \") |&gt; \n  count(notes, sort = T)\n\n\n\n  \n\n\n\n\n\n\nTA: Notes by unit\nThe units with the most correction notes include Garza West, Briscoe, Stevenson, Connally, Dominguez. Given what we found with Stevenson, this may mean they are more accurate, but they should be reviewed.\nWhen it comes to legibility, the Telford unit stands out. Most of the issues are around the signature of the person recording the record.",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "02-outdoor-readability.html#unit-notes",
    "href": "02-outdoor-readability.html#unit-notes",
    "title": "Outdoor log readability",
    "section": "Unit notes",
    "text": "Unit notes\nIn some cases we recorded an overall note about the unit as opposed to notes for each individual line. I’m just printing all of these out for perusal.\n\n\nExpand this to see code\noverall_notes &lt;- read_rds(\"data-processed/01-outdoor-notes.rds\")\n\noverall_notes |&gt; datatable()",
    "crumbs": [
      "Analysis",
      "Outdoor log readability"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html",
    "href": "01-unit-info-cleaning.html",
    "title": "Unit info cleaning",
    "section": "",
    "text": "This data piece will help us join together our unit-focused data and our weather station-focused data. The data was compiled by Media Innovation Group data fellows. The information came from the Texas Department of Criminal Justice website, a TDCJ unit prototype list and the National Weather Service website.",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#goals-of-this-notebook",
    "href": "01-unit-info-cleaning.html#goals-of-this-notebook",
    "title": "Unit info cleaning",
    "section": "Goals of this notebook",
    "text": "Goals of this notebook\nClean up our column names so we can easily combine with our other data.",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#loading-libraries",
    "href": "01-unit-info-cleaning.html#loading-libraries",
    "title": "Unit info cleaning",
    "section": "Loading libraries",
    "text": "Loading libraries\n\nlibrary(tidyverse)\nlibrary(janitor)",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#download-unit-info",
    "href": "01-unit-info-cleaning.html#download-unit-info",
    "title": "Unit info cleaning",
    "section": "Download unit info",
    "text": "Download unit info\n\nOption eval: fasle set to avoid re-download. Change to true for updates.\n\n\ndownload.file(\n  \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRebFs9O2i0HoygXTtIvuzdVTmyrjX3MeUorU9d4fpYkGX7Bb026OradFQ1MMk2ltcGnyILih6ow4F4/pub?gid=1653039441&single=true&output=csv\",\n  \"data-raw/unit-nws-info.csv\")",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#importing-unit-info",
    "href": "01-unit-info-cleaning.html#importing-unit-info",
    "title": "Unit info cleaning",
    "section": "Importing unit info",
    "text": "Importing unit info\nLet’s bring in our unit info sheet. This was compiled by us to be used in this project.\n\nnws_unit_raw &lt;- read_csv(\"data-raw/unit-nws-info.csv\") |&gt; clean_names()\n\nRows: 101 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): Unit Name, Region, Data Fellow, NWS, NWS ID, Unit Code, Type, Stre...\ndbl  (1): Driving miles\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnws_unit_raw |&gt; head()",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#select-columns",
    "href": "01-unit-info-cleaning.html#select-columns",
    "title": "Unit info cleaning",
    "section": "Select columns",
    "text": "Select columns\nI’m gonna change column names and only keep the columns we need.\n\nunit_info_clean &lt;- nws_unit_raw |&gt; \n  select(unit_name, region, unit_code, type, nws, nws_id, county)\n\nunit_info_clean",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-unit-info-cleaning.html#export-data",
    "href": "01-unit-info-cleaning.html#export-data",
    "title": "Unit info cleaning",
    "section": "Export data",
    "text": "Export data\nOur cleaning was pretty easy! Let’s export the data and put it into our processed folder now.\n\nunit_info_clean |&gt; write_rds(\"data-processed/01-unit-info-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Unit info cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html",
    "href": "01-outdoor-cleaning.html",
    "title": "Outdoor logs cleaning",
    "section": "",
    "text": "This notebook compiles data collected from one week of outdoor temperature logs collected by the Texas Department of Criminal Justice. The dates considered were July 24, 2023 through July 31, 2023. The logs are hand-written and data could not be accurately pulled from them using AI, so a group of Media Innovaiton Group fellows transcribed the week of logs for each prison we had records for, XXX in total. Those records were acquired through a public information request by Lauren McGaughey of the Texas Newsroom public radio collaborative.\nWe created a Google Spreadsheet file for each prison, recording the temperatures for each hour as best as we could decipher them. Here we download a tracking spreadsheet, which is then used to download all of the individual log sheets, combining them into a single file. When then prepare that file for analysis.",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#setup",
    "href": "01-outdoor-cleaning.html#setup",
    "title": "Outdoor logs cleaning",
    "section": "Setup",
    "text": "Setup",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#download-the-tracking-sheet",
    "href": "01-outdoor-cleaning.html#download-the-tracking-sheet",
    "title": "Outdoor logs cleaning",
    "section": "Download the tracking sheet",
    "text": "Download the tracking sheet\nThis sheet is in MIG Data &gt; Jail Logs &gt; Outdoor Logs Data. It is called Outdoor Logs Tracking Sheet.\n\nOptions are set to eval: false to avoid re-downloading. Set to true to update.\n\n\n\nExpand this to see code\ndownload.file(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRQBIKas7s5Wwe4bd9tMlAheCsjqeFU2JMZfk6h8Hc_QH6Dx02SEZmP6ieny13hCyZQosmsRirDEu9P/pub?output=csv\", \"data-raw/outdoor-jail-logs-urls.csv\")\n\n\nRead in the tracking sheet url\n\n\nExpand this to see code\nlog_urls &lt;- read_csv(\"data-raw/outdoor-jail-logs-urls.csv\") |&gt; clean_names()\n\nlog_urls |&gt; glimpse()\n\n\nRows: 101\nColumns: 7\n$ unit_name &lt;chr&gt; \"Bell\", \"Byrd\", \"Diboll\", \"Duncan\", \"Ellis\", \"Estelle\", \"Fer…\n$ unit_code &lt;chr&gt; \"CV\", \"DU\", \"DO\", \"N6\", \"E1\", \"E2\", \"FE\", \"GG\", \"GR\", \"NF\", …\n$ region    &lt;chr&gt; \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", \"I\", …\n$ slug      &lt;chr&gt; NA, \"r1-byrd\", NA, \"r1-duncan\", \"r1-ellis\", \"r1-estelle\", \"r…\n$ url       &lt;chr&gt; NA, \"https://docs.google.com/spreadsheets/d/e/2PACX-1vROqTyA…\n$ notes_gid &lt;dbl&gt; NA, 1133970569, NA, 535687266, 1770803795, 1551327169, 96070…\n$ notes     &lt;chr&gt; NA, NA, \"closely linked to Duncan, pre-release\", NA, NA, NA,…\n\n\nIn some cases we had the urls formatted incorrectly. Result should be 0 rows.\n\n\nExpand this to see code\nlog_urls |&gt; \n  filter(str_detect(url, \"pubhtml\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#download-all-the-files",
    "href": "01-outdoor-cleaning.html#download-all-the-files",
    "title": "Outdoor logs cleaning",
    "section": "Download all the files",
    "text": "Download all the files\nThis code below was courtesy of ChatGPT, based on the following prompt: “I’m using R. I have a data frame that has a list of urls and slugs, which are short file names. Please write me a loop or other code that will download all the files at those urls, with their names as the slug. These are all csv files.” There were some edits.\n\nOptions are set to eval: false to avoid re-downloading. Set to true to update.\n\n\n\nExpand this to see code\n# set our urls to df\ndf &lt;- log_urls\n\n# Download loop\nfor (i in seq_len(nrow(df))) {\n  url &lt;- df$url[i]\n  slug &lt;- df$slug[i]\n  file_name &lt;- paste0(\"data-raw/2023_logs/\", slug, \".csv\")\n  \n  if (is.na(url)) {\n    # message(paste(\"No URL for slug:\", slug))\n    next\n  } else {\n    tryCatch({\n      download.file(url, destfile = file_name, mode = \"wb\")\n    }, error = function(e) {\n      message(paste(\"Failed to download:\", url, \"-\", e$message, \"\\n\"))\n    })\n  }\n}\n\n\n\nMake a files list\nMake a list of files:\n\n\nExpand this to see code\nlogs_list &lt;- list.files(\n  \"data-raw/2023_logs\",\n  pattern = \".csv\",\n  full.names = TRUE\n  )\n\nlogs_list\n\n\n [1] \"data-raw/2023_logs/r1-byrd.csv\"              \n [2] \"data-raw/2023_logs/r1-duncan.csv\"            \n [3] \"data-raw/2023_logs/r1-ellis.csv\"             \n [4] \"data-raw/2023_logs/r1-estelle.csv\"           \n [5] \"data-raw/2023_logs/r1-ferguson.csv\"          \n [6] \"data-raw/2023_logs/r1-goodman.csv\"           \n [7] \"data-raw/2023_logs/r1-goree.csv\"             \n [8] \"data-raw/2023_logs/r1-holliday.csv\"          \n [9] \"data-raw/2023_logs/r1-huntsville.csv\"        \n[10] \"data-raw/2023_logs/r1-lewis.csv\"             \n[11] \"data-raw/2023_logs/r1-polunsky.csv\"          \n[12] \"data-raw/2023_logs/r1-wainwright.csv\"        \n[13] \"data-raw/2023_logs/r1-wynne.csv\"             \n[14] \"data-raw/2023_logs/r2-beto.csv\"              \n[15] \"data-raw/2023_logs/r2-boyd.csv\"              \n[16] \"data-raw/2023_logs/r2-choice-moore.csv\"      \n[17] \"data-raw/2023_logs/r2-cole.csv\"              \n[18] \"data-raw/2023_logs/r2-hutchins.csv\"          \n[19] \"data-raw/2023_logs/r2-johnston.csv\"          \n[20] \"data-raw/2023_logs/r2-michael.csv\"           \n[21] \"data-raw/2023_logs/r2-powledge.csv\"          \n[22] \"data-raw/2023_logs/r2-skyview.csv\"           \n[23] \"data-raw/2023_logs/r2-telford.csv\"           \n[24] \"data-raw/2023_logs/r3-clemens.csv\"           \n[25] \"data-raw/2023_logs/r3-gist.csv\"              \n[26] \"data-raw/2023_logs/r3-henley.csv\"            \n[27] \"data-raw/2023_logs/r3-hightower.csv\"         \n[28] \"data-raw/2023_logs/r3-hospital-galveston.csv\"\n[29] \"data-raw/2023_logs/r3-jester-3.csv\"          \n[30] \"data-raw/2023_logs/r3-kegans.csv\"            \n[31] \"data-raw/2023_logs/r3-leblanc.csv\"           \n[32] \"data-raw/2023_logs/r3-lychner.csv\"           \n[33] \"data-raw/2023_logs/r3-memorial.csv\"          \n[34] \"data-raw/2023_logs/r3-plane.csv\"             \n[35] \"data-raw/2023_logs/r3-ramsey.csv\"            \n[36] \"data-raw/2023_logs/r3-scott.csv\"             \n[37] \"data-raw/2023_logs/r3-stiles.csv\"            \n[38] \"data-raw/2023_logs/r3-stringfellow.csv\"      \n[39] \"data-raw/2023_logs/r3-terrell.csv\"           \n[40] \"data-raw/2023_logs/r3-vance.csv\"             \n[41] \"data-raw/2023_logs/r3-young.csv\"             \n[42] \"data-raw/2023_logs/r4-briscoe.csv\"           \n[43] \"data-raw/2023_logs/r4-connally.csv\"          \n[44] \"data-raw/2023_logs/r4-cotulla.csv\"           \n[45] \"data-raw/2023_logs/r4-dominguez.csv\"         \n[46] \"data-raw/2023_logs/r4-fort-stockton.csv\"     \n[47] \"data-raw/2023_logs/r4-garza-west.csv\"        \n[48] \"data-raw/2023_logs/r4-glossbrenner.csv\"      \n[49] \"data-raw/2023_logs/r4-lopez.csv\"             \n[50] \"data-raw/2023_logs/r4-lynaugh.csv\"           \n[51] \"data-raw/2023_logs/r4-mcconnell.csv\"         \n[52] \"data-raw/2023_logs/r4-sanchez.csv\"           \n[53] \"data-raw/2023_logs/r4-stevenson.csv\"         \n[54] \"data-raw/2023_logs/r4-torres.csv\"            \n[55] \"data-raw/2023_logs/r5-allred.csv\"            \n[56] \"data-raw/2023_logs/r5-dalhart.csv\"           \n[57] \"data-raw/2023_logs/r5-daniel.csv\"            \n[58] \"data-raw/2023_logs/r5-formby.csv\"            \n[59] \"data-raw/2023_logs/r5-jordan.csv\"            \n[60] \"data-raw/2023_logs/r5-mechler.csv\"           \n[61] \"data-raw/2023_logs/r5-montford.csv\"          \n[62] \"data-raw/2023_logs/r5-roach.csv\"             \n[63] \"data-raw/2023_logs/r5-smith.csv\"             \n[64] \"data-raw/2023_logs/r5-wallace.csv\"           \n[65] \"data-raw/2023_logs/r5-wheeler.csv\"           \n[66] \"data-raw/2023_logs/r6-crain.csv\"             \n[67] \"data-raw/2023_logs/r6-halbert.csv\"           \n[68] \"data-raw/2023_logs/r6-hamilton.csv\"          \n[69] \"data-raw/2023_logs/r6-havins.csv\"            \n[70] \"data-raw/2023_logs/r6-hilltop.csv\"           \n[71] \"data-raw/2023_logs/r6-hobby.csv\"             \n[72] \"data-raw/2023_logs/r6-hughes.csv\"            \n[73] \"data-raw/2023_logs/r6-luther.csv\"            \n[74] \"data-raw/2023_logs/r6-marlin.csv\"            \n[75] \"data-raw/2023_logs/r6-middleton.csv\"         \n[76] \"data-raw/2023_logs/r6-o-daniel.csv\"          \n[77] \"data-raw/2023_logs/r6-pack.csv\"              \n[78] \"data-raw/2023_logs/r6-robertson.csv\"         \n[79] \"data-raw/2023_logs/r6-san-saba.csv\"          \n[80] \"data-raw/2023_logs/r6-sayle.csv\"             \n[81] \"data-raw/2023_logs/r6-travis-county.csv\"     \n[82] \"data-raw/2023_logs/r6-woodman.csv\"           \n\n\n\n\nFile checks\nA little ChatGPT help here to find a way to count the number of lines in each file. There should be 193 in each one. In some cases our spreadsheets included blank rows, which we fixed. There is still a blank column, but we take care of that later.\nThis takes the list of files above and counts the lines. When then filter the list for any that aren’t 193 lines long. The result should be 0 rows.\n\n\nExpand this to see code\ncount_lines &lt;- function(file) {\n  length(readLines(file, warn = FALSE))\n}\n\nline_counts &lt;- sapply(logs_list, count_lines)\nline_counts_data &lt;- tibble(file = basename(names(line_counts)), lines = line_counts)\n\nline_counts_data |&gt; filter(lines != 193)\n\n\n\n  \n\n\n\nSometimes we needed to check a specific file being downloaded.\n\n\nExpand this to see code\n# download.file(\n#   \"https://docs.google.com/spreadsheets/d/e/2PACX-1vToj_YKfQlL_ZvNFPKItQQDR01HhQu5Fsp0Z2xSSXvaYZrn_Xuw1kh0JDwBV-wSk-uoZhdG7C4Rytw1/pub?output=csv\",\n#   \"data-raw/2023_logs/r5-montford.csv\"\n# )",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#combine-the-files",
    "href": "01-outdoor-cleaning.html#combine-the-files",
    "title": "Outdoor logs cleaning",
    "section": "Combine the files",
    "text": "Combine the files\nAgain uses the logs_list from above.\n\n\nExpand this to see code\nlogs_raw &lt;- logs_list |&gt;  #set_names(basename) |&gt;\n  map(\n  read_csv,\n  col_types = cols(.default = col_character())\n) |&gt; list_rbind() |&gt;\n  clean_names()\n\n\nNew names:\nNew names:\nNew names:\n• `` -&gt; `...12`\n\n\nExpand this to see code\nlogs_raw |&gt; glimpse()\n\n\nRows: 15,744\nColumns: 12\n$ unit    &lt;chr&gt; \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\"…\n$ date    &lt;chr&gt; \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023…\n$ rec     &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",…\n$ time    &lt;chr&gt; \"12:30 a.m.\", \"1:30 a.m.\", \"2:30 a.m.\", \"3:30 a.m.\", \"4:30 a.m…\n$ temp    &lt;chr&gt; \"83\", \"82\", \"81\", \"81\", \"80\", \"79\", \"79\", \"80\", \"83\", \"86\", \"9…\n$ humid   &lt;chr&gt; \"54\", \"62\", \"67\", \"71\", \"74\", \"76\", \"N/A\", \"79\", \"72\", \"69\", \"…\n$ wind    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hi_wc   &lt;chr&gt; \"65\", \"68\", \"69\", \"71\", \"71\", \"71\", \"N/A\", \"84\", \"86\", \"95\", \"…\n$ hi_wc_n &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ person  &lt;chr&gt; \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Jo…\n$ notes   &lt;chr&gt; NA, NA, NA, NA, \"hi_wc corrected\", \"humid corrected\", NA, NA, …\n$ x12     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\n\nCheck and remove extra column\n\n\nExpand this to see code\nlogs_raw |&gt; filter(!is.na(x12))\n\n\n\n  \n\n\n\n\n\nExpand this to see code\nlogs_tight &lt;- logs_raw |&gt; select(-x12)\n\nlogs_tight |&gt; glimpse()\n\n\nRows: 15,744\nColumns: 11\n$ unit    &lt;chr&gt; \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\", \"Byrd\"…\n$ date    &lt;chr&gt; \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023\", \"7/24/2023…\n$ rec     &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",…\n$ time    &lt;chr&gt; \"12:30 a.m.\", \"1:30 a.m.\", \"2:30 a.m.\", \"3:30 a.m.\", \"4:30 a.m…\n$ temp    &lt;chr&gt; \"83\", \"82\", \"81\", \"81\", \"80\", \"79\", \"79\", \"80\", \"83\", \"86\", \"9…\n$ humid   &lt;chr&gt; \"54\", \"62\", \"67\", \"71\", \"74\", \"76\", \"N/A\", \"79\", \"72\", \"69\", \"…\n$ wind    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ hi_wc   &lt;chr&gt; \"65\", \"68\", \"69\", \"71\", \"71\", \"71\", \"N/A\", \"84\", \"86\", \"95\", \"…\n$ hi_wc_n &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ person  &lt;chr&gt; \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Johnson\", \"E. Jo…\n$ notes   &lt;chr&gt; NA, NA, NA, NA, \"hi_wc corrected\", \"humid corrected\", NA, NA, …",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#fix-date-time-rec",
    "href": "01-outdoor-cleaning.html#fix-date-time-rec",
    "title": "Outdoor logs cleaning",
    "section": "Fix date, time, rec",
    "text": "Fix date, time, rec\nTwo decisions in the formatting of our log necessitate this:\n\nWe configured our date and time columns like the form itself instead of datetime formats that easily import.\nWe originally numbered the rec column from 1 to 24 thinking we needed to label each reading, but the weather station data will be labeled 0 to 23 because it is the hour of time.\n\nHere we create a real datetime, date and proper hour.\nAlso, we remove artifact columns and any blank columns by specifying the columns we do need.\n\n\nExpand this to see code\nlogs_dates &lt;- logs_tight |&gt; \n  mutate(\n    new_time = case_when(\n      time == \"12:30 a.m.\" ~ \"00:30:00\",\n      time == \"12:30 p.m.\" ~ \"12:30:00\",\n      str_sub(time, -4, -1) == \"a.m.\" ~ paste0(str_extract(time, \"^(\\\\d+)\"),\":30:00\"),\n      str_sub(time, -4, -1) == \"p.m.\" ~ paste0(str_extract(time, \"^(\\\\d+)\") |&gt; as.numeric() + 12, \":30:00\"),\n      .default = NA\n    )\n  ) |&gt; \n  mutate(\n    datetime = mdy_hms(paste(date, new_time)),\n    date = mdy(date),\n    hour = hour(datetime),\n    .after = time\n  ) |&gt; \n  select(\n    unit,\n    date,\n    rec,\n    datetime,\n    hour,\n    temp,\n    humid,\n    wind,\n    hi_wc,\n    hi_wc_n,\n    person,\n    notes\n  )\n\nlogs_dates |&gt; slice_sample(n = 10)",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#convert-the-numbers",
    "href": "01-outdoor-cleaning.html#convert-the-numbers",
    "title": "Outdoor logs cleaning",
    "section": "Convert the numbers",
    "text": "Convert the numbers\nOur various readings come in as text. We convert them to numbers.\n\n\nExpand this to see code\nlogs_numbs &lt;- logs_dates |&gt; \n  mutate(\n    across(c(temp, humid, wind, hi_wc, hi_wc_n), parse_number)\n  )\n\n\nWarning: There were 5 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(c(temp, humid, wind, hi_wc, hi_wc_n), parse_number)`.\nCaused by warning:\n! 3 parsing failures.\n row col expected actual\n2451  -- a number    N/A\n2476  -- a number    N/A\n2478  -- a number    N/A\nℹ Run `dplyr::last_dplyr_warnings()` to see the 4 remaining warnings.\n\n\nExpand this to see code\nlogs_numbs |&gt; slice_sample(n = 10)\n\n\n\n  \n\n\n\n\n\nExpand this to see code\nlogs_numbs |&gt; filter(str_detect(unit, \"Moore\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#fix-names",
    "href": "01-outdoor-cleaning.html#fix-names",
    "title": "Outdoor logs cleaning",
    "section": "Fix names",
    "text": "Fix names\nIn some cases the name on the outdoor log sheet did not match our tracking sheet, which means we have trouble later when we need to match that information.\n\n\nExpand this to see code\nlogs_fixes &lt;- logs_numbs |&gt; \n  mutate(\n    unit = case_match(\n      unit,\n      \"Travis State Jail\" ~ \"Travis County\",\n      \"Choice Moore\" ~ \"Moore, C.\",\n      \"Jester 3\" ~ \"Jester III\",\n      \"Lane Murray\" ~ \"Murray\",\n      \"Wallace Pack\" ~ \"Pack\",\n      \"Mt. View\" ~ \"Woodman\",\n      .default = unit)\n  )\n\nlogs_fixes |&gt; filter(str_detect(unit, \"Jordan\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#add-the-region",
    "href": "01-outdoor-cleaning.html#add-the-region",
    "title": "Outdoor logs cleaning",
    "section": "Add the region",
    "text": "Add the region\nIt’s helpful later to have the region of the prison as part of this dataset, so I’m going to add it here from the unit info.\n\n\nExpand this to see code\nregions &lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\") |&gt; \n  select(unit_name, region)\n\nlogs_regions &lt;- logs_fixes |&gt; \n  left_join(regions, by = join_by(unit == unit_name)) |&gt; \n  relocate(region, .after = unit)\n\nlogs_regions |&gt; filter(str_detect(unit, \"Jordan\"))",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#export-the-logs",
    "href": "01-outdoor-cleaning.html#export-the-logs",
    "title": "Outdoor logs cleaning",
    "section": "Export the logs",
    "text": "Export the logs\n\n\nExpand this to see code\nlogs_regions |&gt; write_rds(\"data-processed/01-outdoor-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-outdoor-cleaning.html#notes-pages",
    "href": "01-outdoor-cleaning.html#notes-pages",
    "title": "Outdoor logs cleaning",
    "section": "Notes pages",
    "text": "Notes pages\nWhen we transcribed the logs, we also included a notes sheet where we added any overall relavant or interesting information.\nWe will be following the same steps as above, except that we have to piece together the download url from our tracking sheet.\n\n\nExpand this to see code\nnotes_urls &lt;- log_urls |&gt; \n  # filter(region == \"II\") |&gt; # filters to region for testing\n  select(slug, url, notes_gid) |&gt; \n  mutate(\n    pub_url = str_extract(url, \"(.*pub\\\\?)\", group = 1),\n    notes_url = paste0(pub_url, \"gid=\", notes_gid, \"&output=csv\")\n  ) |&gt; \n  filter(!is.na(url)) |&gt; \n  select(slug, notes_url) \n\nnotes_urls\n\n\n\n  \n\n\n\nThen we download all the files. eval: false is set here so files won’t be re-downloaded.\n\n\nExpand this to see code\n# set our urls to df\nnotes_df &lt;- notes_urls\n\n\n# Download loop\nfor (i in seq_len(nrow(notes_df))) {\n  url &lt;- df$notes_url[i]\n  slug &lt;- df$slug[i]\n  file_name &lt;- paste0(\"data-raw/2023_notes/\", slug, \".csv\")\n  \n  if (is.na(url)) {\n    # message(paste(\"No URL for slug:\", slug))\n    next\n  } else {\n    tryCatch({\n      download.file(url, destfile = file_name, mode = \"wb\")\n    }, error = function(e) {\n      message(paste(\"Failed to download:\", url, \"-\", e$message, \"\\n\"))\n    })\n  }\n}\n\n\nMake a list of file paths to combine.\n\n\nExpand this to see code\nnotes_list &lt;- list.files(\n  \"data-raw/2023_notes\",\n  pattern = \".csv\",\n  full.names = TRUE\n  )\n\nnotes_list\n\n\n [1] \"data-raw/2023_notes/r1-byrd.csv\"              \n [2] \"data-raw/2023_notes/r1-duncan.csv\"            \n [3] \"data-raw/2023_notes/r1-ellis.csv\"             \n [4] \"data-raw/2023_notes/r1-estelle.csv\"           \n [5] \"data-raw/2023_notes/r1-ferguson.csv\"          \n [6] \"data-raw/2023_notes/r1-goodman.csv\"           \n [7] \"data-raw/2023_notes/r1-goree.csv\"             \n [8] \"data-raw/2023_notes/r1-holliday.csv\"          \n [9] \"data-raw/2023_notes/r1-huntsville.csv\"        \n[10] \"data-raw/2023_notes/r1-lewis.csv\"             \n[11] \"data-raw/2023_notes/r1-polunsky.csv\"          \n[12] \"data-raw/2023_notes/r1-wainwright.csv\"        \n[13] \"data-raw/2023_notes/r1-wynne.csv\"             \n[14] \"data-raw/2023_notes/r2-beto.csv\"              \n[15] \"data-raw/2023_notes/r2-boyd.csv\"              \n[16] \"data-raw/2023_notes/r2-choice-moore.csv\"      \n[17] \"data-raw/2023_notes/r2-cole.csv\"              \n[18] \"data-raw/2023_notes/r2-hutchins.csv\"          \n[19] \"data-raw/2023_notes/r2-johnston.csv\"          \n[20] \"data-raw/2023_notes/r2-michael.csv\"           \n[21] \"data-raw/2023_notes/r2-powledge.csv\"          \n[22] \"data-raw/2023_notes/r2-telford.csv\"           \n[23] \"data-raw/2023_notes/r3-clemens.csv\"           \n[24] \"data-raw/2023_notes/r3-gist.csv\"              \n[25] \"data-raw/2023_notes/r3-henley.csv\"            \n[26] \"data-raw/2023_notes/r3-hightower.csv\"         \n[27] \"data-raw/2023_notes/r3-hospital-galveston.csv\"\n[28] \"data-raw/2023_notes/r3-jester-3.csv\"          \n[29] \"data-raw/2023_notes/r3-kegans.csv\"            \n[30] \"data-raw/2023_notes/r3-leblanc.csv\"           \n[31] \"data-raw/2023_notes/r3-lychner.csv\"           \n[32] \"data-raw/2023_notes/r3-memorial.csv\"          \n[33] \"data-raw/2023_notes/r3-plane.csv\"             \n[34] \"data-raw/2023_notes/r3-ramsey.csv\"            \n[35] \"data-raw/2023_notes/r3-scott.csv\"             \n[36] \"data-raw/2023_notes/r3-stiles.csv\"            \n[37] \"data-raw/2023_notes/r3-stringfellow.csv\"      \n[38] \"data-raw/2023_notes/r3-terrell.csv\"           \n[39] \"data-raw/2023_notes/r3-vance.csv\"             \n[40] \"data-raw/2023_notes/r3-young.csv\"             \n[41] \"data-raw/2023_notes/r4-briscoe.csv\"           \n[42] \"data-raw/2023_notes/r4-connally.csv\"          \n[43] \"data-raw/2023_notes/r4-cotulla.csv\"           \n[44] \"data-raw/2023_notes/r4-dominguez.csv\"         \n[45] \"data-raw/2023_notes/r4-fort-stockton.csv\"     \n[46] \"data-raw/2023_notes/r4-garza-west.csv\"        \n[47] \"data-raw/2023_notes/r4-glossbrenner.csv\"      \n[48] \"data-raw/2023_notes/r4-lopez.csv\"             \n[49] \"data-raw/2023_notes/r4-lynaugh.csv\"           \n[50] \"data-raw/2023_notes/r4-mcconnell.csv\"         \n[51] \"data-raw/2023_notes/r4-sanchez.csv\"           \n[52] \"data-raw/2023_notes/r4-stevenson.csv\"         \n[53] \"data-raw/2023_notes/r4-torres.csv\"            \n[54] \"data-raw/2023_notes/r5-dalhart.csv\"           \n[55] \"data-raw/2023_notes/r5-daniel.csv\"            \n[56] \"data-raw/2023_notes/r5-formby.csv\"            \n[57] \"data-raw/2023_notes/r5-jordan.csv\"            \n[58] \"data-raw/2023_notes/r5-mechler.csv\"           \n[59] \"data-raw/2023_notes/r5-montford.csv\"          \n[60] \"data-raw/2023_notes/r5-roach.csv\"             \n[61] \"data-raw/2023_notes/r5-smith.csv\"             \n[62] \"data-raw/2023_notes/r5-wallace.csv\"           \n[63] \"data-raw/2023_notes/r6-crain.csv\"             \n[64] \"data-raw/2023_notes/r6-halbert.csv\"           \n[65] \"data-raw/2023_notes/r6-hamilton.csv\"          \n[66] \"data-raw/2023_notes/r6-havins.csv\"            \n[67] \"data-raw/2023_notes/r6-hilltop.csv\"           \n[68] \"data-raw/2023_notes/r6-hobby.csv\"             \n[69] \"data-raw/2023_notes/r6-hughes.csv\"            \n[70] \"data-raw/2023_notes/r6-luther.csv\"            \n[71] \"data-raw/2023_notes/r6-marlin.csv\"            \n[72] \"data-raw/2023_notes/r6-middleton.csv\"         \n[73] \"data-raw/2023_notes/r6-o-daniel.csv\"          \n[74] \"data-raw/2023_notes/r6-pack.csv\"              \n[75] \"data-raw/2023_notes/r6-robertson.csv\"         \n[76] \"data-raw/2023_notes/r6-san-saba.csv\"          \n[77] \"data-raw/2023_notes/r6-sayle.csv\"             \n[78] \"data-raw/2023_notes/r6-travis-county.csv\"     \n[79] \"data-raw/2023_notes/r6-woodman.csv\"           \n\n\nNow we put those files together into a single tibble.\n\n\nExpand this to see code\nnotes_raw &lt;- notes_list |&gt; \n  set_names(basename) |&gt;\n  map(\n  read_csv,\n  col_types = cols(.default = col_character()),\n  col_names = c(\"notes\")\n) |&gt; list_rbind(names_to = \"unit\") |&gt;\n  clean_names()\n\nnotes_raw |&gt; glimpse()\n\n\nRows: 268\nColumns: 4\n$ unit  &lt;chr&gt; \"r1-byrd.csv\", \"r1-byrd.csv\", \"r1-duncan.csv\", \"r1-duncan.csv\", …\n$ notes &lt;chr&gt; \"Name we code as Komomzy is a difficult signature to comprehend\"…\n$ x2    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ x3    &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\nWe have a little cleanup here for the unit and region. We drop the x columns, which are empty.\n\n\nExpand this to see code\nnotes_cleaned &lt;- notes_raw |&gt; \n  filter(!is.na(notes)) |&gt; \n  mutate(\n    region = str_extract(unit, pattern = \"(.*)-\", group = 1),\n    unit = str_extract(unit, pattern = \".*-(.*).csv\", group = 1)\n  ) |&gt; \n  select(unit, region, notes)\n\nnotes_cleaned |&gt; head()\n\n\n\n  \n\n\n\nWe’ll export this as both a csv and an rds because we may just put these into a spreadsheet for review.\n\n\nExpand this to see code\nnotes_cleaned |&gt; write_rds(\"data-processed/01-outdoor-notes.rds\")\nnotes_cleaned |&gt; write_csv(\"data-processed/01-outdoor-notes.csv\")",
    "crumbs": [
      "Cleaning",
      "Outdoor logs cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html",
    "href": "01-activation-cleaning.html",
    "title": "Activations cleaning",
    "section": "",
    "text": "These are the days that a unit was under ICS protocols like providing extra ice water and access to cooler areas during extreme heat events. This data was acquired by Lauren McGaughy of The Texas Newsroom through a public information request to the Texas Department of Criminal Justice.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#overview",
    "href": "01-activation-cleaning.html#overview",
    "title": "Activations cleaning",
    "section": "",
    "text": "These are the days that a unit was under ICS protocols like providing extra ice water and access to cooler areas during extreme heat events. This data was acquired by Lauren McGaughy of The Texas Newsroom through a public information request to the Texas Department of Criminal Justice.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#possible-update",
    "href": "01-activation-cleaning.html#possible-update",
    "title": "Activations cleaning",
    "section": "Possible update",
    "text": "Possible update\nThis pulls from a CSV of unknown provenance. We do have the original Excel spreadsheet now but have not yet updated the list to use this. FWIW, at a glance they look the same.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#set-up",
    "href": "01-activation-cleaning.html#set-up",
    "title": "Activations cleaning",
    "section": "Set up",
    "text": "Set up\nYou know the drill!\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(data.table)",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#download-activation-log-directly",
    "href": "01-activation-cleaning.html#download-activation-log-directly",
    "title": "Activations cleaning",
    "section": "Download activation log directly",
    "text": "Download activation log directly\nThis copy was downloaded from our MIG = TDCJ Basic Information on the “Activation” tab.\nIt is missing a 9/5/2023 record that is in unit-activation-log.csv. WE AREN’T USING THIS. It’s just to check.\n\ndownload.file(\n  \"https://docs.google.com/spreadsheets/d/e/2PACX-1vRebFs9O2i0HoygXTtIvuzdVTmyrjX3MeUorU9d4fpYkGX7Bb026OradFQ1MMk2ltcGnyILih6ow4F4/pub?gid=1968960884&single=true&output=csv\",\n  \"data-raw/unit-activation-direct.csv\"\n      )\n\nImport direct log\n\nraw_activation_direct &lt;- read_csv(\"data-raw/unit-activation-direct.csv\") |&gt; \n  clean_names()\n\nRows: 102 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Initial Date of Extreme Temperature, ICS Implementation Date, Count...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nraw_activation_direct\n\n\n  \n\n\n\nThis original files lacks on activation that is in our already-downloaded unit-activation-log.csv. We’ll continue to use the -log version while we track down an original.",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#import-activation-log",
    "href": "01-activation-cleaning.html#import-activation-log",
    "title": "Activations cleaning",
    "section": "Import activation log",
    "text": "Import activation log\nLet’s add our activation log file in. (This file was downloaded directly into the repo, but we lost track of the original file. It has an additional record or other copies don’t have.)\n\nraw_activation_log &lt;- read_csv(\"data-raw/unit-activation-log.csv\", skip = 1) |&gt; \n  clean_names()\n\nNew names:\nRows: 102 Columns: 10\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): Initial Date of Extreme Temperature, ICS Implementation Date, Count... dbl\n(2): Time...6, Time...8 lgl (2): ...9, ...10\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `Date` -&gt; `Date...5`\n• `Time` -&gt; `Time...6`\n• `Date` -&gt; `Date...7`\n• `Time` -&gt; `Time...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n\nraw_activation_log",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#clean-column-names",
    "href": "01-activation-cleaning.html#clean-column-names",
    "title": "Activations cleaning",
    "section": "Clean column names",
    "text": "Clean column names\nOur column names didn’t translate perfectly, so let’s work on cleaning them up and remove the last two unnecessary columns.\n\nclean_activation_log &lt;- raw_activation_log |&gt; \n  select(\n    initial_extreme_temp = initial_date_of_extreme_temperature,\n    initiation_date = ics_implementation_date,\n    county,\n    unit = unit_affected,\n    activation_date = date_5, \n    activation_time = time_6,\n    deactivation_date = date_7,\n    deactivation_time = time_8 \n  )",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#change-date-columns",
    "href": "01-activation-cleaning.html#change-date-columns",
    "title": "Activations cleaning",
    "section": "Change date columns",
    "text": "Change date columns\nWe need our dates to be date columns instead of character columns. Let’s adjust.\n\nactivation_log_dates &lt;- clean_activation_log |&gt; \n  mutate(\n    initial_extreme_temp = mdy(initial_extreme_temp),\n    initiation_date = mdy(initiation_date),\n    activation_date = mdy(activation_date),\n    deactivation_date = mdy(deactivation_date)\n  ) \n\nactivation_log_dates",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#document-active-days",
    "href": "01-activation-cleaning.html#document-active-days",
    "title": "Activations cleaning",
    "section": "Document active days",
    "text": "Document active days\nWe need to turn our initial_extreme_temp column, implementation_date column and our deactivation_date column into date ranges.\n\ndirty_actives &lt;- activation_log_dates |&gt; \n  group_by(unit) |&gt; \n  mutate(occurence = dense_rank(initial_extreme_temp)) |&gt; \n  mutate(start = as.Date(activation_date), end = as.Date(deactivation_date)) |&gt; \n  mutate(\n    activated = map2(start, end, ~seq(from = .x, to = .y, by = \"day\"))\n    ) |&gt; \n  unnest(activated) \n\ndirty_actives",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#clean-up-our-active-dates",
    "href": "01-activation-cleaning.html#clean-up-our-active-dates",
    "title": "Activations cleaning",
    "section": "Clean up our active dates",
    "text": "Clean up our active dates\nWe need to create a status column, so let’s remove everything other than unit and active dates.\n\nclean_actives &lt;- dirty_actives |&gt; \n  select(c(unit, activated)) |&gt; \n  mutate(protocol_active = T) |&gt; \n  mutate(date = activated) |&gt; \n  select(!activated)\n\nclean_actives |&gt; \n  filter(unit == \"Byrd\")",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-activation-cleaning.html#export",
    "href": "01-activation-cleaning.html#export",
    "title": "Activations cleaning",
    "section": "Export",
    "text": "Export\n\nclean_actives |&gt; write_rds(\"data-processed/01-activation-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Activations cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html",
    "href": "01-indoor-cleaning.html",
    "title": "Indoor logs cleaning",
    "section": "",
    "text": "This data is the record indoor temperatures at each unit from April to September. This information was acquired by Lauren McGaughy at KUT through a public information request to the Texas Department of Criminal Justice.\nThe original PDFs were transcribed into data using Google Pinpoint, resulting in the csv files found in data-raw. The original documents are found in data-original.",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#goals-of-this-notebook",
    "href": "01-indoor-cleaning.html#goals-of-this-notebook",
    "title": "Indoor logs cleaning",
    "section": "Goals of this notebook",
    "text": "Goals of this notebook\nWhat we’ll do to prepare the data:\n\nDownload the data\nImport it into our notebook\nClean up data types and columns\nExport data into our next notebook",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#setup",
    "href": "01-indoor-cleaning.html#setup",
    "title": "Indoor logs cleaning",
    "section": "Setup",
    "text": "Setup\n\nlibrary(tidyverse)\nlibrary(janitor)",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#downloading-data",
    "href": "01-indoor-cleaning.html#downloading-data",
    "title": "Indoor logs cleaning",
    "section": "Downloading data",
    "text": "Downloading data\nThis data comes from a public information request to the Texas Department of Criminal Justice. We have two seperate sheets with our 30-day months and our 31-day months, so we’ll read each of those in separately.\n\nindoor_temps &lt;- read_csv(\"data-raw/Indoor-30days-edited.csv\")\n\nRows: 201 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): File Name, Unit, Validation Link\ndbl (30): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nindoor_temps |&gt; glimpse()\n\nRows: 201\nColumns: 33\n$ `File Name`       &lt;chr&gt; \"SB1R56 - April 2023_1.pdf\", \"SB1R56 - April 2023_1.…\n$ Unit              &lt;chr&gt; \"Allred\", \"Beto\", \"Boyd\", \"Bradshaw\", \"Briscoe\", \"By…\n$ `1`               &lt;dbl&gt; 75.5, 73.4, 76.6, 80.1, 84.0, 76.9, 77.5, 71.4, 76.4…\n$ `2`               &lt;dbl&gt; 79.0, 73.9, 76.5, 82.1, 88.0, 74.0, 79.1, 72.8, 76.1…\n$ `3`               &lt;dbl&gt; 78.1, 78.6, 81.3, 86.3, 91.2, 81.1, 78.6, 74.7, 80.8…\n$ `4`               &lt;dbl&gt; 80.5, 79.4, 82.5, 80.5, 88.0, 81.2, 75.6, 73.3, 82.2…\n$ `5`               &lt;dbl&gt; 74.3, 74.2, 77.0, 73.8, 76.4, 75.4, 81.5, 71.4, 73.8…\n$ `6`               &lt;dbl&gt; 74.2, 69.9, 68.7, 68.5, 67.7, 72.7, 73.2, 72.3, 66.3…\n$ `7`               &lt;dbl&gt; 76.1, 69.6, 72.1, 70.2, 69.1, 65.1, 67.0, 73.3, 76.4…\n$ `8`               &lt;dbl&gt; 77.9, 71.3, 74.2, 78.4, 73.1, 72.3, 67.5, 72.5, 78.5…\n$ `9`               &lt;dbl&gt; 79.0, 73.2, 74.2, 73.0, 76.5, 69.8, 72.0, 73.2, 75.7…\n$ `10`              &lt;dbl&gt; 78.4, 71.6, 73.1, 79.1, 75.6, 75.8, 72.2, 73.3, 72.1…\n$ `11`              &lt;dbl&gt; 78.9, 72.1, 78.8, 78.5, 80.6, 76.9, 71.1, 74.4, 77.1…\n$ `12`              &lt;dbl&gt; 78.3, 72.5, 77.0, 78.3, 80.0, 73.0, 72.4, 75.0, 77.2…\n$ `13`              &lt;dbl&gt; 79.7, 72.6, 76.5, 75.8, 81.0, 76.0, 74.7, 74.9, 77.2…\n$ `14`              &lt;dbl&gt; 80.4, 73.4, 77.2, 85.3, 88.6, 74.0, 70.9, 74.4, 75.8…\n$ `15`              &lt;dbl&gt; 77.1, 79.2, 80.9, 84.9, 90.6, 73.1, 76.4, 69.8, 79.9…\n$ `16`              &lt;dbl&gt; 76.3, 70.9, 73.3, 82.0, 88.2, 72.0, 72.6, 67.0, 71.0…\n$ `17`              &lt;dbl&gt; 77.8, 70.8, 76.1, 82.2, 79.4, 77.2, 73.1, 69.3, 77.0…\n$ `18`              &lt;dbl&gt; 82.2, 71.9, 72.3, 73.7, 83.0, 73.0, 76.5, 77.6, 72.6…\n$ `19`              &lt;dbl&gt; 81.3, 74.9, 77.8, 82.1, 80.0, 72.5, 75.6, 72.5, 76.5…\n$ `20`              &lt;dbl&gt; 76.9, 75.8, 78.9, 80.1, 85.0, 76.0, 75.3, 69.6, 79.8…\n$ `21`              &lt;dbl&gt; 74.4, 75.9, 74.8, 78.1, 82.5, 73.0, 75.5, 74.1, 76.6…\n$ `22`              &lt;dbl&gt; 75.4, 72.3, 75.6, 80.7, 81.0, 71.0, 74.2, 73.5, 73.8…\n$ `23`              &lt;dbl&gt; 77.0, 65.3, 65.0, 72.6, 67.9, 76.8, 66.1, 61.0, 76.4…\n$ `24`              &lt;dbl&gt; 71.6, 69.1, 73.0, 78.2, 72.0, 76.0, 67.3, 61.7, 76.8…\n$ `25`              &lt;dbl&gt; 77.1, 70.1, 73.3, 79.7, 77.0, 72.6, 72.9, 64.7, 75.7…\n$ `26`              &lt;dbl&gt; 76.3, 71.2, 73.1, 74.0, 83.6, 71.5, 77.1, 62.2, 76.7…\n$ `27`              &lt;dbl&gt; 76.9, 71.4, 73.5, 74.8, 83.2, 67.9, 70.3, 61.9, 78.5…\n$ `28`              &lt;dbl&gt; 77.2, 71.6, 78.6, 80.5, 86.3, 74.5, 78.9, 60.7, 77.4…\n$ `29`              &lt;dbl&gt; 71.8, 67.7, 73.0, 76.0, 77.1, 67.1, 68.8, 61.9, 76.3…\n$ `30`              &lt;dbl&gt; 77.0, 71.7, 81.3, 79.1, 84.3, 69.6, 74.9, 63.9, 78.5…\n$ `Validation Link` &lt;chr&gt; \"https://journaliststudio.google.com/pinpoint-extrac…\n\n\n\nother_indoor_temps &lt;- read_csv(\"data-raw/Indoor-31days-edited.csv\")\n\nRows: 203 Columns: 34\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): File Name, Unit, Validation Link\ndbl (31): 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nother_indoor_temps |&gt; glimpse()\n\nRows: 203\nColumns: 34\n$ `File Name`       &lt;chr&gt; \"SB1R56 - August 2023_1.pdf\", \"SB1R56 - August 2023_…\n$ Unit              &lt;chr&gt; \"Allred\", \"Beto\", \"Boyd\", \"Bradshaw\", \"Briscoe\", \"By…\n$ `1`               &lt;dbl&gt; 97.7, 91.9, 95.1, 98.2, 96.0, 93.8, 84.3, 89.3, 97.0…\n$ `2`               &lt;dbl&gt; 99.7, 94.4, 97.0, 97.7, 94.1, 94.2, 87.4, 89.0, 93.8…\n$ `3`               &lt;dbl&gt; 95.6, 92.8, 97.4, 98.4, 94.9, 93.4, 85.6, 88.8, 92.7…\n$ `4`               &lt;dbl&gt; 97.6, 91.1, 96.0, 97.6, 91.7, 93.7, 83.0, 88.0, 94.0…\n$ `5`               &lt;dbl&gt; 100.5, 93.8, 96.4, 97.8, 95.1, 93.9, 84.2, 89.3, 92.…\n$ `6`               &lt;dbl&gt; 97.1, 94.7, 94.9, 97.4, 95.3, 90.1, 87.2, 85.1, 93.7…\n$ `7`               &lt;dbl&gt; 93.8, 92.3, 94.5, 94.5, 95.5, 95.2, 84.9, 82.5, 93.5…\n$ `8`               &lt;dbl&gt; 92.3, 93.1, 95.3, 95.3, 95.4, 94.0, 89.4, 83.5, 93.1…\n$ `9`               &lt;dbl&gt; 95.8, 91.3, 94.4, 92.8, 98.1, 92.5, 89.9, 83.9, 94.5…\n$ `10`              &lt;dbl&gt; 93.0, 93.9, 95.3, 95.9, 97.5, 92.9, 91.7, 84.0, 96.2…\n$ `11`              &lt;dbl&gt; 99.1, 92.7, 93.2, 95.4, 96.1, 92.8, 90.2, 87.3, 94.2…\n$ `12`              &lt;dbl&gt; 100.1, 93.7, 95.4, 96.3, 97.2, 93.0, 88.6, 87.2, 94.…\n$ `13`              &lt;dbl&gt; 97.5, 92.4, 95.6, 95.0, 98.1, 92.6, 86.2, 86.0, 93.8…\n$ `14`              &lt;dbl&gt; 83.3, 93.2, 95.0, 94.2, 96.3, 92.3, 88.6, 81.4, 94.9…\n$ `15`              &lt;dbl&gt; 85.9, 89.0, 93.7, 92.1, 94.6, 91.2, 87.9, 81.0, 91.1…\n$ `16`              &lt;dbl&gt; 87.5, 86.8, 91.1, 89.7, 94.5, 90.2, 89.5, 83.5, 88.8…\n$ `17`              &lt;dbl&gt; 94.3, 90.3, 94.8, 94.6, 96.7, 90.4, 90.5, 87.9, 89.6…\n$ `18`              &lt;dbl&gt; 97.9, 93.2, 98.0, 89.7, 94.1, 91.8, 90.1, 82.0, 95.1…\n$ `19`              &lt;dbl&gt; 95.0, 91.9, 94.9, 95.2, 95.0, 91.9, 89.4, 85.7, 96.5…\n$ `20`              &lt;dbl&gt; 96.3, 94.9, 97.6, 96.1, 98.2, 92.9, 88.7, 85.9, 99.1…\n$ `21`              &lt;dbl&gt; 93.4, 94.2, 98.2, 96.1, 93.7, 93.3, 87.2, 87.0, 96.6…\n$ `22`              &lt;dbl&gt; 95.7, 92.3, 96.1, 98.1, 85.6, 92.1, 84.4, 89.9, 93.6…\n$ `23`              &lt;dbl&gt; 97.8, 94.3, 96.2, 98.6, 89.5, 91.4, 89.1, 89.4, 94.8…\n$ `24`              &lt;dbl&gt; 99.1, 95.7, 98.9, 98.5, 92.2, 94.2, 91.7, 86.0, 99.5…\n$ `25`              &lt;dbl&gt; 95.6, 96.2, 98.0, 98.9, 96.2, 92.2, 90.9, 87.8, 95.6…\n$ `26`              &lt;dbl&gt; 99.3, 96.3, 98.7, 98.6, 93.2, 95.4, 90.7, 84.8, 99.1…\n$ `27`              &lt;dbl&gt; 97.3, 96.5, 99.6, 98.1, 96.3, 93.2, 85.7, 83.8, 101.…\n$ `28`              &lt;dbl&gt; 88.0, 86.3, 89.9, 84.3, 97.1, 86.2, 86.9, 84.0, 86.2…\n$ `29`              &lt;dbl&gt; 87.7, 87.1, 90.7, 89.3, 93.8, 88.2, 88.9, 81.7, 88.6…\n$ `30`              &lt;dbl&gt; 87.8, 87.3, 91.8, 89.1, 95.4, 86.5, 88.8, 81.6, 88.8…\n$ `31`              &lt;dbl&gt; 88.6, 87.6, 91.8, 91.2, 97.2, 81.9, 87.9, 81.0, 88.0…\n$ `Validation Link` &lt;chr&gt; \"https://journaliststudio.google.com/pinpoint-extrac…",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#pivot-longer",
    "href": "01-indoor-cleaning.html#pivot-longer",
    "title": "Indoor logs cleaning",
    "section": "Pivot Longer",
    "text": "Pivot Longer\nRight now each day is a column, let’s change it so days are all under one column and clean up the names of our columns.\nWe’ll do this individually before we join the columns so we don’t have to remove the 31st rows for months with only 30 days.\n\nother_temps_raw &lt;- other_indoor_temps |&gt; \n  pivot_longer(\n    cols = \"1\":\"31\",\n    names_to = \"day\",\n    values_to = \"temperature\"\n  )\n\nother_temps_raw\n\n\n  \n\n\n\n\nindoor_temps_raw &lt;- indoor_temps |&gt; \n  pivot_longer(\n    cols = \"1\":\"30\",\n    names_to = \"day\",\n    values_to = \"temperature\"\n  )\n\nindoor_temps_raw",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#combine-data-frames",
    "href": "01-indoor-cleaning.html#combine-data-frames",
    "title": "Indoor logs cleaning",
    "section": "Combine data frames",
    "text": "Combine data frames\nLet’s combine both of our datasets to make a single table.\n\nindoor_temps_combined &lt;- bind_rows(other_temps_raw, indoor_temps_raw)\n\nindoor_temps_combined",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#clean-names",
    "href": "01-indoor-cleaning.html#clean-names",
    "title": "Indoor logs cleaning",
    "section": "Clean names",
    "text": "Clean names\nOne column we’ll have to convert is our file_name column into a month/year column. In order to do that, we first have to create a value with all of the characters we want to remove.\n\nword_list &lt;- c(\"SB1R56 - |_1.pdf|_2.pdf\")\n\nNow that we’ve done that, let’s clean names and make our new column.\n\ndaily_indoor &lt;- indoor_temps_combined|&gt; \n  clean_names() |&gt; \n  mutate(\n    month_year = str_remove_all(file_name, word_list) \n  ) \n\ndaily_indoor",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#add-date-column",
    "href": "01-indoor-cleaning.html#add-date-column",
    "title": "Indoor logs cleaning",
    "section": "Add date column",
    "text": "Add date column\nWe’ll need to clean up the dates so they’re standardized and readable. We’ll combine our month/year column with the day column to put through the lubridate function and, while we’re at it, remove unnecessary columns.\n\ndaily_indoor_dated &lt;- daily_indoor |&gt; \n    distinct() |&gt; \n    unite(\n    col = \"temp_date\", \n    day, month_year,\n    sep = \" \"\n    ) |&gt; \n  mutate(\n    date = dmy(temp_date)\n  ) |&gt; \n  select(!c(file_name, validation_link, temp_date)) |&gt; \n  mutate(unit_temp = temperature) |&gt; \n  select(!temperature) \n\n\ndaily_indoor_dated |&gt; glimpse()\n\nRows: 12,323\nColumns: 3\n$ unit      &lt;chr&gt; \"Allred\", \"Allred\", \"Allred\", \"Allred\", \"Allred\", \"Allred\", …\n$ date      &lt;date&gt; 2023-08-01, 2023-08-02, 2023-08-03, 2023-08-04, 2023-08-05,…\n$ unit_temp &lt;dbl&gt; 97.7, 99.7, 95.6, 97.6, 100.5, 97.1, 93.8, 92.3, 95.8, 93.0,…",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#clean-some-unit-names",
    "href": "01-indoor-cleaning.html#clean-some-unit-names",
    "title": "Indoor logs cleaning",
    "section": "Clean some unit names",
    "text": "Clean some unit names\nIt looks like the Ramsey unit was recorded with slightly different names in each csv. Let’s combine both of those columns so all of the data is together and then remove any leftover NA’s.\nMcconnell 183\nO’Daniel 91\nWainwright 183\n\ndaily_indoor_named &lt;- daily_indoor_dated |&gt; \n  mutate(\n    unit = case_match(\n      unit,\n      \"Mcconnell\" ~ \"McConnell\",\n      \"O’Daniel\" ~ \"O'Daniel\",\n      .default = unit\n    )\n  )\n\nWe don’t have recordings for every unit. These are the units we do not have readings for: Baten, Bell, Bridgeport, Coleman, Cotulla, Diboll, Duncan, East Texas, Estes, Fort Stockton, Garza East, Glossbrenner, Halbert, Hamilton, Havins, Henley, Hodge, Hospital Galveston, Kegans, Kyle, LeBlanc, Lindsey, Marlin, Mechler, Moore, B., Ney, Pack, San Saba, Sayle, Scott, Skyview, Travis, Willacy.",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-indoor-cleaning.html#export-the-data",
    "href": "01-indoor-cleaning.html#export-the-data",
    "title": "Indoor logs cleaning",
    "section": "Export the data",
    "text": "Export the data\nWe did it! Now let’s export our data and place it in our processed data folder.\n\ndaily_indoor_named |&gt; write_rds(\"data-processed/01-indoor-cleaned.rds\")",
    "crumbs": [
      "Cleaning",
      "Indoor logs cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html",
    "href": "01-station-protocols.html",
    "title": "Stations cleaning",
    "section": "",
    "text": "Here we enhance our weather station data to include prison protocol data. We do this both for our hourly readings and our daily summaries.\nWill decide later if we add the prison info. That may need to be in more specific analysis.\nlibrary(tidyverse)\nlibrary(janitor)",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html#flag-function",
    "href": "01-station-protocols.html#flag-function",
    "title": "Stations cleaning",
    "section": "Flag function",
    "text": "Flag function\nHere we want a function that will create the flags, once fed a tmp and heat index.\n\nset_protocol_flag &lt;- function(df, tmp_name, hi_name) {\n    df |&gt; mutate(\n    tmp_flag = (case_when(\n      tmp_name &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi_name &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == TRUE | hi_flag == TRUE ~ TRUE,\n      .default = FALSE)\n    )\n}",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html#hourly-data",
    "href": "01-station-protocols.html#hourly-data",
    "title": "Stations cleaning",
    "section": "Hourly data",
    "text": "Hourly data\n\nImport hourly data\n\nstations_hourly &lt;- read_rds(\"data-processed/weather-logs/01-station-hourly-readings.rds\")\n\n\n\nAdd protocol flags\n\nstations_hourly_protocol &lt;- stations_hourly |&gt; \n   mutate(\n    tmp_flag = (case_when(\n      tmp &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == TRUE | hi_flag == TRUE ~ TRUE,\n      .default = FALSE)\n    )\n\n# check results\nstations_hourly_protocol |&gt;\n  select(-name) |&gt; \n  filter(tmp &gt; 100) |&gt; \n  slice_sample(n = 10)\n\n\n  \n\n\n\n\n\nExport hourly\n\nstations_hourly_protocol |&gt; \n  write_rds(\"data-processed/01-station-hourly-protocols.rds\")",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01-station-protocols.html#daily-summarized-data",
    "href": "01-station-protocols.html#daily-summarized-data",
    "title": "Stations cleaning",
    "section": "Daily summarized data",
    "text": "Daily summarized data\n\nImport daily summaries\n\nstations_daily &lt;- read_rds(\"data-processed/weather-logs/01-station-daily-summary.rds\")\n\nstations_daily |&gt; glimpse()\n\nRows: 17,453\nColumns: 5\n$ station_id &lt;chr&gt; \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KABI\", \"KA…\n$ name       &lt;chr&gt; \"ABILENE REGIONAL AIRPORT, TX US\", \"ABILENE REGIONAL AIRPOR…\n$ date       &lt;date&gt; 2023-01-01, 2023-01-02, 2023-01-03, 2023-01-04, 2023-01-05…\n$ tmp_high   &lt;dbl&gt; 78, 76, 60, 64, 63, 77, 63, 63, 77, 83, 83, 60, 64, 67, 67,…\n$ hi_high    &lt;dbl&gt; 76.7, 74.8, 56.9, 61.0, 60.2, 76.1, 62.1, 60.5, 75.3, 80.4,…\n\n\n\n\nAdd daily protocols\n\nstations_daily_protocol &lt;- stations_daily |&gt; \n  mutate(\n    tmp_flag = (case_when(\n      tmp_high &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi_high &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == \"TRUE\" | hi_flag == \"TRUE\" ~ TRUE,\n      .default = FALSE)\n  )\n\n# check results\nstations_daily_protocol |&gt; \n  select(-name) |&gt; \n  filter(tmp_high &gt; 100) |&gt; \n  slice_sample(n = 10)\n\n\n  \n\n\n\n\n\nExport daily\n\nstations_daily_protocol |&gt; write_rds(\"data-processed/01-station-daily-protocols.rds\")",
    "crumbs": [
      "Cleaning",
      "Stations cleaning"
    ]
  },
  {
    "objectID": "01.1-indoor-combining.html",
    "href": "01.1-indoor-combining.html",
    "title": "Combine indoor logs",
    "section": "",
    "text": "I’ve created station files that already have the protocol flags. This notebook could bring those in, but I’m not sure it is any more clear. I might trash those protocol data files."
  },
  {
    "objectID": "01.1-indoor-combining.html#to-do",
    "href": "01.1-indoor-combining.html#to-do",
    "title": "Combine indoor logs",
    "section": "",
    "text": "I’ve created station files that already have the protocol flags. This notebook could bring those in, but I’m not sure it is any more clear. I might trash those protocol data files."
  },
  {
    "objectID": "01.1-indoor-combining.html#overview",
    "href": "01.1-indoor-combining.html#overview",
    "title": "Combine indoor logs",
    "section": "Overview",
    "text": "Overview\nHere we bring together our indoor readings together with our station readings, and then create our protocol flags.\nWe’ll compare two ways to handle the station data:\n\nFind the 3 p.m. hours to match when the prison is supposed to record their records.\nThe daily summaries that have the hottest temp/heat index."
  },
  {
    "objectID": "01.1-indoor-combining.html#load-our-libraries",
    "href": "01.1-indoor-combining.html#load-our-libraries",
    "title": "Combine indoor logs",
    "section": "Load our libraries",
    "text": "Load our libraries\nlibrary(data.table)\n\nlibrary(tidyverse)\nlibrary(janitor)"
  },
  {
    "objectID": "01.1-indoor-combining.html#downloading-weather-logs-files",
    "href": "01.1-indoor-combining.html#downloading-weather-logs-files",
    "title": "Combine indoor logs",
    "section": "Downloading weather-logs files",
    "text": "Downloading weather-logs files\nWe’ll download a copy of our cleaned weather-logs into this repo so we don’t have to rely on the original.\nThe station files come from the weather-logs repo. That repo is currently private, so we’ll manually download the files into data-processed/weather-logs."
  },
  {
    "objectID": "01.1-indoor-combining.html#import-our-files",
    "href": "01.1-indoor-combining.html#import-our-files",
    "title": "Combine indoor logs",
    "section": "Import our files",
    "text": "Import our files\n\nstations_daily &lt;- read_rds(\"data-processed/weather-logs/01-station-daily-summary.rds\")\nstations_hourly &lt;- read_rds(\"data-processed/weather-logs/01-station-hourly-readings.rds\")\nheat_warnings &lt;- read_rds(\"data-processed/weather-logs/01-heat-warning-cleaned.rds\")\nactivation &lt;- read_rds(\"data-processed/01-activation-cleaned.rds\")\nindoor &lt;-  read_rds(\"data-processed/01-indoor-cleaned.rds\")\nunit_info&lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\")"
  },
  {
    "objectID": "01.1-indoor-combining.html#combine-data-files",
    "href": "01.1-indoor-combining.html#combine-data-files",
    "title": "Combine indoor logs",
    "section": "Combine data files",
    "text": "Combine data files\nNow that we’ve brought all of the files into our environment, it’s time to put them all together!\nThe process we’ll use is this:\n\nstart with indoor readings\njoin with unit-info to get the station ids\njoin with the stations to get that information\ncreate our protocol flags\nexport\n\nFirst we’ll combine our unit temperature logs with our activation status. We’ll also take this time to fill in our inactives because our original file only denoted activation."
  },
  {
    "objectID": "01.1-indoor-combining.html#indoor-unit-info",
    "href": "01.1-indoor-combining.html#indoor-unit-info",
    "title": "Combine indoor logs",
    "section": "Indoor + unit info",
    "text": "Indoor + unit info\n\nindoor_unit &lt;- indoor |&gt; \n  left_join(unit_info, by = join_by(unit == unit_name)) |&gt; \n  select(unit, date, unit_temp, nws_id, county) |&gt; \n  arrange(unit, date)\n\n# sample of data\nindoor_unit |&gt; slice_sample(n = 10)\n\n\n  \n\n\n# total row count\nindoor_unit |&gt; nrow()\n\n[1] 12323\n\n# missing join count\nindoor_unit |&gt; filter(is.na(nws_id))"
  },
  {
    "objectID": "01.1-indoor-combining.html#add-stations",
    "href": "01.1-indoor-combining.html#add-stations",
    "title": "Combine indoor logs",
    "section": "Add stations",
    "text": "Add stations\nI did check and the daily summaries have more possible matches than pulling 3p-only values. Here we join by both the station id and date.\n\nindoor_stations &lt;- indoor_unit |&gt; \n  left_join(stations_daily, by = join_by(nws_id == station_id, date))\n\nindoor_stations |&gt; nrow()\n\n[1] 12323\n\nindoor_stations |&gt; head()\n\n\n  \n\n\n\nUnfortunately there are There are 1580 indoor records (out of 12323) without a matching station record.\n\nindoor_stations_peek &lt;- indoor_stations |&gt; \n  group_by(unit, nws_id) |&gt;\n  summarize(\n    total_cnt = n(),\n    na_cnt = sum(is.na(name))\n  ) |&gt; \n  ungroup() |&gt; \n  adorn_totals(\"row\") |&gt; \n  tibble()\n\n`summarise()` has grouped output by 'unit'. You can override using the\n`.groups` argument.\n\nindoor_stations_peek\n\n\n  \n\n\n\nIn some cases there are just missing days of readings, which could be for any number of reasons, like the observation station was down.\nBut there are a number of prison units where we could not find a NWS weather data within 40 miles. These fit into that category.\n\nindoor_stations_peek |&gt; \n  filter(na_cnt &gt; 0,\n         total_cnt == na_cnt) |&gt; \n  arrange(unit)\n\n\n  \n\n\n\n\nRemove non-matches\nWe’ll remove indoor records where we don’t have a matching weather temperature since we can’t do any comparison here. (May decide later we need this, but later code might have to change to deal with missing variables.)\n\nindoor_clipped &lt;- indoor_stations |&gt; \n  filter(!is.na(name))\n\nindoor_clipped |&gt; nrow()\n\n[1] 10743"
  },
  {
    "objectID": "01.1-indoor-combining.html#create-a-flag",
    "href": "01.1-indoor-combining.html#create-a-flag",
    "title": "Combine indoor logs",
    "section": "Create a flag",
    "text": "Create a flag\nWe know that one form of criteria for our heat protocol is if the feels like is greater than or equal to 113 for three days. Let’s create a flag for days greater than or equal to 113 and then count the number of consecutive days.\n\nindoor_flags &lt;- indoor_clipped |&gt; \n  mutate(\n    tmp_flag = (case_when(\n      tmp_high &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    hi_flag = (case_when(\n      hi_high &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    protocol_flag = case_when(\n      tmp_flag == \"TRUE\" | hi_flag == \"TRUE\" ~ TRUE,\n      .default = FALSE)\n    )\n  \nindoor_flags"
  },
  {
    "objectID": "01.1-indoor-combining.html#look-for-runs",
    "href": "01.1-indoor-combining.html#look-for-runs",
    "title": "Combine indoor logs",
    "section": "Look for runs",
    "text": "Look for runs\nCreate if either are true column Add another column that showsd if true is along with active\n\nindoor_protocol &lt;- indoor_flags |&gt; \n  group_by(unit) |&gt; \n  mutate(\n    protocol_run = accumulate(protocol_flag, ~if_else(.y, .x + 1, 0))\n  ) |&gt; \n  mutate(\n    protocol_met = case_when(\n    protocol_run &gt;= 3 ~ TRUE,\n    .default = FALSE\n  )) |&gt; \n  ungroup()\n\nindoor_protocol |&gt; \n  select(unit, date, starts_with(\"protocol\")) |&gt; \n  # skipping lines to get to true values\n  slice(80:100)"
  },
  {
    "objectID": "01.1-indoor-combining.html#add-activation-days",
    "href": "01.1-indoor-combining.html#add-activation-days",
    "title": "Combine indoor logs",
    "section": "Add activation days",
    "text": "Add activation days\nNow we’ll add a column that shows which prisons were actively under heat protocols.\n\nindoor_active &lt;- indoor_protocol |&gt; \n  left_join(activation) |&gt; \n  mutate(protocol_active = if_else(is.na(protocol_active), F, T),\n         protocol_fail = case_when(\n           protocol_met == T & protocol_active == F ~ T,\n           .default = F\n         ))\n\nJoining with `by = join_by(unit, date)`\n\nindoor_active |&gt; filter(protocol_met == T)"
  },
  {
    "objectID": "01.1-indoor-combining.html#consider-heat-warnings",
    "href": "01.1-indoor-combining.html#consider-heat-warnings",
    "title": "Combine indoor logs",
    "section": "Consider heat warnings",
    "text": "Consider heat warnings\nThere are only six cases where we’ve found a heat warning in effect. This could be because we don’t have a good match on these warnings unless it is set for the whole county.\nWe won’t save this join since it is of little use.\n\n# add later\nindoor_active |&gt;\n  left_join(heat_warnings, by = join_by(nws_id == station_code, date)) |&gt; \n  filter(ehw_active == T)"
  },
  {
    "objectID": "01.1-indoor-combining.html#export",
    "href": "01.1-indoor-combining.html#export",
    "title": "Combine indoor logs",
    "section": "Export",
    "text": "Export\nWe’ve put everything together! Now let’s export it into data-processed.\n\nindoor_protocol |&gt;\n  write_rds(\"data-processed/02-indoor-combined.rds\")"
  },
  {
    "objectID": "02-outdoor-reliability.html",
    "href": "02-outdoor-reliability.html",
    "title": "Outdoor log reliability",
    "section": "",
    "text": "Which days met protocol measures based on prison readings?\n\nHow does that compare to actual protocols that were place?\n\nHow do log temperature readings compare to the nearest weather station?\n\ndiff in temp\ndiff in heat index\n\n\nAnswering protocol questions will be tricky because we only have that one week of data and it takes three days of meeting the protocol to enact it.",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#reliability-questions",
    "href": "02-outdoor-reliability.html#reliability-questions",
    "title": "Outdoor log reliability",
    "section": "",
    "text": "Which days met protocol measures based on prison readings?\n\nHow does that compare to actual protocols that were place?\n\nHow do log temperature readings compare to the nearest weather station?\n\ndiff in temp\ndiff in heat index\n\n\nAnswering protocol questions will be tricky because we only have that one week of data and it takes three days of meeting the protocol to enact it.",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#setup-import-combine",
    "href": "02-outdoor-reliability.html#setup-import-combine",
    "title": "Outdoor log reliability",
    "section": "Setup, Import & Combine",
    "text": "Setup, Import & Combine\n\n\nExpand this to see code\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(scales)\nlibrary(DT)\n\n\nWe’re bringing in:\n\nOur coded and cleaned outdoor log data\nActivation records\nHourly weather readings\nPrison unit information\n\n\n\nExpand this to see code\nlogs_all &lt;- read_rds(\"data-processed/01-outdoor-cleaned.rds\")\nactivations &lt;- read_rds(\"data-processed/01-activation-cleaned.rds\")\nhourly &lt;- read_rds(\"data-processed/01-station-hourly-protocols.rds\")\nunits &lt;- read_rds(\"data-processed/01-unit-info-cleaned.rds\")\n\n\n\nClip log data\nWe’ll remove July 24th so we have the last seven days of July, 2023.\n\n\nExpand this to see code\nlogs &lt;- logs_all |&gt; filter(date != \"2023-07-24\")\n\nlogs |&gt; count(date) |&gt; adorn_totals() |&gt; tibble()\n\n\n\n  \n\n\n\n\n\nCombining files\nHere we bring in the unit info and hourly logs.\n\n\nExpand this to see code\n# getting station from units\nlogs_expanded &lt;- logs |&gt; \n  left_join(units, by = join_by(unit == unit_name, region)) |&gt; \n  # removing some unneeded cols\n  select(!c(unit_code, type, county, nws))\n\n# joining to get weather info\nlogs_nws &lt;- logs_expanded |&gt; \n  left_join(hourly, by = join_by(nws_id == station_id, date == date, hour == hr))\n\nlogs_expanded |&gt; filter(str_detect(unit, \"Jordan\"))\n\n\n\n  \n\n\n\n\n\nContext on matches\nHere we find the percentage of records with no NWS data.\n\n\nExpand this to see code\nmatch_checks &lt;- logs_nws |&gt; \n  mutate(match_null = if_else(is.na(name), T, F))\n\nmatch_checks |&gt; \n  tabyl(match_null) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nAbout 15 percent of our hourly logs don’t have a station match. Here TRUE values means there is missing station data.\nLet’s take a look at which ones are at issue:\n\n\nExpand this to see code\nmatch_checks |&gt; \n  tabyl(unit, match_null) |&gt; \n  adorn_percentages() |&gt; \n  adorn_pct_formatting() |&gt; \n  adorn_ns() |&gt; \n  tibble()\n\n\n\n  \n\n\n\nIn some cases we didn’t have a prison close enough. In other cases – like Jordan – we had a station, but there were no recordings for the time period.\nHere is a filtered list to more easily see units that have no NWS readings:\n\n\nExpand this to see code\nno_compare &lt;- match_checks |&gt; \n  tabyl(unit, match_null) |&gt; \n  filter(`TRUE` == 168) |&gt; \n  tibble()\n\nno_compare\n\n\n\n  \n\n\n\nWe’re going to remove these units going forward, leaving us with 71 units.\n\n\nExpand this to see code\nno_compare_units &lt;- no_compare |&gt; pull(unit)\n\nlogs_nws_compare &lt;- logs_nws |&gt; \n  filter(!unit %in% no_compare_units)\n\nlogs_nws_compare |&gt; count(unit) |&gt; nrow()\n\n\n\n\nTA: Missing weather stations\nWe don’t have a matching weather station for 11 of the 82 unites. Using visualcrossing we might be able to do a better job finding weather stations, and also to keep a better record of how far away these stations are from the units. We will do this, but later.",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#protocols",
    "href": "02-outdoor-reliability.html#protocols",
    "title": "Outdoor log reliability",
    "section": "Protocols",
    "text": "Protocols\n\nWhich days met protocol measures based on each prison’s outdoor temperature log?\n\nHow does that compare to actual protocols that were place?\n\n\nWe’ll attempt here to determine if heat protocols thresholds were met at different prisons each day, according to their own readings.\nIn this case we’ll start with all 8 days of readings that we have. That is still tricky as we protocols go into place only when the threshold is met for 3 consecutive days.\n\nPreparing the protoco data\nFirst, we need to find the hottest temp and heat index for each day in each unit according to their logs. There are some warnings here because some values are blank and can’t calculat properly. They become -Inf which is treated similar to NA for our purposes.\n\n\nExpand this to see code\nlogs_daily_sums &lt;- logs_all |&gt; \n  group_by(unit, date) |&gt; \n  summarize(\n    u_cnt = n(),\n    u_max_tmp = max(temp, na.rm = T),\n    u_max_hi = max(hi_wc, na.rm = T),\n    .groups = \"drop\"\n  )\n\n\nWarning: There were 22 warnings in `summarize()`.\nThe first warning was:\nℹ In argument: `u_max_hi = max(hi_wc, na.rm = T)`.\nℹ In group 81: `unit = \"Dalhart\"` and `date = 2023-07-24`.\nCaused by warning in `max()`:\n! no non-missing arguments to max; returning -Inf\nℹ Run `dplyr::last_dplyr_warnings()` to see the 21 remaining warnings.\n\n\nExpand this to see code\nlogs_daily_sums |&gt; head()\n\n\n\n  \n\n\n\nThen we add the protocol calculations. We are viewing a sample here.\n\n\nExpand this to see code\nlogs_daily_sums_flags &lt;- logs_daily_sums |&gt; \n  mutate(\n    u_tmp_flag = (case_when(\n      u_max_tmp &gt;= 105 ~ TRUE,\n      .default = FALSE)),\n    u_hi_flag = (case_when(\n      u_max_hi &gt;= 113 ~ TRUE,\n      .default = FALSE)),\n    u_protocol_flag = case_when(\n      u_tmp_flag == \"TRUE\" | u_hi_flag == \"TRUE\" ~ TRUE,\n      .default = FALSE),\n    )\n\nlogs_daily_sums_flags |&gt; slice_sample(n = 5)\n\n\n\n  \n\n\n\nI wanted to check if when we have -Inf as a value if the protocol flag works correctly, but there weren’t any records where the heat index is missing but the temp was 105.\n\n\nExpand this to see code\nlogs_daily_sums_flags |&gt; filter(is.infinite(u_max_hi))\n\n\n\n\nChecking flags vs protocols\nOK now we add whether a protocol was active on a given day at a prison, for comparison.\nWe need to remember that meeting the protocol_flag DOES NOT put a prison in protocol. The flag conditions have to exist for three days for them to go into affect. (While we could calculate the run of days with the data we have, we only have 8 days so it would not be accurate for the first two days.)\n\n\nExpand this to see code\nlogs_flags_activations &lt;- logs_daily_sums_flags |&gt; \n  left_join(activations, by = join_by(unit, date)) |&gt; \n  mutate(protocol_active = if_else(is.na(protocol_active), F, protocol_active))\n\nlogs_flags_activations_unit &lt;- logs_flags_activations |&gt;\n  group_by(unit) |&gt; \n  summarise(\n    flag_yes_active_no = sum(u_protocol_flag == T & protocol_active == F),\n    flag_yes_active_yes = sum(u_protocol_flag == T & protocol_active == T)\n  )\n\nlogs_flags_activations_unit\n\n\n\n  \n\n\n\nHow many units have three or more days with the flag, but not be in protocol? (Remember they have to be a run of three.)\n\n\nExpand this to see code\nlogs_flags_activations_unit |&gt; \n  filter(flag_yes_active_no &gt;= 3) |&gt; \n  arrange(desc(flag_yes_active_no))\n\n\n\n  \n\n\n\n\n\nLooking deeper\n\npeek_cols &lt;- c(\"unit\", \"date\", \"u_max_tmp\", \"u_max_hi\", \"u_protocol_flag\", \"protocol_active\")\n\nFor Young, the heat index is flipping the flag, though the temperatures are at or below 100. All 8 days have the flag, so the protocol should’ve been put into place.\n\n\nExpand this to see code\nlogs_flags_activations |&gt; filter(unit == \"Young\") |&gt; select(all_of(peek_cols))\n\n\n\n  \n\n\n\nLet’s check Havins it did have activiation in the first of our dates, and the remaining dates should’ve also kept protocols based on temperature.\n\n\nExpand this to see code\nlogs_flags_activations |&gt; filter(unit == \"Havins\") |&gt; select(all_of(peek_cols))\n\n\n\n  \n\n\n\nFor Plane, there were some pretty hot days and high heat indexes, including four flags in a row.\n\n\nExpand this to see code\nlogs_flags_activations |&gt; filter(unit == \"Plane\") |&gt; select(all_of(peek_cols))\n\n\n\n  \n\n\n\nAllred had 6 out of 8 days reach protocol levels, but not go into protocol?\nIn this case it did not reach three days in a row until the end of the 8 days, so perhaps protocols when into place the next day?\n\n\nExpand this to see code\nlogs_flags_activations |&gt; filter(unit == \"Allred\") |&gt; select(all_of(peek_cols))",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#reliability-answers",
    "href": "02-outdoor-reliability.html#reliability-answers",
    "title": "Outdoor log reliability",
    "section": "Reliability answers",
    "text": "Reliability answers\nWe must remeber these weather stations may be up to 40 miles away from the unit.\n\nHow do log temperature readings compare to the nearest weather station?\n\ndiff in temp\ndiff in heat index\n\n\nHere we find the difference in the prison recorded temp and heat index compared to the nearest station, if we have one. In some cases the diffs are NA because we don’t have a nearby station, or one of the calculating numbers is missing for whatever reason.\nWe only do this heat index calculation if one of the temperatures if above 80 degrees, because heat indexes get wonky when it is below 80.\n\n\nExpand this to see code\nlogs_diffs &lt;- logs_nws_compare |&gt; \n  mutate(\n    tmp_diff = temp - tmp,\n    hi_diff = case_when(\n      temp &gt;= 80 | tmp &gt;= 80 ~ hi_wc - hi,\n      .default = NA\n    )\n  )\n\nlogs_diffs |&gt; slice_sample(n = 20)",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#temperature-differences",
    "href": "02-outdoor-reliability.html#temperature-differences",
    "title": "Outdoor log reliability",
    "section": "Temperature differences",
    "text": "Temperature differences\n\nAverage temp difference\nHere we look across the dataset at the diff_tmp (unit temperature - weather staiton temperaure).\n\n\nExpand this to see code\nlogs_diffs |&gt; \n  summarise(\n    max_tmp_diff = max(tmp_diff, na.rm = T),\n    avg_tmp_diff = (mean(tmp_diff, na.rm = T) * 100) |&gt; round(1),\n    med_tmp_diff = median(tmp_diff, na.rm = T)\n  )\n\n\n\n  \n\n\n\n\n\nDistribution temp difference\nLet’s do a quick plot of these to see how the are distributed. i.e., how many rows are within x degrees of the weather station temperature. Note: there are 248 blank rows where we could not determine the difference because of a missing value.\n\n\nExpand this to see code\nggplot(logs_diffs, aes(x = tmp_diff)) +\n  geom_histogram(binwidth = 1, fill = \"lightblue\", color = \"black\") +\n  labs(\n    title = \"Compare temp draft\",\n    subtitle = str_wrap(\"Each bar is how many unit records are within x degrees of the closest weather station.\"))\n\n\nWarning: Removed 248 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nI can see that there are a just a couple of outliers Let’s consider removing those and see the spread mo betta.\nThere are only five records (out of 11,928) where the temp is 20 or more degrees off.\n\n\nExpand this to see code\n# the outliers\nlogs_diffs |&gt; \n  filter(abs(tmp_diff) &gt;= 20)\n\n\n\n  \n\n\n\nExpand this to see code\n# chart it\nlogs_diffs |&gt; \n  filter(abs(tmp_diff) &lt; 20) |&gt; \n  ggplot() +\n  aes(x = tmp_diff) +\n  geom_histogram(binwidth = 2, fill = \"lightblue\", color = \"black\") +\n  scale_x_continuous(breaks = seq(-20, 20, 2)) +\n  theme(panel.grid.minor.x = element_blank()) +\n  labs(\n    title = \"Most temps within about 4 degrees\",\n    subtitle = str_wrap(\"Each bar is how many unit records are within x degrees of the closest weather station. We've removed any rows that are more the 20 degrees off (of which there are five).\")\n  )\n\n\n\n\n\n\n\n\n\nHow many rows negative vs positive?\n\n\nExpand this to see code\nlogs_diffs |&gt; filter(tmp_diff &lt;= 0) |&gt; nrow() # unit is lower\n\n\n[1] 6158\n\n\nExpand this to see code\nlogs_diffs |&gt; filter(tmp_diff &gt; 0) |&gt; nrow() # unit is higher\n\n\n[1] 5522\n\n\n\n\nStandard deviation temp difference\nThis might be too technical or unneeded, but let’s look at this using the standard deviation. The standard deviation describes how much the values in the dataset typically vary from the mean (average). Like how whacked are they.\n\n\nExpand this to see code\n# The standard deviation of tmp_diff\n# logs_diffs |&gt; summarise(td_sd = sd(tmp_diff, na.rm = T))\ntmp_diff_sd &lt;- \n  logs_diffs |&gt;\n  filter(abs(tmp_diff) &lt; 20) |&gt;\n  pull(tmp_diff) |&gt; sd(na.rm = T)\n\ntmp_diff_sd\n\n\n[1] 4.084984\n\n\nExpand this to see code\n# percent outside the sd\nlogs_diffs |&gt; \n  filter(abs(tmp_diff) &lt; 20) |&gt; \n  mutate(\n    td_sd_out = case_when(\n      abs(tmp_diff) &gt; tmp_diff_sd ~ T,\n      .default = F\n    )\n  ) |&gt; \n  tabyl(td_sd_out) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nHere about 1/4 of the rows are outside the standard deviation.\n\n\nPercent temp within range\nThis might make more sense to a human: What percentage of the overall records fall within 2 degrees of the nearest weather station. How about within 4 degrees?\nWe are NOT removing outliers here.\nThe TRUE value here is the percentage of records within 2 or 4 degrees, respectively. To be clear, within 4 degrees also includes those within 2 degrees.\n\n\nExpand this to see code\ntmp_diff_2_4 &lt;- logs_diffs |&gt;\n  # filter(abs(tmp_diff) &lt; 20) |&gt; \n  filter(!is.na(tmp_diff)) |&gt; \n  mutate(in_2d = abs(tmp_diff) &lt;= 2,\n         in_4d = abs(tmp_diff) &lt;= 4)\n\ntmp_diff_2_4 |&gt; \n  tabyl(in_2d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nExpand this to see code\ntmp_diff_2_4 |&gt; \n  tabyl(in_4d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nNow that seems easier to understand than standard deviation.\n\n\nWithin range degrees by unit\nLet’s see how this differs by unit.\nin_2d_prc is the percentage of records for that unit within 2 degrees the closest weather station. in_4d_prc is the same for within 4 degrees.\n\n\nExpand this to see code\ntmp_diff_2_4 |&gt; \n  group_by(unit) |&gt; \n  summarize(\n    cnt = n(),\n    in_2d_true = sum(in_2d == T),\n    in_2d_prc = ((in_2d_true / cnt) * 100) |&gt; round(1),\n    in_4d_true = sum(in_4d == T),\n    in_4d_prc = ((in_4d_true / cnt) * 100) |&gt; round(1)\n  ) |&gt; \n  # removes the cnt true rows from display\n  select(unit, ends_with(\"prc\")) |&gt; \n  arrange(in_2d_prc)\n\n\n\n  \n\n\n\nSorted above based on lowest percentage within 2 degrees.\nNow, these numbers could really depend on how close the weather station is to the unit. For instance, I’m not happy with the choice of a station at Pearland Regional Airport for the Ramsey unit (maybe 30 miles?). The Angleton/Brazoria airport is closer (13 miles), as is Houston Southwest Airport (14 miles).\n\nWe will check all the weather stations and add distance between unit and weather station at a future date.",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  },
  {
    "objectID": "02-outdoor-reliability.html#heat-index",
    "href": "02-outdoor-reliability.html#heat-index",
    "title": "Outdoor log reliability",
    "section": "Heat index",
    "text": "Heat index\n\nHI diff distribution\nLet’s do the same for heat index. We’ll skip the standard deviations and just look at the distribution and percentages.\nThere are about 50 records outside a 20 degree difference that we show here, but remove for the plot.\n\n\nExpand this to see code\nlogs_diffs |&gt; \n  filter(abs(hi_diff) &gt;= 20)\n\n\n\n  \n\n\n\nExpand this to see code\nlogs_diffs |&gt; \n  filter(abs(hi_diff) &lt; 20) |&gt;\n  ggplot() +\n  aes(x = hi_diff) +\n  geom_histogram(binwidth = 2, fill = \"lightblue\", color = \"black\") +\n  scale_x_continuous(breaks = seq(-20, 20, 2)) +\n  theme(panel.grid.minor.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\nWithin HI range\nHere we find the percentage of heat index records within 2 or 4 degrees. We are NOT removing outliers here.\nTRUE means within 2 or 4 degrees, respectively.\n\n\nExpand this to see code\nhi_diff_2_4 &lt;- logs_diffs |&gt;\n  filter(!is.na(hi_diff)) |&gt; \n  mutate(in_2d = abs(hi_diff) &lt;= 2,\n         in_4d = abs(hi_diff) &lt;= 4)\n\nhi_diff_2_4 |&gt; \n  tabyl(in_2d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\nExpand this to see code\nhi_diff_2_4 |&gt; \n  tabyl(in_4d) |&gt; \n  adorn_pct_formatting()\n\n\n\n  \n\n\n\n\n\nWithin HI range by unit\nAnd now to do that by unit. We have more cases where we don’t have the heat index for both the unit and the weather station, so we include the count of records we are comparing here.\n\n\nExpand this to see code\nhi_diff_2_4 |&gt; \n  group_by(unit) |&gt; \n  summarize(\n    cnt = n(),\n    in_2d_true = sum(in_2d == T),\n    in_2d_prc = ((in_2d_true / cnt) * 100) |&gt; round(1),\n    in_4d_true = sum(in_4d == T),\n    in_4d_prc = ((in_4d_true / cnt) * 100) |&gt; round(1)\n  ) |&gt; \n  # removes the cnt true rows from display\n  select(unit, cnt, ends_with(\"prc\")) |&gt; \n  arrange(in_2d_prc)",
    "crumbs": [
      "Analysis",
      "Outdoor log reliability"
    ]
  }
]