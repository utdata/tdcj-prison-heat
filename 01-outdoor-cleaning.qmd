---
title: "Outdoor logs cleaning"
author: "Media Innovation Group"
code-fold: true
code-summary: "Expand this to see code"
---

This notebook compiles data collected from one week of outdoor temperature logs collected by the Texas Department of Criminal Justice. The dates considered were July 24, 2023 through July 31, 2023. The logs are hand-written and data could not be accurately pulled from them using AI, so a group of Media Innovation Group fellows transcribed the week of logs for each prison where we had records. Those records were acquired through a public information request by Lauren McGaughey of the Texas Newsroom public radio collaborative.

We created a Google Spreadsheet file for each prison, recording the temperatures for each hour as best as we could decipher them. Here we download a tracking spreadsheet, which is then used to download all of the individual log sheets, combining them into a single file. When then prepare that file for analysis.

## Setup

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(janitor)
```

## Download the tracking sheet

This sheet is in MIG Data > Jail Logs > Outdoor Logs Data. It is called Outdoor Logs Tracking Sheet.

> Options are set to `eval: false` to avoid re-downloading. Set to `true` to update.

```{r}
#| label: tracking-file
#| eval: false

# download.file("https://docs.google.com/spreadsheets/d/e/2PACX-1vRQBIKas7s5Wwe4bd9tMlAheCsjqeFU2JMZfk6h8Hc_QH6Dx02SEZmP6ieny13hCyZQosmsRirDEu9P/pub?output=csv", "data-original/outdoor-jail-logs-urls.csv")
```

Read in the tracking sheet url

```{r}
#| label: read-tracking
#| warning: false
#| message: false

log_urls <- read_csv("data-original/outdoor-jail-logs-urls.csv") |> clean_names()

log_urls |> glimpse()
```

In some cases we had the urls formatted incorrectly. Result should be 0 rows.

```{r}
log_urls |> 
  filter(str_detect(url, "pubhtml"))
```

## Download all the files

This code below was courtesy of ChatGPT, based on the following prompt: "I'm using R. I have a data frame that has a list of urls and slugs, which are short file names. Please write me a loop or other code that will download all the files at those urls, with their names as the slug. These are all csv files." There were some edits.

> Options are set to `eval: false` to avoid re-downloading. Set to `true` to update.

```{r}
#| label: download-logs
#| warning: false
#| message: false
#| eval: false

# set our urls to df
df <- log_urls

# Download loop
for (i in seq_len(nrow(df))) {
  url <- df$url[i]
  slug <- df$slug[i]
  file_name <- paste0("data-original/2023_logs/", slug, ".csv")
  
  if (is.na(url)) {
    # message(paste("No URL for slug:", slug))
    next
  } else {
    tryCatch({
      download.file(url, destfile = file_name, mode = "wb")
    }, error = function(e) {
      message(paste("Failed to download:", url, "-", e$message, "\n"))
    })
  }
}

```

### Make a files list

Make a list of files:

```{r}
logs_list <- list.files(
  "data-original/2023_logs",
  pattern = ".csv",
  full.names = TRUE
  )

logs_list
```

### File checks

A little ChatGPT help here to find a way to count the number of lines in each file. There should be 193 in each one. In some cases our spreadsheets included blank rows, which we fixed. There is still a blank column, but we take care of that later.

This takes the list of files above and counts the lines. When then filter the list for any that aren't 193 lines long. The result _should_ be 0 rows.

```{r}
count_lines <- function(file) {
  length(readLines(file, warn = FALSE))
}

line_counts <- sapply(logs_list, count_lines)
line_counts_data <- tibble(file = basename(names(line_counts)), lines = line_counts)

line_counts_data |> filter(lines != 193)
```

Sometimes we needed to check a specific file being downloaded.

```{r}
# download.file(
#   "https://docs.google.com/spreadsheets/d/e/2PACX-1vRPdRSPyFwnYPgrLkJktmtg2hy4BmWl0eILzwO7KLalYxjPF2xYMZj4kSTuo3u7wwUckskou4-Dm4zF/pub?gid=0&single=true&output=csv",
#   "data-original/2023_logs/r1-estelle.csv"
# )
```


## Combine the files

Again uses the logs_list from above.

```{r}
logs_raw <- logs_list |>  #set_names(basename) |>
  map(
  read_csv,
  col_types = cols(.default = col_character())
) |> list_rbind() |>
  clean_names()

logs_raw |> glimpse()
```

### Check and remove extra column

```{r}
logs_raw |> filter(!is.na(x12))
```

```{r}
logs_tight <- logs_raw |> select(-x12)

logs_tight |> glimpse()
```

## Fix date, time, rec

Two decisions in the formatting of our log necessitate this:

- We configured our date and time columns like the form itself instead of datetime formats that easily import.
- We originally numbered the `rec` column from 1 to 24 thinking we needed to label each reading, but the weather station data will be labeled 0 to 23 because it is the hour of time.

Here we create a real datetime, date and proper hour.

Also, we remove artifact columns and any blank columns by specifying the columns we do need.

```{r}
logs_dates <- logs_tight |> 
  mutate(
    new_time = case_when(
      time == "12:30 a.m." ~ "00:30:00",
      time == "12:30 p.m." ~ "12:30:00",
      str_sub(time, -4, -1) == "a.m." ~ paste0(str_extract(time, "^(\\d+)"),":30:00"),
      str_sub(time, -4, -1) == "p.m." ~ paste0(str_extract(time, "^(\\d+)") |> as.numeric() + 12, ":30:00"),
      .default = NA
    )
  ) |> 
  mutate(
    datetime = mdy_hms(paste(date, new_time)),
    date = mdy(date),
    hour = hour(datetime),
    .after = time
  ) |> 
  select(
    unit,
    date,
    # rec,
    datetime,
    hour,
    temp,
    humid,
    wind,
    hi_wc,
    hi_wc_n,
    person,
    notes
  )

logs_dates |> slice_sample(n = 10)
```

## Convert the numbers

Our various readings come in as text. We convert them to numbers.

If in our transcriptions we wrote in "N/A" or anything else other than a number, it will be removed.

```{r}
logs_numbs <- logs_dates |> 
  mutate(
    across(c(temp, humid, wind, hi_wc, hi_wc_n), parse_number)
  )

logs_numbs |> slice_sample(n = 10)
```

## Fix names

In some cases the name on the outdoor log sheet did not match our tracking sheet, which means we have trouble later when we need to match that information.

```{r}
logs_fixes <- logs_numbs |> 
  mutate(
    unit = case_match(
      unit,
      "Travis State Jail" ~ "Travis County",
      "Choice Moore" ~ "Moore, C.",
      "Jester 3" ~ "Jester III",
      "Lane Murray" ~ "Murray",
      "Wallace Pack" ~ "Pack",
      "Mt. View" ~ "Woodman",
      .default = unit)
  )

logs_fixes |> filter(str_detect(unit, "Jordan"))
```


## Add the region

It's helpful later to have the region of the prison as part of this dataset, so I'm going to add it here from the unit info.

```{r}
regions <- read_rds("data-processed/01-unit-info-cleaned.rds") |> 
  select(unit_name, region)

logs_regions <- logs_fixes |> 
  left_join(regions, by = join_by(unit == unit_name)) |> 
  relocate(region, .after = unit)

logs_regions |> filter(str_detect(unit, "Jordan"))
```

## Export the logs

```{r}
logs_regions |> write_rds("data-processed/01-outdoor-cleaned.rds")
```

## Notes pages

When we transcribed the logs, we also included a notes sheet where we added any overall relevant or interesting information.

We will be following the same steps as above, except that we have to piece together the download url from our tracking sheet.

```{r}
#| label: notes-urls

notes_urls <- log_urls |> 
  # filter(region == "II") |> # filters to region for testing
  select(slug, url, notes_gid) |> 
  mutate(
    pub_url = str_extract(url, "(.*pub\\?)", group = 1),
    notes_url = paste0(pub_url, "gid=", notes_gid, "&output=csv")
  ) |> 
  filter(!is.na(url)) |> 
  select(slug, notes_url) 

notes_urls
```

Then we download all the files. `eval: false` is set here so files won't be re-downloaded.

```{r}
#| label: notes-download
#| warning: false
#| message: false
#| eval: false

# set our urls to df
notes_df <- notes_urls


# Download loop
for (i in seq_len(nrow(notes_df))) {
  url <- df$notes_url[i]
  slug <- df$slug[i]
  file_name <- paste0("data-original/2023_notes/", slug, ".csv")
  
  if (is.na(url)) {
    # message(paste("No URL for slug:", slug))
    next
  } else {
    tryCatch({
      download.file(url, destfile = file_name, mode = "wb")
    }, error = function(e) {
      message(paste("Failed to download:", url, "-", e$message, "\n"))
    })
  }
}

```

Make a list of file paths to combine.

```{r}
notes_list <- list.files(
  "data-original/2023_notes",
  pattern = ".csv",
  full.names = TRUE
  )

notes_list
```

Now we put those files together into a single tibble.

```{r}
notes_raw <- notes_list |> 
  set_names(basename) |>
  map(
  read_csv,
  col_types = cols(.default = col_character()),
  col_names = c("notes")
) |> list_rbind(names_to = "unit") |>
  clean_names()

notes_raw |> glimpse()
```

We have a little cleanup here for the unit and region. We drop the x columns, which are empty.

```{r}
notes_cleaned <- notes_raw |> 
  filter(!is.na(notes)) |> 
  mutate(
    region = str_extract(unit, pattern = "(.*)-", group = 1),
    unit = str_extract(unit, pattern = ".*-(.*).csv", group = 1)
  ) |> 
  select(unit, region, notes)

notes_cleaned |> head()
```

We'll export this as both a csv and an rds because we may just put these into a spreadsheet for review.

```{r}
notes_cleaned |> write_rds("data-processed/01-outdoor-notes.rds")
notes_cleaned |> write_csv("data-processed/01-outdoor-notes.csv")
```
